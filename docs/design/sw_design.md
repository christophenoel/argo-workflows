# Software Design Document for Argo-Workflow

### General
This section provides a detailed look at the design aspects of each component used in the software. 

### Component Identifier

- **Naming Convention**: Argo-Workflows

Argo Workflows enables the chaining of container-based modules and workflows.


### Type
 
#### Logical Characteristics

Argo Workflows is the main functional component implementing the Data Processing Environment sub-system of the OHDSA platform.

#### Physical Characteristics

Argo Workflows is a Commercial-Of-The-Shelf (COTS) Kubernetes-native workflow engine that orchestrates parallel jobs on a Kubernetes cluster. It uses Kubernetes resources like custom resources, pods, jobs, ConfigMaps, and Secrets to implement and manage workflows. Each workflow is defined as a Workflow object, which dictates the sequence of tasks to be executed. These tasks are encapsulated as Kubernetes Jobs or Pods, making each step of the workflow a discrete, schedulable unit under Kubernetes management.

### Purpose

Argo Workflows component is designed to fulfill a pivotal role in managing and executing complex workflows that integrate various modules within Docker containers. The purpose of Argo Workflows is to enable users to submit both simple and elaborate workflows with a versatile Workflow Definition Language that accommodates Directed Acyclic Graph models, dynamic resource allocation, retry strategies, the utilisation of specialized hardware, and completion notifications. 

### Function

Argo Workflows is a container-native workflow engine for Kubernetes, designed to orchestrate parallel jobs in a cloud environment.

The following subsections focus on the implementation of the **OHDSA capabilities** supported by the workflow engine.

#### API


The Argo Workflows component provides a REST API that facilitates the creation, management, and monitoring of workflows.

Key API Operations:

1. **Create Workflow Template**: Allows the user to define and register a reusable workflow template.
2. **List Available Workflow Templates**: Retrieves a list of all registered workflow templates.
3. **Describe Workflow Template**: Provides detailed information about a specific workflow template.
4. **List Workflows**: Returns a list of all active and completed workflows.
5. **Submit Workflow**: Initiates a new workflow based on a specified workflow template.
6. **Delete Workflow**: Removes a specified workflow from the system.
7. **Retrieve Workflow Status**: Fetches the current status of a specific workflow.
8. **Retrieve Workflow Logs**: Provides access to the logs generated by a specific workflow.

For detailed documentation and examples of these API operations, refer to the [API Design Documentation](rest_api_design.md).


#### Reusable Workflow Templates

Argo Workflows supports the creation and utilisation of reusable workflow templates to standardise and streamline workflow definitions.

Reusable workflow templates in Argo Workflows allow users to define a set of tasks and their configurations once and reuse them across multiple workflows. 

The templates encapsulate the following elements:

- **Entrypoint**: Defines the main entry point for the workflow, specifying the starting task.
- **Templates**: Lists the tasks included in the workflow template, each with its own configuration such as container image, commands, and arguments.

The following [example](../../examples/hello-world-template/hello-world-wf-template.yml)
represents a basic workflow template definition:
```yaml
apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: hello-world-wft
spec:
  entrypoint: whalesay
  templates:
    - name: whalesay
      inputs:
        parameters:
          - name: message
      container:
        image: docker/whalesay
        command: [ cowsay ]
        args: ["{{inputs.parameters.message}}"]
```

The usage of workflow template works as follows:

1. **Definition**: Users define a workflow template, specifying the tasks and their configurations. This is done using YAML, where each task within the template is defined with its specific parameters and resources.

2. **Registration**: The defined workflow template is registered with the Argo system using the appropriate API endpoint. This makes the template available for use in workflows.

3. **Reuse**: Once registered, the workflow template can be referenced in multiple workflows. This is achieved by specifying the template reference in the workflow definition, allowing the workflow to utilise the predefined tasks and configurations.

4. **Execution**: When a workflow that references a reusable template is executed, Argo orchestrates the tasks as defined in the template, ensuring that each task is executed according to the specified configuration.

Argo provides a REST API to manage workflow templates, including operations to create, list, describe, and delete templates. This API facilitates the integration of workflow template management into automated processes and external systems.

#### DAG Workflow Creation

Argo Workflows supports the creation of Directed Acyclic Graph (DAG) workflows, which allow for the definition of complex workflows with dependencies between tasks. This capability ensures optimal execution order and efficient resource utilisation.

DAG workflows in Argo enable users to define workflows where tasks are executed based on their dependencies. This means that a task will only start once all its dependent tasks have been completed successfully.

- **Task Dependencies**: Tasks in a DAG can specify dependencies, indicating which tasks need to be completed before they can be executed.
- **Parallel Execution**: Independent tasks can run in parallel, maximising resource usage and reducing overall execution time.
- **Template Reuse**: DAG workflows can reference reusable workflow templates, allowing for modular and maintainable workflow definitions.
- **Dynamic Execution**: The DAG model supports dynamic execution paths, where the workflow can adapt based on the results of preceding tasks.

The process of creating and executing a DAG workflow is as follows:

1. **Definition**: A DAG workflow is defined in a YAML file, where the `templates` section includes a `dag` template that specifies the tasks and their dependencies.
2. **Task Specification**: Each task within the DAG is defined with a `name`, `templateRef` to reference reusable templates, and `dependencies` to specify other tasks that must complete before this task starts.
3. **Execution**: When the DAG workflow is submitted, Argo orchestrates the execution based on the defined dependencies. Tasks are scheduled to run in the optimal order, ensuring that each task only starts once all its dependencies are resolved.

Below is a minimal example of a DAG workflow definition:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: dag-example-
spec:
  entrypoint: dag-example
  templates:
    - name: dag-example
      dag:
        tasks:
          - name: task-1
            templateRef:
              name: some-template
              template: some-task
          - name: task-2
            dependencies: [task-1]
            templateRef:
              name: some-template
              template: another-task
```

In this example:
* apiVersion: Specifies the API version.
* kind: Indicates the resource type, which is Workflow.
* metadata: Contains metadata, including the workflow name.
* spec: Defines the specifications of the workflow.
* entrypoint: Specifies the main entry point for the DAG workflow.
* templates: Lists the tasks included in the DAG workflow.
* dag: Defines the DAG structure with tasks and their dependencies.

#### Data Artefacts

Argo Workflows supports the management of data artefacts, enabling the staging-in, staging-out, and passing of data between workflow steps. This capability is essential for handling input and output data efficiently within workflows.

The features of Argo Workflows for managing data artefacts include:

- **Artifact Repositories**: Argo supports the use of artifact repositories for storing and retrieving data. These repositories can be any S3-compatible storage, such as Minio, or other interfaces like GIT.
- **Input Artefacts**: Workflows can specify input artefacts that are provided as arguments when the workflow is executed.
- **Intermediate Artefacts**: Intermediate data generated during workflow execution can be stored in artifact repositories to be used by subsequent steps.
- **Output Artefacts**: Workflows can create and store output artefacts in designated repositories, which can then be accessed by other workflows or external systems.
- **Credential Agnosticism**: Workflow templates do not manage credentials directly. Instead, artefact management is handled by the workflow runtime, ensuring security and separation of concerns.

The process for managing data artefacts in Argo Workflows is as follows:

1. **Configuration**: Define artifact repositories within Kubernetes ConfigMaps. These repositories are referenced in workflow templates to standardise artefact handling across workflows.
2. **Data Input**: Specify input artefacts in the workflow template. These artefacts are passed as arguments when the workflow is invoked.
3. **Intermediate Storage**: Store intermediate artefacts in the default or specified artifact repository, ensuring they are accessible for subsequent workflow steps.
4. **Data Output**: Create and store output artefacts in artifact repositories, enabling reuse and integration with other workflows or systems.


Below is a minimal example of an artifact repository configuration in a Kubernetes ConfigMap:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: artifact-repositories
data:
  default-artifact-repository: |
    s3:
      bucket: argo-bucket
      endpoint: minio:9000
      insecure: true
      accessKeySecret:
        name: my-minio-cred
        key: accesskey
      secretKeySecret:
        name: my-minio-cred
        key: secretkey
```

In this example:

* apiVersion: Specifies the API version.
* kind: Indicates the resource type, which is ConfigMap.
* metadata: Contains metadata, including the ConfigMap name.
* data: Defines the artifact repository configuration, including S3 endpoint details and credentials.

#### Retry Strategy

Argo Workflows provides robust retry strategies to handle task failures effectively, ensuring that workflows can recover from transient errors and improve reliability.

The retry strategy in Argo Workflows allows for the configuration of retries for failed tasks, providing flexibility in handling different types of errors. Key features include:

- **Limit Parameter**: Specifies the maximum number of retry attempts for a task.
- **Retry Policies**: Defines conditions under which a task should be retried. Available policies include:
  - **Always**: Retries all failed tasks regardless of the failure type.
  - **OnFailure**: Retries tasks that are marked as failed by Kubernetes.
  - **OnError**: Retries tasks that encounter errors related to the Argo controller or when init or wait containers fail.
  - **OnTransientError**: Retries tasks that encounter transient errors or errors matching a specific pattern.
- **Expressions**: Utilises expressions to control retry behaviour, accessing variables such as `lastRetry.exitCode`, `lastRetry.status`, `lastRetry.duration`, and `lastRetry.message`.
- **Backoff**: Implements a delay between retry attempts to prevent immediate consecutive retries, which can help mitigate issues from transient failures.

The process for implementing retry strategies in Argo Workflows is as follows:

1. **Define Retry Parameters**: Specify the limit for retries and the retry policy within the workflow or template definition.
2. **Set Retry Conditions**: Use expressions to determine when a retry should be attempted based on the status and exit codes of previous attempts.
3. **Configure Backoff**: Define the backoff strategy to introduce delays between retry attempts, helping to manage the load and reduce the likelihood of repeated failures.

Below is a minimal example of a retry strategy configuration in a workflow:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: retry-strategy-
spec:
  entrypoint: retry-example
  templates:
    - name: retry-example
      retryStrategy:
        limit: 3
        retryPolicy: "OnFailure"
        backoff:
          duration: "5s"
          factor: 2
      container:
        image: busybox
        command: ["sh", "-c"]
        args: ["exit 1"]
```
In this example:

* retryStrategy.limit: Specifies the maximum number of retry attempts.
* retryStrategy.retryPolicy: Sets the retry policy to retry on failure.
* retryStrategy.backoff: Configures a backoff strategy with an initial delay of 5 seconds and an exponential backoff factor of 2.


#### Notification

Argo Workflows provides mechanisms to send notifications based on workflow events, ensuring users are informed about the status and results of their workflows.

The notification features in Argo Workflows enable users to configure alerts and messages for various workflow events, such as completions, failures, and other significant statuses. Key features include:

- **Exit Handlers**: Define actions to be taken upon workflow completion, allowing notifications to be sent based on the workflow's outcome.
- **Notification Types**: Support for various notification types, such as emails, messages via protocols like AMQP, or custom scripts.
- **Custom Containers**: Use custom containers to execute commands that send notifications, offering flexibility in how notifications are handled and delivered.

The notification system in Argo Workflows operates as follows:

1. **Define Exit Handlers**: Configure exit handlers in the workflow definition. These handlers specify the actions to be executed when the workflow completes, regardless of success or failure.
2. **Custom Containers for Notifications**: Implement custom containers within exit handlers to perform notification actions. These containers can run scripts or commands to send emails, messages, or other notifications.


Below is a minimal example of a workflow configuration with an exit handler that sends a message to a RabbitMQ queue upon workflow completion:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: notify-rabbitmq-example-
spec:
  entrypoint: main
  templates:
    - name: main
      container:
        image: busybox
        command: ["sh", "-c"]
        args: ["exit 0"]
  onExit: notify
  templates:
    - name: notify
      container:
        image: rabbitmq:3-management
        command: ["sh", "-c"]
        args:
          - |
            apt-get update && apt-get install -y curl
            curl -i -u guest:guest -H "content-type:application/json" \
            -X POST -d '{"properties":{},"routing_key":"","payload":"Workflow completed successfully","payload_encoding":"string"}' \
            http://rabbitmq:15672/api/exchanges/%2F/amq.default/publish
```

#### Hardware Constraints

Argo Workflows allows for the specification of hardware constraints to ensure that workflows run on nodes with the appropriate resources and capabilities. This feature is essential for optimising resource usage and ensuring that workflows can utilise specialised hardware as needed.

##### Capabilities and Usage

The features for managing hardware constraints in Argo Workflows include:

- **Resource Requests and Limits**: Define the amount of CPU and memory resources required for each task.
- **Node Affinity**: Specify node affinity rules to ensure that tasks are scheduled on nodes with specific labels, indicating specialised hardware or other node-specific attributes.
- **Tolerations**: Define tolerations to allow tasks to be scheduled on tainted nodes, which can be used to reserve nodes for specific types of workloads.

##### How Hardware Constraints Work

The process for implementing hardware constraints in Argo Workflows is as follows:

1. **Define Resource Requirements**: In the workflow template, specify the CPU and memory resources required for each task using `resources.requests` and `resources.limits`.
2. **Set Node Affinity**: Use `nodeSelector` or `affinity` fields in the workflow template to ensure tasks are scheduled on nodes with the desired hardware characteristics.
3. **Configure Tolerations**: Add tolerations to the workflow template to allow tasks to run on nodes with specific taints, enabling the reservation of nodes for specialised tasks.

##### Benefits

The advantages of using hardware constraints in Argo Workflows include:

- **Optimised Resource Usage**: Ensures that tasks are scheduled on nodes with sufficient and appropriate resources, preventing resource contention and optimising performance.
- **Specialised Hardware Utilisation**: Allows workflows to take advantage of nodes with specialised hardware, such as GPUs or high-memory nodes, by specifying appropriate constraints.
- **Improved Workflow Efficiency**: Enhances the efficiency of workflows by ensuring that tasks are executed on nodes that meet their resource and hardware requirements.

##### Example Configuration

Below is a minimal example of a workflow configuration with hardware constraints:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: hardware-constraints-
spec:
  entrypoint: main
  templates:
    - name: main
      container:
        image: busybox
        resources:
          requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
            memory: "128Mi"
            cpu: "500m"
      nodeSelector:
        disktype: ssd
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "gpu"
          effect: "NoSchedule"
```

In this example:

* resources.requests: Specifies the minimum required CPU and memory for the task.
* resources.limits: Specifies the maximum allowed CPU and memory for the task.
* nodeSelector: Ensures the task is scheduled on a node with the label disktype: ssd.
* tolerations: Allows the task to be scheduled on nodes with the taint dedicated=gpu:NoSchedule.


#### Artefact Persistence and Retention

> TBD section

Argo Workflows provides mechanisms to manage the persistence of artefacts, ensuring that important data generated during workflow execution is retained and accessible for future use.

The features for managing artefact persistence in Argo Workflows include:

- **Persistent Artefacts**: Mark specific artefacts for persistence, ensuring they are not discarded after workflow completion.
- **Retention Policies**: Define policies to manage the lifecycle of persistent artefacts, including archiving and expiration.

The workflow engine operator must configure retention policies to remove expired workflows automatically
(see [Archiving workflows](https://argo-workflows.readthedocs.io/en/stable/workflow-archive/). 

The workflow archive stores information about the workflows such as the status, pods executed, results and more. This information is stored in a database such as PostgreSQL.

**Archive TTL**:
Specifies the time period to keep archived workflows before they will be deleted by the archived workflow garbage collection function. The default is forever.

Example:
```yaml
persistence:
  archiveTTL: 10d
```

#### Logging

Argo Workflows provides robust logging capabilities to help users monitor and debug their workflows. Logs are crucial for understanding the behaviour of workflows and diagnosing issues when they arise.

The logging features in Argo Workflows include:

- **Log Retrieval**: Access logs for individual tasks directly through the Argo Workflows UI, CLI, or API.
- **Log Storage**: Configure log storage to persist logs for later analysis.
- **Integration with Log Management Systems**: Forward logs to external log management systems like Elasticsearch, Fluentd, or Kibana for advanced searching, indexing, and analysis.


Argo Workflows captures logs from the containers running each workflow step. These logs can be accessed and managed as follows:

1. **Log Retrieval via UI**: Users can view the logs of each workflow step directly in the Argo Workflows web UI. This interface provides a convenient way to inspect logs without needing to access the underlying infrastructure.

2. **Log Retrieval via CLI**: The `argo logs` command allows users to fetch and display logs for a specific workflow or task from the command line. This is useful for scripting and automated log retrieval.

3. **Log Retrieval via API**: Logs can also be accessed programmatically through the Argo Workflows API, allowing integration with other systems and custom dashboards.

4. **Log Storage Configuration**: Configure the storage of logs by specifying log archival options. This can be done using Kubernetes ConfigMaps or similar mechanisms to define where and how logs should be stored.

5. **Integration with Log Management Systems**: For advanced log management, integrate Argo Workflows with external systems like Elasticsearch and Fluentd. These systems provide capabilities for indexing, searching, and visualising logs, enhancing the ability to monitor and troubleshoot workflows.

#### Metrics


- **Monitoring**: Argo produces metrics that provide information on the controller's status. As those metrics follow the same format as required by prometheus, those can be integrated with Prometheus.
  Two types of metrics are emitted by Argo:
  - Controller metrics: concerns the state of the controller;
  - Custom metrics: regards the state of a Workflow, or a series of Workflows. Those metrics can be defined on the Workflow/Step/Task emitting the metric. using the same name and help string, is a required by prometheus to track the metrics over time.

  For more information, see: See: https://argo-workflows.readthedocs.io/en/stable/metrics/

### Subordinates

Argo Workflows encompasses the various components and resources it manages and interacts with to orchestrate and execute workflows.

- **Workflow Templates** : These are predefined configurations that describe specific workflows. They serve as blueprints from which instances of workflows are generated and executed. Workflow templates define the sequence of tasks, their dependencies, and the resources needed for each task.
- **Tasks/Pods** - In the context of Argo Workflow, each step in a workflow is executed as a separate pod within the Kubernetes environment. These pods can be considered subcomponents of the workflow they belong to, executing specific actions defined by the workflow and then reporting back their status upon completion or failure.

> Note: identify the DB, message broker, prometheus, etc.

### Dependencies
- **Description**: Detail any operations or conditions that must be met before this component can function properly, including exclusions during its operation.

> Note: if relevant, express dependency with other components (e.g. workflow needs module registry)
> Reflect the config of charts 

### Interfaces

- **Control Flow**: Describe how the component starts and terminates, including any interactions during execution (such as interrupts).
- **Data Flow**: Explain the input and output data flows, ensuring data structures are linked with control flows and interface components through common data areas or files.

Documentation detailing the API specifications and usage is provided in teh [API Design](./rest_api_design)

> NOte: sequence diagram and link to the api-design document.


### Resources
- **Requirements**: Itemize what the component needs from its environment to perform its function, excluding items that are part of the component interface.

> NOTE: include docker dependencies, might include maven dependencies (libs)


### Data

Argo Workflows utilises complex internal data structures to orchestrate and manage workflows within Kubernetes. The main structures include:

- **Workflow Definitions**: Defined as custom resources in YAML, these include:
  - **Element Descriptions**: Identifiers (names), types (e.g., script, container), and dimensions (step hierarchies and dependencies).
  - **Relationships**: Defines parent-child relationships among steps using dependencies that structure the workflow as a Directed Acyclic Graph (DAG).
  - **Value Range**: Includes everything from static strings to dynamic outputs from previous steps.
  - **Initial Values**: Elements such as parameters may have default values specified in the workflow template.

- **Workflow Status**: This structure tracks the real-time state of each step within the workflow, indicating whether a step is pending, running, succeeded, or failed.

- **Artifacts**: Used to manage data inputs and outputs for each step. Artifacts can be files stored in various storage backends (e.g., S3, Artifactory).

For more detailed information about the internal data structures used by Argo Workflows, including their specifications and how they are utilised within workflows, refer to the [official Argo Workflows Field Reference](https://argo-workflows.readthedocs.io/en/stable/fields/). This documentation provides comprehensive guides on defining and managing workflows, with specific sections on workflow specifications, status monitoring, and artifact handling.
