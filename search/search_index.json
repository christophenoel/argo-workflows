{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Argo Workflow","text":""},{"location":"#overview","title":"Overview","text":"<p>Argo Workflows serves as the Workflow Engine of the Data Processing Environment (DPE) within the DimSum platform, developed in the context of ESA project OHDSA.</p> <p>The Workflow Engine implemented by Argo Workflows receives the execution requests from the Workflow Management System (WMTS). These workflows are implemented as a collection of Pluggable Acquisition and Processing Modules (PAPM).</p> <pre>2de0415a-b9fb-4e4d-9eaf-db20063f7e09</pre> <p>Argo Workflows enables the chaining of container-based modules and workflows. The software 'Argo Workflows' is a Commercial-Of-The-Shelf (COTS) which consists in a Kubernetes-native workflow engine that orchestrates parallel jobs on a Kubernetes cluster.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Chaining of Container-Based Modules: Create workflows connecting multiple containerized processes.</li> <li>Seamless Container Interfacing: Facilitate smooth interaction between containers.</li> <li>Workflow Completion Notifications: Provide notifications upon workflow completion (planned in a future version of the DimSum platform)</li> <li>Workflow Monitoring and Management: Monitor and manage workflows.</li> <li>Versatile Workflow Definition Language: Support Directed Acyclic Graph (DAG) models.</li> <li>Dynamic Resource Allocation: Allocate resources dynamically.</li> <li>Retry Strategies: Implement retry strategies for task failures.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To begin using this project, please refer to the Installation Manual.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Detailed documentation is available to help you understand and use the project effectively:</p> <ul> <li>User Manual: A comprehensive guide for end-users on how to use the application.</li> <li>Software Design: Documentation detailing the API specifications and usage.</li> <li>API Design: Documentation detailing the API specifications and usage.</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>For a detailed description of the API, see the API Specifications.</p>"},{"location":"#configuration-and-deployment","title":"Configuration and Deployment","text":""},{"location":"#configuration-instructions-and-deployment-guides-are-provided","title":"Configuration instructions and deployment guides are provided:","text":"<ul> <li>Installation Manual: comprehensive guidance on how to install and set up Argo Workflows.</li> <li>Helm Charts: Guidelines on using and configuring Helm charts for Kubernetes deployment.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Explore practical examples to better understand how to integrate and use the project's features:</p> <ul> <li>Example Usage</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<p>The component relies on the open source software Argo Workflows (Copyright 2017-2018 The Argo Authors) licensed under the Apache License, Version 2.0 (the \"License\").</p> <p>You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>All work developed within the context of the OHDSA project, including modifications and extensions to the original components, is licensed under the EUROPEAN SPACE AGENCY COMMUNITY LICENSE \u2013 V2.4 PERMISSIVE (TYPE 3).</p>"},{"location":"docker/","title":"Docker Image Deployment for [Project Name]","text":""},{"location":"docker/#overview","title":"Overview","text":"<p>This document outlines the Docker image configuration and deployment instructions for [Component Name]. It includes details on building the Docker image, running containers, and handling common configurations.</p>"},{"location":"docker/#building-the-docker-image","title":"Building the Docker Image","text":"<p>To build the Docker image, navigate to the directory containing the <code>Dockerfile</code> and run the following command:</p> <pre><code>docker build -t [your-image-name]:[tag] .\n</code></pre> <p>Replace <code>[your-image-name]</code> with the appropriate name for your Docker image and <code>[tag]</code> with the desired version or tag.</p>"},{"location":"docker/#running-the-docker-container","title":"Running the Docker Container","text":"<p>Once the image is built, you can run a container based on that image with the following command:</p> <pre><code>docker run -d --name [your-container-name] -p [port]:[port] [your-image-name]:[tag]\n</code></pre> <p>Replace <code>[your-container-name]</code> with a name for your container, <code>[port]</code> with the appropriate port numbers, and <code>[your-image-name]:[tag]</code> with the image name and tag used in the build step.</p>"},{"location":"docker/#additional-resources","title":"Additional Resources","text":"<ul> <li>Docker Documentation</li> </ul>"},{"location":"examples/","title":"Examples for [Project Name]","text":""},{"location":"examples/#overview","title":"Overview","text":"<p>This directory contains various examples to help you understand and utilize the functionalities of [Project Name]. Each example is self-contained and illustrates specific use cases or features of the software.</p>"},{"location":"examples/#list-of-examples","title":"List of Examples","text":"<p>Below are brief descriptions of the examples included in this directory:</p>"},{"location":"examples/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<ul> <li>Description: Demonstrates the basic functions of the application.</li> <li>Path: <code>basic_usage/</code></li> </ul>"},{"location":"examples/#example-2-advanced-features","title":"Example 2: Advanced Features","text":"<ul> <li>Description: Showcases the advanced features and how to configure them for custom use.</li> <li>Path: <code>advanced_features/</code></li> </ul>"},{"location":"examples/#example-3-integration","title":"Example 3: Integration","text":"<ul> <li>Description: Illustrates how to integrate [Project Name] with other software or services.</li> <li>Path: <code>integration/</code></li> </ul>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<p>To run an example, navigate to the example's directory and follow the instructions specified in the <code>README.md</code> file of that directory. Generally, you can run an example using the following commands:</p> <pre><code>cd [example-directory]\n./run_example.sh\n</code></pre>"},{"location":"helm_charts/","title":"Helm Charts for [Project Name]","text":""},{"location":"helm_charts/#overview","title":"Overview","text":"<p>The official chart for Argo Workflows is used to deploy Argo Workflows. It is described and detailed in Argo Workflows Helm Documentation with a list of all properties.</p> <p>The present repository provides the specific parameters used for the DimSum platform. This document only details these parameters and those that need to be provided during installation (depending on the environment).</p> <p>The exact configuration and prerequisites required for the DimSum platform are detailed in the installation manual.</p>"},{"location":"helm_charts/#prerequisites","title":"Prerequisites","text":"<p>Before you can use this Helm chart, ensure you meet the following prerequisites: * Target cluster with Kubernetes 1.23+ and PV support is available * S3 Bucket must be deployed and a <code>secret</code> is created for providing the credentials. * Helm 3.8.0+ installed locally</p>"},{"location":"helm_charts/#installing-the-helm-chart","title":"Installing the Helm Chart","text":"<ol> <li> <p>Clone locally the present GitHub repository.</p> <pre><code>git clone https://github.com/argoproj/argo-helm.git /charts\n</code></pre> </li> <li> <p>Create a custom configuration file</p> </li> </ol> <p>The custom configuration file <code>custom-values.yaml</code> should specify the specific configuration for your deployment as described in section Helm Configuration Details</p> <p>Example content:</p> <pre><code>artifactRepositoryRef:\n  artifact-repositories:\n    annotations:\n      workflows.argoproj.io/default-artifact-repository: default-artifact-repository\n    default-artifact-repository:\n      s3:\n        bucket: test\n        insecure: true\n        endpoint: l-k8s01-master.spb.spacebel.be:30901\n        accessKeySecret:\n          name: minio-credentials\n          key: accessKey\n        secretKeySecret:\n          name: minio-credentials\n          key: secretKey\n</code></pre> <ol> <li> <p>Install the Helm chart with:</p> <p><pre><code>helm install [release-name] ./charts/dimsum-argo-workflows --namespace [namespace] -f custom-values.yaml \n</code></pre>    Replace <code>[release-name]</code> with a name for your Helm release, and <code>[namespace]</code> with the Kubernetes namespace where you want to deploy.</p> </li> </ol>"},{"location":"helm_charts/#uninstalling-the-chart","title":"Uninstalling the Chart","text":"<p>To remove the deployed Helm chart: <pre><code>helm delete [release-name] --namespace [namespace]\n</code></pre></p>"},{"location":"helm_charts/#helm-configuration-details","title":"Helm Configuration Details","text":"<p>This section details the parameters that need to be adapted for the environment on which Argo Workflows is deployed.</p> <p>The following parameters are expected to be provided: * artifactRepositoryRef: the default artifact repository must be configured with the endpoint and credentials of the S3 storage of the environment * server.ingress: the external domain name must be configured as a Kuberntes Ingress</p> <p>Additional parameters are detailed below for advanced configuration but are not expected to be overridden in nominal scenarios.</p>"},{"location":"helm_charts/#artifact-repository-configuration","title":"Artifact Repository Configuration","text":"<p>To configure the artifact repository, you need to set up the <code>artifactRepositoryRef</code> in the <code>values.yaml</code> file. This configuration defines where Argo Workflows will store its artifacts. </p> <p>By default, the workflow uses the artifact repositories defined by the config map <code>artifact-repositories</code> (specifying one or more repositories). Within this ConfigMap, the  default repository must be designated using the annotation workflows.argoproj.io/default-artifact-repository. </p> <p>The following configuration defines the details of the default repository.</p> <p>You need to specify the bucket name, endpoint, and the secrets for the access key and secret key. Here's how it works:</p> <ul> <li>Bucket: The name of the bucket where artifacts will be stored.</li> <li>Endpoint: The endpoint URL for the S3-compatible storage service.</li> <li>Access Key and Secret Key: These are used to authenticate with the storage service. You need to provide the secrets for these keys. The <code>accessKeySecret</code> and <code>secretKeySecret</code> fields specify the names and keys of the Kubernetes secrets containing the access key and secret key, respectively.</li> </ul> <pre><code>artifactRepositoryRef:\n  artifact-repositories:\n    annotations:\n      workflows.argoproj.io/default-artifact-repository: default-artifact-repository\n      default-artifact-repository:\n        s3:\n            bucket: test\n            insecure: true\n            endpoint: l-k8s01-master.spb.spacebel.be:30901 # TBD create an additional property\n            accessKeySecret:\n              name: minio-credentials\n              key: accessKey\n            secretKeySecret:\n              name: minio-credentials\n              key: secretKey\n</code></pre>"},{"location":"helm_charts/#ingress-parameters","title":"Ingress Parameters","text":"<pre><code>server:\n  ingress:\n    enabled: true\n    annotations:\n      kubernetes.io/ingress.class: nginx\n      # Add other necessary annotations here\n    hosts:\n      - argo.your-domain.com\n    paths:\n      - /\n    tls: \n      - secretName: argo-tls\n        hosts:\n          - argo.your-domain.com\n</code></pre>"},{"location":"helm_charts/#service-account-parameters","title":"Service Account Parameters","text":"<p>IMPORTANT: These parameters should only be modified by experienced users for advanced configurations. </p> <p>Each workflow is associated with a service account that dictates the permissions and actions the workflow can perform within the cluster. </p> <p>It is recommended to create a default service account <code>argo-workflow</code>, assign all required roles for workflows, and bind the role to that default service account.</p> <pre><code>workflow:\n  serviceAccount:\n    create: true\n    name: \"argo-workflow\"\n  rbac:\n    create: true\nserver:\n  rbac:\n    create: true\ncontroller:\n  workflowDefaults:\n    spec:\n      serviceAccountName: argo-workflow\n</code></pre>"},{"location":"helm_charts/#authentication-parameters","title":"Authentication Parameters","text":"<p>IMPORTANT: These parameters should only be modified by experienced users for advanced configurations.</p> <p>Authentication parameters define the authentication modes and settings for Argo Workflows. These parameters are specified in the values.yaml file.</p> <p>TBD section</p>"},{"location":"helm_charts/#parameters","title":"Parameters","text":"<p>The present section provides the definition of the properties overridden in the yaml file ../charts/values.yaml.</p>"},{"location":"helm_charts/#artefacts-repository-parameters","title":"Artefacts Repository Parameters","text":"Key Type Default Description artifactRepositoryRef object See example above Store artifact in a S3-compliant object store"},{"location":"helm_charts/#service-account-parameters_1","title":"Service Account Parameters","text":"Key Type Value Description workflow.serviceAccount.create bool <code>true</code> Specifies whether a service account should be created workflow.serviceAccount.name string <code>\"argo-workflow\"</code> Service account which is used to run workflows workflow.serviceAccount.pullSecrets list <code>[]</code>  (TBD) Secrets with credentials to pull images from a private registry. Same format as <code>.Values.images.pullSecrets</code> controller.workflowDefaults object <code>{    spec.serviceAccountName: argo-workflow}</code> Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level. Only valid for 2.7+"},{"location":"helm_charts/#authentication-parameters_1","title":"Authentication Parameters","text":"Key Type Default Description server.authModes list <code>[]</code> A list of supported authentication modes. Available values are <code>server</code>, <code>client</code>, or <code>sso</code>. If you provide sso, please configure <code>.Values.server.sso</code> as well."},{"location":"helm_charts/#ingress-configuration","title":"Ingress Configuration","text":"Key Type Default Description server.serviceType string <code>\"ClusterIP\"</code> Service type for server pods server.serviceNodePort string <code>nil</code> Service node port"},{"location":"helm_charts/#additional-resources","title":"Additional Resources","text":"<ul> <li>Helm Documentation</li> <li>Kubernetes Documentation</li> </ul>"},{"location":"installation_manual/","title":"Installation Manual","text":""},{"location":"installation_manual/#overview","title":"Overview","text":"<p>This installation manual provides detailed instructions for the installation and configuration of the Argo Workflows component as part of the DimSum platform. </p> <p>The DimSum platform's comprehensive setup is documented in the related platform installation guide. This manual focuses specifically on Argo Workflows, facilitating its reuse as an independent component or enabling advanced configuration within the system.</p>"},{"location":"installation_manual/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the installation, ensure that the following prerequisites are met and properly configured on your target Kubernetes cluster:</p> <ul> <li>Helm: Helm is required for deploying the Argo Workflows component on Kubernetes. Ensure that Helm is installed and configured on your system.</li> <li>kubectl: The <code>kubectl</code> command-line tool must be installed and configured with access to your target Kubernetes cluster (<code>kubeconfig</code>).</li> <li>S3 Storage: The DimSum platform relies on S3 storage to store the artifacts used by the workflows. Ensure that your S3 storage is configured and accessible.</li> <li>Domain Name or External IP: The Argo Workflows component requires a domain name or an external IP address that can be bound to the Argo Workflows server.</li> <li>Argo CLI: The Argo Command Line Interface (CLI) is useful for interacting with Argo Workflows. It can be downloaded from the official Argo Workflows GitHub repository: https://github.com/argoproj/argo-workflows/releases/</li> </ul>"},{"location":"installation_manual/#build-procedure","title":"Build Procedure","text":"<p>No build process is required.</p>"},{"location":"installation_manual/#deployment","title":"Deployment","text":"<p>This project uses Helm for deployment on Kubernetes. </p> <p>Make sure Helm is installed and set up correctly then follow the instructions in Helm Charts Manual</p> <p>You can ensure that the installation was successful, by submitting a workflow to Argo:</p> <pre><code>argo -n argo-helm submit --serviceaccount executor https://raw.githubusercontent.com/argoproj/argo-workflows/master/examples/hello-world.yaml --watch\n</code></pre>"},{"location":"installation_manual/#configuration","title":"Configuration","text":"<p>This section provides guidelines for advanced setup related to Argo Workflows.</p>"},{"location":"installation_manual/#label-kubernetes-nodes","title":"Label Kubernetes nodes","text":"<p>NOTE: TBD detail this section</p> <p>Labels associated with nodes are used by Argo Workflows and Kubernetes to choose on which node a pods must be scheduled. Here is an example of how to associate a label with a node: <pre><code>kubectl label nodes &lt;node-name&gt; accelerator=&lt;my-label&gt;\n</code></pre></p> <p>To list labels associated with nodes: <pre><code>kubectl get nodes --show-labels=true\n</code></pre></p>"},{"location":"installation_manual/#uninstall","title":"Uninstall","text":"<pre><code>helm uninstall --namespace argo-helm argo-workflows\nkubectl delete namespace argo-helm\n</code></pre>"},{"location":"kubernetes/","title":"Kubernetes Configuration for [Project Name]","text":""},{"location":"kubernetes/#overview","title":"Overview","text":"<p>This document provides an overview of the Kubernetes configurations used for deploying and managing [Component Name]. It includes details on deployment manifests, service definitions, and other Kubernetes resources critical to the operation of the component.</p>"},{"location":"kubernetes/#configuration-files","title":"Configuration Files","text":"<p>Below is a list of key Kubernetes configuration files included in this directory:</p> <ul> <li><code>deployment.yaml</code>: Defines the Deployment configuration for [Component Name].</li> <li><code>service.yaml</code>: Specifies the Service configuration that provides network access to the component.</li> <li><code>ingress.yaml</code>: (Optional) Manages external access to the services, typically via HTTP.</li> <li><code>configmap.yaml</code>: Contains non-confidential data in key-value pairs that can be consumed by pods.</li> <li><code>secret.yaml</code>: Manages sensitive information, such as passwords and API keys, used by the component.</li> </ul>"},{"location":"kubernetes/#deployment","title":"Deployment","text":"<p>To deploy the component to a Kubernetes cluster, use the following command:</p> <pre><code>kubectl apply -f deployment.yaml\n</code></pre>"},{"location":"kubernetes/#services","title":"Services","text":"<p>To establish the network access to the component, apply the service configuration:</p> <pre><code>kubectl apply -f service.yaml\n</code></pre>"},{"location":"kubernetes/#scaling","title":"Scaling","text":"<p>To scale the component, adjust the <code>replicas</code> field in the <code>deployment.yaml</code> file or use the following command:</p> <pre><code>kubectl scale deployment [deployment-name] --replicas=[number]\n</code></pre>"},{"location":"kubernetes/#updating","title":"Updating","text":"<p>To update the component, modify the relevant Docker image or configuration and apply the changes:</p> <pre><code>kubectl apply -f deployment.yaml\n</code></pre>"},{"location":"kubernetes/#cleanup","title":"Cleanup","text":"<p>To remove the deployed resources from your cluster, use:</p> <pre><code>kubectl delete -f .\n</code></pre>"},{"location":"kubernetes/#additional-resources","title":"Additional Resources","text":"<ul> <li>Kubernetes Documentation</li> <li>Helm Charts for managing complex deployments with Helm.</li> </ul>"},{"location":"tutorial_cli/","title":"CLI Tutorial","text":""},{"location":"tutorial_cli/#argo-cli","title":"Argo CLI","text":"<p>This tutorial demonstrates how to use the Argo CLI to register a workflow template, submit a workflow, monitor its status, and retrieve logs and results.</p>"},{"location":"tutorial_cli/#installing-argo-cli","title":"Installing Argo CLI","text":"<p>The Argo CLI is a command-line interface tool that allows users to interact with Argo Workflows. Follow the steps below to install the Argo CLI:</p> <ol> <li>Download the latest Argo CLI release:</li> </ol> <p>Visit the Argo Workflows GitHub releases page and download the appropriate binary for your operating system.</p> <p><code>Example: curl -sLO https://github.com/argoproj/argo-workflows/releases/latest/download/argo-linux-amd64.gz</code></p> <ol> <li>Unzip the downloaded binary:</li> </ol> <p>Unzip the downloaded binary.</p> <p><code>Example: gunzip argo-linux-amd64.gz</code></p> <ol> <li>Make the binary executable:</li> </ol> <p>Make the unzipped binary executable.</p> <p><code>Example: chmod +x argo-linux-amd64</code></p> <ol> <li>Move the binary to your PATH:</li> </ol> <p>Move the executable binary to a directory included in your system's <code>PATH</code>.</p> <p><code>Example: mv argo-linux-amd64 /usr/local/bin/argo</code></p> <ol> <li>Verify the installation:</li> </ol> <p>Confirm that the Argo CLI has been installed correctly by running the version command.</p> <p><code>Example: argo version</code></p>"},{"location":"tutorial_cli/#workflow-template-operations","title":"Workflow Template operations","text":"<p>The main operations required to manage Workflow Templates are the following:</p> <ul> <li>List Workflow templates: <code>argo template list</code></li> <li>Create Workflow template: <code>argo -n &lt;k8s-namespace&gt; template create &lt;workflow-template-definition.yaml&gt;</code></li> <li>Delete workflow template: <code>argo -n &lt;k8s-namespace&gt; template delete &lt;workflow-template-name&gt;</code></li> </ul>"},{"location":"tutorial_cli/#workflow-operations","title":"Workflow operations","text":"<p>The main operations required to manage Workflows are the following:</p> <ul> <li>List Workflows: <code>argo -n &lt;k8s-namespace&gt; list</code></li> <li>Submit a workflow: <code>argo -n &lt;k8s-namespace&gt; submit &lt;workflow-definition.yaml&gt;</code></li> <li>Get status of a workflow: <code>argo -n &lt;k8s-namespace&gt; get &lt;workflow-name&gt;</code></li> <li>Retrieve logs of a workflow: <code>argo -n &lt;k8s-namespace&gt; logs &lt;workflow-name&gt;</code></li> <li>Delete a workflow: <code>argo -n &lt;k8s-namespace&gt; delete &lt;workflow-name&gt;</code></li> </ul>"},{"location":"tutorial_cli/#register-a-workflow-template","title":"Register a Workflow template","text":"<p>To register a workflow template, we first need to define one.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\nname: hello-world-wft\nspec:\nentrypoint: whalesay\ntemplates:\n- name: whalesay\ninputs:\nparameters:\n- name: message\ncontainer:\nimage: docker/whalesay\ncommand: [ cowsay ]\nargs: [\"{{inputs.parameters.message}}\"]\n</code></pre> <p>See: hello-world-wf-template.yml</p> <p>After defining a workflow template in a YAML file, we can register it using the Argo CLI. To proceed, open a new terminal window at the root of this project and execute the following command:</p> <pre><code>argo -n &lt;k8s-namespace&gt; template create examples/hello-world-template/hello-world-wf-template.yml\n</code></pre> <p>After registering the workflow template, verify its registration with the following command: <pre><code>argo -n &lt;k8s-namespace&gt; template list\n</code></pre></p>"},{"location":"tutorial_cli/#submit-a-workflow","title":"Submit a workflow","text":"<p>Define the workflow in a YAML file as shown below.</p> <p><pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-workflow-\nspec:\n  arguments:\n    parameters:\n      - name: message\n        value: This is a whale message # Message to pass to the template\n  # Reference the WorkflowTemplate\n  workflowTemplateRef:\n    name: hello-world-wft\n</code></pre> See: hello-world-wf.yml</p> <p>Once we have defined a workflow in a YAML file, we can submit it by calling ARGO CLI. To proceed, open a new terminal windows at the root of this project. Then execute the following command:</p> <pre><code>argo -n &lt;k8s-namespace&gt; submit examples/hello-world-template/hello-world-wf.yml\n</code></pre>"},{"location":"tutorial_cli/#monitor-a-workflow","title":"Monitor a workflow","text":"<p>In order to monitor a submitted workflow, we can retrieve its status with the following command: <pre><code>argo -n &lt;k8s-namespace&gt; get &lt;workflow-name&gt;\n</code></pre> Note that <code>&lt;workflow-name&gt;</code> correspond to the name of the workflow (available in the response to the submit workflow operation). In this example, it corresponds to: hello-world-workflow-qltqd</p> <p>In the response, we can observe that <code>Status</code> is <code>Succeeded</code>. meaning that the execution of this workflow terminated successfully.</p> <p>To retrieve the logs associated with this workflow, one can use the following command: <pre><code>argo -n &lt;k8s-namespace&gt; get &lt;workflow-name&gt;\n</code></pre></p>"},{"location":"tutorial_gui/","title":"GUI Tutorial","text":""},{"location":"tutorial_gui/#web-gui-tutorial","title":"WEB GUI Tutorial","text":"<p>In this tutorial we will see how we can leverage the graphical user interface to manage workflow templates, submit and monitor workflows.</p> <p>The first step is to open a web browser and open the url on which your argo workflow web server is exposed. From the Home page you can use the navigation menu displayed on the left to choose the desired section.</p> <p></p> <p>In our case we will select the 'Workflow Templates tab'. From this page we can see the different workflow templates available, create new ones, delete old ones,....</p> <p></p> <p>After deploying the desired workflow template, you can choose the 'Workflow' page (still from the navigation menu on the left).</p> <p></p> <p>From this page, we can see the workflows already submitted, submit a new one, monitor a workflow, delete one,...</p> <p>Now, if we click on a workflow we can see its status, this section can also help us to monitor a workflow during its execution.</p> <p></p> <p>By clicking on the output artifact 'results.txt' we can see its content and download it.</p> <p></p> <p>This concludes this tutorial. You can find more information online on: Argo Workflow - User Guide</p>"},{"location":"tutorial_rest/","title":"REST Tutorial","text":""},{"location":"tutorial_rest/#rest-tutorial","title":"REST Tutorial","text":"<p>In this section, we will see how to use the REST API for managing workflows. For more information on those operations,see API design documentation: API</p> <p>Note: Authentication aspects are not covered in this tutorial. See https://argo-workflows.readthedocs.io/en/latest/argo-server-auth-mode/</p> <p>Postman collection of requests used in this tutorial: Postman DEMO Collection</p> <p>Open the Postman collection HELLOWOLRD-TEMPLATE-EXAMPLE</p>"},{"location":"tutorial_rest/#register-a-workflow-template","title":"Register a Workflow template","text":"<p>In this tutorial, we will use the template located at hello-world-wf-template.yml</p> <p>Once we have defined a workflow template in a YAML file, we can register this template by calling the REST API.</p> <p>Open the Postman collection and select the request located in Template/ Create Workflow Template:</p> <p></p>"},{"location":"tutorial_rest/#submit-a-workflow","title":"Submit a Workflow","text":"<p>Open the Postman request located in Workflow / Submit Workflow:</p> <p></p>"},{"location":"tutorial_rest/#monitor-a-workflow","title":"Monitor a workflow","text":"<p>The REST API provide an operation to retrieve the status of a Workflow including the status of the pods deployed on the Kubernetes cluster.</p> <p>Open the Postman collection and select the request located in Workflow/ Get Status:</p> <p></p> <p>To retrieve the logs associated with a workflow, a REST Operation also exists.</p> <p>Open the Postman collection and select the request located in Workflow/ Get LOGS:</p> <p></p>"},{"location":"user_manual/","title":"User Manual for Argo Workflow","text":""},{"location":"user_manual/#purpose-of-the-software","title":"Purpose of the Software","text":"<p>Argo Workflows is an open-source engine tailored for orchestrating workflows on Kubernetes, enabling users to define, manage, and execute tasks in containers.</p>"},{"location":"user_manual/#operations-environment","title":"Operations Environment","text":"<p>Argo Workflows is executed on a Kubernetes cluster, on which it orchestrates the containers required fo executing the desired workflows.</p>"},{"location":"user_manual/#hardware-configuration","title":"Hardware Configuration","text":"<p>Argo Workflows requires a Kubernetes cluster to operate effectively. </p>"},{"location":"user_manual/#operations-manual","title":"Operations Manual","text":""},{"location":"user_manual/#set-up-and-initialization","title":"Set-up and Initialization","text":"<p>The set-up procedures are described in the Installation Manual.</p>"},{"location":"user_manual/#normal-operations","title":"Normal Operations","text":"<p>The normal operations of the Argo Workflows engine are thoroughly documented on the official Argo Workflows documentation site, with detailed references to various operational fields. </p> <p>The resources below aim to complement the official documentation with guidance for performing activities expected within the DimSum platform such as composing workflows, executing workflows through the REST API, and monitoring workflows with the GUI.</p>"},{"location":"user_manual/#argo-workflows-key-concepts","title":"Argo Workflows Key Concepts","text":"<p>With Argo Workflows, three key concepts are essential: Workflow, template, and Workflow Template.</p> <ul> <li>Workflow: A structured sequence of tasks that defines the steps to be executed. It includes a specification (<code>spec</code>) of the tasks, a current status (<code>state</code>), and may reference a <code>WorkflowTemplate</code>.</li> <li>template: A task within a workflow that acts as a function or method.</li> <li>Workflow Template: A predefined definition of a workflow that can be persisted, submitted, or referenced within other workflows.</li> </ul> <pre>af3b68e3-f142-4c10-b077-49719d71a485</pre>"},{"location":"user_manual/#workflow-composition","title":"Workflow Composition","text":"<p>The Workflow Design Manual explains the main Argo Workflows concepts and provides guidance on designing workflows that include features relevant to the platform, such as workflow templates, artefacts, and retry strategies.</p>"},{"location":"user_manual/#rest-api-operations","title":"REST API Operations","text":"<p>This section outlines the REST API operations executed by the DimSum platform components such as the Workflow Management System (WMTS).</p> <p>The API Design Document provides the details about the REST API operations referenced below..</p> <p>The sequence of operations performed by the platform consists in the following steps: </p> <ol> <li>The WMTS submits the Argo Workflows Template to the Module Registry microservice.</li> <li>The Module Registry registers the workflow template in the DimSum namespace using the Argo Workflows</li> <li>The WMTS submits the Workflow using the REST API. </li> <li>The WMTS monitors the Workflow until it is complete.</li> <li>Upon completion, the WMTS retrieves the results.</li> <li>If the execution fails, the WMTS retrieves logs from an operator.</li> </ol> <pre>1c10f51e-c290-44e3-a66b-d11d1cd4a117</pre>"},{"location":"user_manual/#registration-of-the-workflow-template","title":"Registration of the Workflow Template","text":"<p>To register a workflow template, send a POST request to the Argo Workflows API with the template definition.</p> <pre><code>curl -X POST \"https://your-argo-server/api/v1/workflow-templates/{namespace}\" -H \"Content-Type: application/json\" -d '{\n  \"template\": {\n    // your workflow template JSON here\n  }\n}'\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and provide your workflow template JSON in the <code>-d</code> parameter.</p>"},{"location":"user_manual/#submit-the-workflow","title":"Submit the Workflow","text":"<p>To submit the workflow for execution, send a POST request to the Argo Workflows API.</p> <pre><code>curl -X POST \"https://your-argo-server/api/v1/workflows/{namespace}/{name}/submit\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and <code>{name}</code> with the workflow name.</p>"},{"location":"user_manual/#monitor-the-status","title":"Monitor the Status","text":"<p>To monitor the status of the submitted workflow until it completes or get the progress and status of individual steps (tasks), send a GET request to the Argo Workflows API.</p> <pre><code>curl -X GET \"https://your-argo-server/api/v1/workflows/{namespace}/{name}\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and <code>{name}</code> with the workflow name.</p> <p>The response will include detailed information about the workflow, such as:</p> <ul> <li><code>status.phase</code>: The overall phase of the workflow (e.g., Running, Succeeded, Failed).</li> <li><code>status.startedAt</code>: The timestamp when the workflow started.</li> <li><code>status.finishedAt</code>: The timestamp when the workflow finished (if applicable).</li> <li><code>status.nodes</code>: An object containing the status of each node (step) in the workflow, including:</li> <li><code>name</code>: The name of the node.</li> <li><code>phase</code>: The phase of the node (e.g., Running, Succeeded, Failed).</li> <li><code>startedAt</code>: The timestamp when the node started.</li> <li><code>finishedAt</code>: The timestamp when the node finished (if applicable).</li> <li><code>outputs</code>: Any outputs produced by the node.</li> </ul>"},{"location":"user_manual/#retrieve-the-results","title":"Retrieve the Results","text":"<p>To retrieve the results of the completed workflow, send again a GET request to the Argo Workflows API when the workflow is completed.</p> <pre><code>curl -X GET \"https://your-argo-server/api/v1/workflows/{namespace}/{name}\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and <code>{name}</code> with the workflow name.</p> <p>The response will include the outputs produced by the workflow, which can be found in the <code>status.outputs</code> field. This field contains:</p> <ul> <li><code>artifacts</code>: An array of artifacts produced by the workflow, each with:</li> <li><code>name</code>: The name of the artifact.</li> <li><code>path</code>: The path where the artifact is stored.</li> <li><code>s3</code>: S3 bucket details if the artifact is stored in an S3 bucket.</li> <li><code>parameters</code>: Any parameters produced by the workflow, each with:</li> <li><code>name</code>: The name of the parameter.</li> <li><code>value</code>: The value of the parameter.</li> <li><code>value</code>: The value of the parameter.</li> </ul>"},{"location":"user_manual/#retrieve-the-logs","title":"Retrieve the Logs","text":"<p>If the workflow execution fails, retrieve the logs to diagnose the failure by sending a GET request to the Argo Workflows API.</p> <pre><code>curl -X GET \"https://your-argo-server/api/v1/workflows/{namespace}/{name}/logs\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and <code>{name}</code> with the workflow name.</p>"},{"location":"user_manual/#manually-delete-a-workflow","title":"Manually Delete a Workflow","text":"<p>To manually delete a workflow, send a DELETE request to the Argo Workflows API.</p> <pre><code>curl -X DELETE \"https://your-argo-server/api/v1/workflows/{namespace}/{name}\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and <code>{name}</code> with the workflow name.</p>"},{"location":"user_manual/#suspend-a-workflow","title":"Suspend a Workflow","text":"<p>To suspend a workflow, send a PUT request to the Argo Workflows API. Suspending a workflow pauses its execution, and it can be resumed later.</p> <pre><code>curl -X PUT \"https://your-argo-server/api/v1/workflows/{namespace}/{name}/suspend\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and <code>{name}</code> with the workflow name.</p>"},{"location":"user_manual/#resume-a-workflow","title":"Resume a Workflow","text":"<p>To resume a previously stopped or suspended workflow, send a PUT request to the Argo Workflows API.</p> <pre><code>curl -X PUT \"https://your-argo-server/api/v1/workflows/{namespace}/{name}/resume\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and <code>{name}</code> with the workflow name.</p>"},{"location":"user_manual/#stop-or-terminate-a-workflow","title":"Stop or Terminate a Workflow","text":"<p>To stop a workflow, send a PUT request to the Argo Workflows API. Stopping a workflow will halt its execution but execute all exit handlers.</p> <pre><code>curl -X PUT \"https://your-argo-server/api/v1/workflows/{namespace}/{name}/stop\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and <code>{name}</code> with the workflow name.</p> <p>To terminate a workflow, send a PUT request to the Argo Workflows API. Terminating a workflow immediately stops its execution and marks it as terminated. Exit handlers are not executed.</p> <pre><code>curl -X PUT \"https://your-argo-server/api/v1/workflows/{namespace}/{name}/terminate\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and <code>{name}</code> with the workflow name.</p>"},{"location":"user_manual/#list-the-workflow-templates","title":"List the Workflow Templates","text":"<p>To list all workflow templates, send a GET request to the Argo Workflows API.</p> <pre><code>curl -X GET \"https://your-argo-server/api/v1/workflow-templates/{namespace}\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace. This will return a list of all registered workflow templates.</p>"},{"location":"user_manual/#retrieve-a-workflow-template","title":"Retrieve a Workflow Template","text":"<p>To retrieve a specific workflow template, send a GET request to the Argo Workflows API.</p> <pre><code>curl -X GET \"https://your-argo-server/api/v1/workflow-templates/{namespace}/{template-name}\"\n</code></pre> <p>Replace <code>{namespace}</code> with your namespace and <code>{template-name}</code> with the name of the workflow template you want to retrieve.</p> <p>By following these instructions, you can effectively manage your workflows using the Argo Workflow REST API.</p>"},{"location":"user_manual/#workflow-notifications","title":"Workflow Notifications","text":"<p>Argo Workflows does not natively support notification mechanisms for status updates, except through implementing a custom container in an exit handler to handle such notifications. However, this functionality can be considered in phase 2 of the project.</p> <pre>6ed88368-5677-46a5-9e06-50669424c497</pre> <p>See Workflow Notifications Documentation for additional details.</p>"},{"location":"user_manual/#gui-operations","title":"GUI Operations","text":"<p>This section provides instructions on how to perform various operations using the Argo Workflow GUI.</p>"},{"location":"user_manual/#list-workflows-running-or-complete","title":"List Workflows (Running or Complete)","text":"<ol> <li>Open the Argo Workflows GUI.</li> <li>Navigate to the \"Workflows\" tab.</li> <li>Here, you will see a list of all workflows, including both running and completed ones.</li> <li>You can use filters to refine the list based on workflow status, name, and other criteria.</li> </ol>"},{"location":"user_manual/#display-workflow-status","title":"Display Workflow Status","text":"<ol> <li>In the Argo Workflows GUI, navigate to the \"Workflows\" tab.</li> <li>Click on the workflow for which you want to view the status.</li> <li>The workflow details page will open, displaying the current status at the top (e.g., Running, Succeeded, Failed).</li> </ol>"},{"location":"user_manual/#monitor-workflow-steps","title":"Monitor Workflow Steps","text":"<ol> <li>From the workflow details page in the Argo Workflows GUI, you can see the graphical representation of the workflow.</li> <li>Each step in the workflow is displayed as a node in the graph.</li> <li>Click on any node to view the details of that step, including logs, outputs, and the current status.</li> </ol>"},{"location":"user_manual/#retrieve-workflow-logs","title":"Retrieve Workflow Logs","text":"<ol> <li>In the workflow details page, click on the specific node (step) you are interested in.</li> <li>A sidebar will appear showing details about that step.</li> <li>Click on the \"Logs\" tab in the sidebar to view the logs for that specific step.</li> </ol>"},{"location":"user_manual/#retrieve-workflow-outputs","title":"Retrieve Workflow Outputs","text":"<ol> <li>Navigate to the workflow details page in the Argo Workflows GUI.</li> <li>At the bottom of the page, you will see a section labeled \"Outputs.\"</li> <li>This section lists the outputs produced by the workflow. You can click on each output to view more details or download the data.</li> </ol>"},{"location":"user_manual/#manually-delete-a-workflow_1","title":"Manually Delete a Workflow","text":"<ol> <li>Open the Argo Workflows GUI and navigate to the \"Workflows\" tab.</li> <li>Find the workflow you want to delete in the list.</li> <li>Click the three-dot menu (ellipsis) next to the workflow.</li> <li>Select \"Delete\" from the dropdown menu.</li> <li>Confirm the deletion in the prompt that appears.</li> </ol>"},{"location":"user_manual/#normal-termination","title":"Normal Termination","text":"<p>To cease or interrupt the use of Argo Workflow, users can terminate their workflows through the Argo REST API or Argo UI. Using the Argo REST API, users can send a POST request to the <code>/api/v1/workflows/{namespace}/{name}/terminate</code> endpoint to terminate a workflow or to the <code>/api/v1/workflows/{namespace}/{name}/stop</code> endpoint to stop a workflow. Additionally, the Argo UI provides an intuitive interface to manage and halt workflows.</p> <p>To verify if the termination has been normal, users should check the status of the workflow. A normally terminated workflow will have a status of <code>Succeeded</code> or <code>Failed</code>, indicating that the workflow has completed its execution or encountered an error during the process. Users can inspect the workflow status via the Argo REST API by sending a GET request to the <code>/api/v1/workflows/{namespace}/{name}</code> endpoint or through the Argo UI.</p>"},{"location":"user_manual/#example-terminate-a-workflow-using-rest-api","title":"Example: Terminate a Workflow Using REST API","text":"<p>To terminate a workflow named <code>example-workflow</code> in the <code>default</code> namespace, users can send the following POST request:</p> <pre><code>curl -X POST \"https://your-argo-server/api/v1/workflows/default/example-workflow/terminate\"\n</code></pre>"},{"location":"user_manual/#example-check-workflow-status-using-rest-api","title":"Example: Check Workflow Status Using REST API","text":"<p>To check the status of a workflow named example-workflow in the default namespace, users can send the following GET request:</p> <p>```sh curl -X GET \"https://your-argo-server/api/v1/workflows/default/example-workflow\" ```` </p>"},{"location":"user_manual/#error-conditions","title":"Error Conditions","text":"<p>Common error conditions in Argo Workflows include:</p>"},{"location":"user_manual/#workflow-fails-to-start","title":"Workflow Fails to Start","text":"<p>This may be due to incorrect workflow definitions or missing dependencies. Detection methods include checking the Argo CLI or UI for error messages indicating validation issues.</p> <p>Troubleshooting Steps:   - Verify the workflow YAML file for syntax errors.   - Ensure all required parameters and artifacts are correctly defined.   - Use the <code>argo lint</code> command to validate the workflow definition.</p>"},{"location":"user_manual/#pod-failures","title":"Pod Failures","text":"<p>Pods may fail to start or run due to resource constraints, image pull errors, or misconfigurations. Detection involves monitoring pod status in the Kubernetes dashboard or using the Argo CLI.</p> <p>Troubleshooting Steps:   - Check pod logs using <code>kubectl logs &lt;pod-name&gt;</code> for detailed error messages.   - Ensure the container images are accessible and correctly specified.   - Verify resource requests and limits to ensure the cluster can accommodate the pod.</p>"},{"location":"user_manual/#timeouts","title":"Timeouts","text":"<p>Workflows or specific steps may time out if they exceed the allotted execution time. Detection methods include checking the workflow status for timeout errors.</p> <p>Troubleshooting Steps:   - Review and adjust the timeout settings in the workflow definition.   - Optimize the workflow steps to reduce execution time.   - Ensure external services or dependencies are responsive and not causing delays.</p>"},{"location":"user_manual/#permission-denied-errors","title":"Permission Denied Errors","text":"<p>These errors occur when the workflow does not have the necessary permissions to perform certain actions. Detection involves examining the error messages related to access issues.</p> <p>Troubleshooting Steps:   - Verify the service account permissions and roles assigned to the workflow.   - Update the role-based access control (RBAC) settings to grant the required permissions.   - Check and correct any file or directory permissions within the workflow.</p>"},{"location":"user_manual/#resource-quota-exceeded","title":"Resource Quota Exceeded","text":"<p>Workflows may fail if they exceed the assigned resource quotas in the cluster. Detection involves checking for resource-related error messages in the workflow status.</p> <p>Troubleshooting Steps: - Review the resource quotas assigned to the namespace and adjust if necessary. - Optimize workflow resource usage to stay within the quota limits. - Consult with the cluster administrator to request increased resource allocations if needed.</p>"},{"location":"user_manual/#recover-runs","title":"Recover Runs","text":"<p>In the event of workflow failures or interruptions, Argo Workflows provides mechanisms to restart or recover runs, ensuring continuity and minimizing downtime.</p>"},{"location":"user_manual/#retry-failed-steps","title":"Retry Failed Steps","text":"<p>Argo allows users to retry failed steps within a workflow. This can be configured in the workflow YAML by specifying the <code>retryStrategy</code>. Users can define the number of retries and the backoff strategy.</p> <p>Procedure: - Edit the workflow definition to include a <code>retryStrategy</code> for the relevant steps. - Resubmit the workflow using the Argo REST API by sending a POST request to <code>/api/v1/workflows/{namespace}</code> with the updated YAML file.</p>"},{"location":"user_manual/#resume-suspended-workflows","title":"Resume Suspended Workflows","text":"<p>If a workflow is manually or automatically suspended, it can be resumed using the Argo REST API.</p> <p>Procedure: - Send a PUT request to the Argo REST API to resume a suspended workflow:</p> <p><code>sh   curl -X PUT \"https://your-argo-server/api/v1/workflows/{namespace}/{name}/resume\"</code></p> <ul> <li>Verify the workflow status to ensure it continues from the point of suspension.</li> </ul>"},{"location":"user_manual/#resubmit-failed-workflows","title":"Resubmit Failed Workflows","text":"<p>Entire workflows that have failed can be resubmitted. This can be useful if the failure was due to transient issues or if changes have been made to the environment.</p> <p>Procedure: - Identify the failed workflow by sending a GET request to list workflows:</p> <p><code>sh   curl -X GET \"https://your-argo-server/api/v1/workflows/{namespace}\"</code></p> <ul> <li>Resubmit the workflow by sending a POST request to the Argo REST API:</li> </ul> <p><code>sh   curl -X POST \"https://your-argo-server/api/v1/workflows/{namespace}/{name}/resubmit\"</code></p> <ul> <li>Monitor the workflow status to ensure it runs to completion.</li> </ul>"},{"location":"user_manual/#workflow-archiving-and-retrieval","title":"Workflow Archiving and Retrieval","text":"<p>Argo Workflows can be configured to archive completed workflows. Archived workflows can be retrieved and resubmitted if needed.</p> <p>Procedure: - Ensure workflow archiving is enabled in the Argo configuration. - Retrieve archived workflows by sending a GET request to the Argo REST API:</p> <p><code>sh   curl -X GET \"https://your-argo-server/api/v1/workflows/{namespace}/archived/{name}\"</code></p> <ul> <li>Resubmit the archived workflow by sending a POST request to the Argo REST API:</li> </ul> <p><code>sh   curl -X POST \"https://your-argo-server/api/v1/workflows/{namespace}\"</code></p>"},{"location":"user_manual/#handling-emergency-situations","title":"Handling Emergency Situations","text":"<p>In emergencies, such as cluster failures or critical resource shortages, maintaining workflow continuity is crucial.</p> <p>Procedure: - Implement backup and restore strategies for the Argo Workflows controller and associated databases. - Utilize high-availability (HA) configurations for critical components to minimize downtime. - Regularly test disaster recovery plans to ensure workflows can be recovered quickly.</p>"},{"location":"user_manual/#tutorials","title":"Tutorials","text":"<p>The following tutorials are provided for testing purposes to facilitate the learning of Argo Workflows:</p> <ul> <li>CLI Tutorial: Offers a brief overview of the main operations of the Argo CLI, a command-line interface that allows users to manage and interact with their workflows efficiently.</li> <li>REST API Tutorial: Demonstrates how to interact with the Argo Workflows through REST API calls, providing examples of common operations such as submitting, monitoring, and managing workflows.</li> <li>GUI Tutorial: Guides users through the graphical user interface of Argo Workflows, showcasing how to visually design, manage, and monitor workflows using the web-based interface.</li> </ul>"},{"location":"workflow_design_manual/","title":"Workflow Design Manual","text":""},{"location":"workflow_design_manual/#workflow-structure","title":"Workflow Structure","text":"<p>A Workflow is composed of several key components:</p> <ul> <li>kind: Defines the type or category of the workflow.</li> <li>metadata: Contains metadata information about the workflow, such as name, namespace, labels, and annotations.</li> <li>spec: The specification of the workflow, which includes detailed definitions of the workflow's behavior and structure.<ul> <li>entrypoint: The starting point or main task of the workflow.</li> <li>templates: A collection of tasks or functions to be executed within the workflow.</li> <li>arguments: Parameters or inputs required for the workflow to execute.</li> </ul> </li> </ul> <pre>7f75b261-017c-4c18-bf29-5507924a8291</pre>"},{"location":"workflow_design_manual/#templates-structure","title":"Templates Structure","text":"<p>In the context of a Workflow, the <code>templates</code> section defines the tasks to be executed. Each template can have the following components:</p> <ul> <li>name: The name of the template. This is a unique identifier for the template within the workflow.</li> <li>inputs: (Optional) Specifies the inputs required by the template. This can include parameters, artifacts, and other input resources.</li> <li>outputs: (Optional) Specifies the outputs produced by the template. This can include parameters, artifacts, and other output resources.</li> <li>type: (Optional) Indicates the type of template. Common types include:<ul> <li><code>container</code>: Specifies a container to run.</li> <li><code>steps</code>: Defines a set of sequential or parallel steps to be executed.</li> <li><code>dag</code>: Defines a Directed Acyclic Graph of tasks, allowing more complex workflows.</li> <li><code>resource</code>: Manages Kubernetes resources as part of the workflow.</li> <li><code>script</code>: Executes a script.</li> </ul> </li> </ul> <p>The following example shows the details of the container template with inputs and outputs.</p> <pre><code>templates:\n  - name: whalesay\n    container:\n      image: docker/whalesay\n      command: [ cowsay ]\n      args: [ \"hello world\" ]\n    inputs:\n      parameters:\n        - name: message\n          value: \"hello world\"\n    outputs:\n      artifacts:\n        - name: output\n          path: /path/to/output\n</code></pre> <p>In this example, the whalesay template is of type container. It specifies the container image, command, and arguments to run. The template also defines an input parameter (message) and an output artifact (output).</p>"},{"location":"workflow_design_manual/#workflow-parameter-and-argument","title":"Workflow Parameter and Argument","text":"<p>It is important to understand the distinction between parameters and arguments:</p> <ul> <li>Input Parameter: This is a definition of the inputs required by a task or function. For example, in a function definition <code>addFour(int a) {}</code>, <code>a</code> is an input parameter.</li> <li>Output Parameter: This is a definition of the output produced by a task or function. For example, in a function definition <code>int addFour(\u2026) {}</code>, the function returns an integer, which is an output parameter.</li> <li>Argument: This is the actual value provided to a parameter when a task or function is executed. For instance, if you call the function <code>addFour(4)</code>, the value <code>4</code> is the argument.</li> </ul> <p>The following example demonstrates how input parameters and arguments are used within a workflow:</p> <pre><code>spec:\n  entrypoint: whalesay\n  arguments:\n    parameters:\n      - name: message\n        value: hello world\n  templates:\n    - name: whalesay\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: docker/whalesay\n        command: [ cowsay ]\n        args: [ \"{{inputs.parameters.message}}\" ]\n</code></pre> <p>In this example:</p> <ul> <li>The <code>arguments</code> section at the <code>spec</code> level provides live parameter values. Here, the parameter named <code>message</code> is given the value <code>\"hello world\"</code>.</li> <li>The <code>templates</code> section defines a template named <code>whalesay</code>. This template has an input parameter named <code>message</code>.</li> <li>Within the container specification, the command to be run (<code>cowsay</code>) uses the argument provided to the <code>message</code> parameter (<code>\"hello world\"</code>), which is referenced as <code>{{inputs.parameters.message}}</code>.</li> </ul>"},{"location":"workflow_design_manual/#workflowtemplate","title":"WorkflowTemplate","text":"<p>A WorkflowTemplate is a reusable definition of a workflow that can be registered, persisted, and referenced by other workflows. This template defines a workflow's structure and behavior, making it possible to standardize and reuse workflow definitions across multiple workflows.</p> <p>Key Components:</p> <ul> <li>entrypoint: The main task or starting point of the workflow. This defines the initial action to be executed when the workflow starts.</li> <li>inputs: Parameters required by the entrypoint. These parameters define what inputs are necessary for the workflow to run.</li> <li>outputs: Parameters produced by the entrypoint. These parameters define the results or outputs generated by the workflow.</li> </ul> <p>The entrypoint, along with its input and output parameters, defines the external interface of the registerable <code>WorkflowTemplate</code>. This interface specifies what inputs are needed and what outputs will be produced, making it possible for other workflows to interact with this template.</p> <p>Below is a very basic example of a workflow template which defines an input parameter <code>message</code> which is passed as argument to the container <code>cowsay</code>:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: my-template\nspec:\n  entrypoint: whalesay-template\n  templates:\n    - name: whalesay-template\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: docker/whalesay\n        command: [cowsay]\n        args: [\"{{inputs.parameters.message}}\"]]\n</code></pre> <p>When a workflow references a <code>WorkflowTemplate</code>, it must provide the required parameter values as arguments. This ensures that all necessary inputs are available for the workflow to execute correctly.</p>"},{"location":"workflow_design_manual/#composition-with-a-single-workflowtemplate","title":"Composition with a Single WorkflowTemplate","text":"<p>In the example below, the above WorkflowTemplate <code>my-template</code> is called with <code>from workflow</code>as argument of the message parameter:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world\nspec:\n  arguments:\n    parameters:\n      - name: message\n        value: \"from workflow\u201c\n  workflowTemplateRef:\n    name: my-template\n</code></pre>"},{"location":"workflow_design_manual/#composition-with-multiple-workflowtemplates","title":"Composition with Multiple WorkflowTemplates","text":"<p>In more complex scenarios, you may need to compose workflows that leverage multiple <code>WorkflowTemplates</code>. This allows for modular design, enabling you to build sophisticated workflows by reusing and combining smaller, well-defined templates. Here's how you can compose workflows using multiple <code>WorkflowTemplates</code>.</p> <p>In this example, two <code>WorkflowTemplates</code>, <code>workflowtemplate-1</code> and <code>workflowtemplate-2</code>, are executed sequentially. The output of the first template can be passed as input to the second template.</p> <p>Workflow Composition:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: composed-workflow-\nspec:\n  entrypoint: composed-entrypoint\n  templates:\n    - name: composed-entrypoint\n      steps:\n        - - name: call-template-one\n            templateRef:\n              name: workflowtemplate-1\n              template: step-one\n            arguments:\n              parameters:\n                - name: input-param\n                  value: \"input for template one\"\n        - - name: call-template-two\n            templateRef:\n              name: workflowtemplate-2\n              template: step-two\n            arguments:\n              parameters:\n                - name: input-param\n                  value: \"{{steps.call-template-one.outputs.parameters.output-param}}\"\n</code></pre> <p>In this example, <code>call-template-one</code> executes first, and its output is used as the input for <code>call-template-two</code>.</p>"},{"location":"workflow_design_manual/#managing-artefacts","title":"Managing Artefacts.","text":"<p>WorkflowTemplates in Argo Workflows can define input and output artefacts to facilitate data management within workflows. An artefact in Argo Workflows can be either a file or a directory.</p> <ul> <li>Input Artefacts: Allow workflows to receive data from external sources, which can be used as input for various tasks within the workflow.</li> <li>Output Artefacts: Enable workflows to produce and store data generated during execution, making it available for subsequent tasks or external systems.</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: my-template\nspec:\n  entrypoint: whalesay-template\n  templates:\n    - name: whalesay-template\n      inputs:\n        artifacts:\n          - name: input-data\n            path: /tmp/input-file\n      outputs:\n        artifacts:\n          - name: output-file\n            path: /tmp/output-file.txt\n      container:\n        image: docker/whalesay\n        command: [ sh, -c ]\n        args: [\"ls -lh /tmp/input-file &gt;&gt; /tmp/output-file.txt\"]\n</code></pre> <p>This workflow template is designed to:</p> <ol> <li>Accept an input artifact (input-data) and place it at /tmp/input-file inside the container.</li> <li>Execute a command inside a container using the docker/whalesay image.</li> <li>The command lists the contents of the input file in a detailed format and appends this listing to an output file located at /tmp/output-file.txt.</li> <li>The resulting file (/tmp/output-file.txt) is then made available as an output artifact (output-file).</li> </ol>"},{"location":"workflow_design_manual/#workflow-input-artefact","title":"Workflow Input Artefact","text":"<p>The concept of artefacts in WorkflowTemplates abstracts away the specifics of where an artefact is provided. This means that the WorkflowTemplate does not need to know or manage the details of the artefact's storage or retrieval. </p> <p>Instead, when a workflow calls a template, it should define the source of the artefact using various supported protocols such as S3, HTTP, or other storage systems.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world\nspec:\n  arguments:\n    parameters:\n      - name: message\n        value: \"from workflow\u201c\n    artifacts:\n      - name: input-data\n        s3:\n          key: \u201c/data/input.gzarr\u201d\n  workflowTemplateRef:\n    name: my-template\n</code></pre>"},{"location":"workflow_design_manual/#artefacts-flow-between-steps","title":"Artefacts Flow between Steps","text":"<p>Argo Workflows allows you to use the output of one step as the input to another step within the same workflow.</p> <p>The example below demonstrates how to pass an artefact between steps in an Argo Workflow: </p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nspec:\n  templates:\n    steps:\n    - - name: step-1\n        template: step-1-template\n    - - name: step-2\n        template: step-2-template\n        arguments:\n          artifacts:\n            - name: message\n              from: \"{{steps.step-1.outputs.artifacts.hello-artefact}}\"\n</code></pre> <ul> <li>In the first step (step-1), an output artefact named <code>hello-artefact</code> is generated. The template <code>step-1 -template</code> (not provided in the example) is expected to generate this artefact. </li> <li>The second step (step-2) references this output artefact as its input, using the from field to specify the source of the artefact from the previous step. This allows the workflow to use the data produced by step-1 in step-2.</li> </ul>"},{"location":"workflow_design_manual/#artefact-storage-configuration","title":"Artefact Storage Configuration","text":"<p>The default artefact repository in Argo Workflows is a pre-configured storage location where artefacts are stored and retrieved if no specific storage location is provided. This repository simplifies the configuration of workflows by providing a common, centralised storage solution for artefacts. </p>"},{"location":"workflow_design_manual/#key-only-artifacts","title":"Key Only Artifacts","text":"<ul> <li>A key-only artefact is an input or output artefact where you only specify the key, omitting the bucket, secrets, etc. When these parameters are omitted, the default bucket and secrets from the configured artefact repository are used. </li> <li>When you don't specify even a key for an output artefact, Argo Workflows store the output with a system-generated key based on the workflow name, node name, and artefact name to ensure uniqueness.</li> </ul> <p>IMPORTANT: Consider parameterising your S3 keys by {{workflow.uid}} if there's a possibility that you could have concurrent Workflows of the same spec. This would be to avoid a scenario in which the artifact from one Workflow is being deleted while the same S3 key is being generated for a different Workflow.</p>"},{"location":"workflow_design_manual/#s3-configuration-parameters","title":"S3 Configuration Parameters","text":"<p>You can provide specific S3 configuration parameters to customize where and how the artifact is stored. The parameters include:</p> <ul> <li>endpoint: Specifies the S3 service URL. This is the address of the S3-compatible storage service.</li> <li>bucket: Defines the specific S3 bucket where the artifact will be stored. Buckets are containers for storing objects (files).</li> <li>key: Sets the unique path within the bucket for the artifact. This key is a string that identifies the object in the bucket.</li> <li>accessKeySecret: Provides the access key for the S3 bucket. This is retrieved from a Kubernetes secret.</li> <li>secretKeySecret: Provides the secret key for the S3 bucket. This is also retrieved from a Kubernetes secret.</li> </ul> <p>Example Configuration:</p> <pre><code>artifacts:\n  - name: my-artifact\n    s3:\n      endpoint: s3.amazonaws.com\n      bucket: my-bucket\n      key: path/to/artifact\n      accessKeySecret:\n        name: my-s3-secret\n        key: accessKey\n      secretKeySecret:\n        name: my-s3-secret\n        key: secretKey\n</code></pre>"},{"location":"workflow_design_manual/#httphttps","title":"HTTP/HTTPS","text":"<p>Argo Workflows can retrieve artifacts from HTTP/HTTPS endpoints.</p> <ul> <li>url: The URL from which the artifact can be downloaded.</li> </ul> <p>Example Configuration:</p> <pre><code>artifacts:\n  - name: my-artifact\n    http:\n      url: https://example.com/path/to/artifact\n</code></pre>"},{"location":"workflow_design_manual/#git","title":"Git","text":"<p>Argo Workflows supports retrieving artifacts from Git repositories.</p> <ul> <li>repo: The URL of the Git repository.</li> <li>revision: The specific commit SHA, tag, or branch to checkout.</li> <li>sshPrivateKeySecret: The SSH private key used to access the repository.</li> </ul> <p>Example Configuration:</p> <pre><code>artifacts:\n  - name: my-artifact\n    git:\n      repo: git@github.com:my/repo.git\n      revision: main\n      sshPrivateKeySecret:\n        name: my-git-secret\n        key: sshPrivateKey\n</code></pre>"},{"location":"workflow_design_manual/#workflow-output-artefact","title":"Workflow Output Artefact","text":"<p>When a workflow needs to expose an artifact on a particular S3 bucket and key, it is expected to call a publication step. This step will handle the actual publishing of the artifact to the designated S3 bucket. </p> <p>The publication step receives the key from a configured secret:</p> <pre><code>outputs:\n  artifacts:\n    - name: result\n      s3:\n        endpoint: s3.custom-endpoint.com\n        bucket: custom-bucket\n        key: custom-path/to/output.txt\n        accessKeySecret:\n          name: custom-s3-secret\n          key: accessKey\n        secretKeySecret:\n          name: custom-s3-secret\n          key: secretKey\n</code></pre>"},{"location":"workflow_design_manual/#creating-dag","title":"Creating DAG","text":"<p>The workflow Template is as follows: <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: dag-template\nspec:\n  entrypoint: gdal-manipulation\n  nodeSelector:\n    accelerator: \"example-gpu\"\n  templates:\n    - name: gdal-manipulation\n      podSpecPatch: '{\"containers\":[{\"name\":\"main\", \"resources\":{\"requests\":{\"memory\": \"100M\" }}}]}'\n      retryStrategy:\n        limit: \"10\"\n      inputs:\n        parameters:\n          - name: image-url\n          - name: export-key\n      outputs:\n        artifacts:\n          - name: result\n            globalName: result\n            from: \"{{tasks.stage-out.outputs.artifacts.result}}\"\n      dag:\n        tasks:\n          - name: gdal-translate\n            templateRef:\n              name: gdal-translate-template\n              template: gdal-translate\n            arguments:\n              artifacts:\n                - name: input-dataset\n                  http:\n                    url: \"{{inputs.parameters.image-url}}\"\n          - name: gdal-info\n            dependencies: [gdal-translate]\n            templateRef:\n              name: gdal-info-template\n              template: gdal-info\n            arguments:\n              artifacts:\n                - name: input-dataset\n                  from: \"{{tasks.gdal-translate.outputs.artifacts.processed-dataset}}\"\n          - name: stage-out\n            dependencies: [gdal-info]\n            templateRef:\n              name: stage-out-template\n              template: stage-out\n            arguments:\n              parameters:\n                - name: export-key\n                  value: \"{{inputs.parameters.export-key}}\"\n              artifacts:\n                - name: artifact-to-export\n                  from: \"{{tasks.gdal-translate.outputs.artifacts.processed-dataset}}\"\n</code></pre></p> <p>See: Example</p> <p>This template defines a Direct Acyclic Graph (DAG), each task re-using a workflow template: 1. GDAL Translate: Used to append metadata to a file Template 2. GDAl Info: used to generate a report concernin the file modified in task 1. Template 3. Stage OUT: use to stage out the file modified in task 1.Template</p> <p>The first tak argument is an artifact of type HTTP artifact. The url is provided in the Workflow definition: <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: dag-workflow-\nspec:\n  arguments:\n    parameters:\n      - name: image-url\n        value: https://dagshub.com/DagsHub-Datasets/sentinel-2-l2a-cogs-dataset/raw/e9420f518fa204e0b3665bf66aba30ba38449c2b/s3:/sentinel-cogs/sentinel-s2-l2a-cogs/1/C/CV/2024/1/S2B_1CCV_20240106_0_L2A/B01.tif\n      - name: export-key\n        value: result.tif\n  workflowTemplateRef:\n    name: dag-template\n</code></pre> See: Workflow Example</p>"},{"location":"design/rest_api_design/","title":"Argo Workflows REST API","text":""},{"location":"design/rest_api_design/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Interface Overview</li> <li>Interface Type</li> <li>Producers and Consumers</li> <li>Operations</li> <li>Data Model</li> </ul>"},{"location":"design/rest_api_design/#interface-overview","title":"Interface Overview","text":"<p>The Argo Workflows API provides a robust platform for defining, managing, and executing complex workflows on Kubernetes. It is designed to enable users to automate the orchestration of containerized tasks, integrating seamlessly with other systems and services within a Kubernetes environment. </p> <p>The API allows for the creation, submission, monitoring, and management of workflows, facilitating tasks such as data processing, CI/CD pipelines, and machine learning workflows. Argo Workflows is designed to be highly scalable and flexible, supporting a wide range of use cases and providing extensive customization options through its API.</p>"},{"location":"design/rest_api_design/#interface-type","title":"Interface Type","text":"<ul> <li>Description: REST API</li> <li>Communication Protocols: HTTP/HTTPS</li> </ul>"},{"location":"design/rest_api_design/#producers-and-consumers","title":"Producers and Consumers","text":"<p>Producers: - CI/CD systems that need to automate deployment pipelines. - Data processing systems that require complex task orchestration. - Machine learning platforms that automate model training and deployment workflows. - Users and developers who define and manage workflows through the API.</p> <p>Consumers: - Monitoring systems that track the status and progress of workflows. Logging systems that aggregate and analyze workflow logs. Notification services that alert users about workflow events and statuses. Reporting tools that generate insights based on workflow executions. Users and administrators who retrieve workflow status, logs, and results through the API.</p>"},{"location":"design/rest_api_design/#operations","title":"Operations","text":"<p>Detail each operation provided by the API, including method names, request and response formats, and a brief description of their functionality.</p>"},{"location":"design/rest_api_design/#example-operation-getitem","title":"Example Operation: GetItem","text":"<ul> <li>HTTP Method: <code>GET</code></li> <li>Endpoint: <code>/items/{id}</code></li> <li>Description: Retrieves an item by its unique ID.</li> <li>Request Parameters:<ul> <li><code>id</code> (path): The unique identifier for the item.</li> </ul> </li> <li>Response:<ul> <li>Success (200 OK):   <pre><code>{\n  \"id\": \"item1\",\n  \"name\": \"Example Item\",\n  \"description\": \"A sample item in the catalog.\"\n}\n</code></pre></li> <li>Error (404 Not Found):   <pre><code>{\n  \"error\": \"Item not found.\"\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"design/rest_api_design/#workflow-template-creation","title":"Workflow Template creation","text":"<p>The following operation is used to register a reusable workflow template by providing the target namespace as a path parameter and the workflow template in the request's body.</p> <p>POST /api/v1/workflow-templates/{namespace} </p> Parameter In Type Required Description namespace path string true Namespace of the workflow template. body body object true Workflow template object to create."},{"location":"design/rest_api_design/#list-available-workflow-templates","title":"List available Workflow Templates","text":"<p>The following operation list the reusable workflow templates already available. This request requires to provide the namespace as a path parameter, other parameters must be provided as query parameters and are optionals parameters used to filter the list returned by this operation.</p> <p>GET /api/v1/workflow-templates/{namespace}</p> Parameter In Type Required Description namespace path string true Namespace of the workflow template."},{"location":"design/rest_api_design/#describe-workflow-template","title":"Describe Workflow Template","text":"<p>The following operation provide information about a specific workflow template.</p> <p>GET /api/v1/workflow-templates/{namespace}/{name}</p> Parameter In Type Required Description namespace path string true Namespace of the workflow template. name path string true Name of the workflow template."},{"location":"design/rest_api_design/#list-workflows","title":"List Workflows","text":"<p>The following operation list all workflows submitted in a specific namespace.</p> <p>GET /api/v1/workflows/{namespace}</p> Parameter In Type Required Description namespace path string true Namespace of the workflows."},{"location":"design/rest_api_design/#submit-workflow","title":"Submit Workflow","text":"<p>The following operation submit a workflow for execution in the namespace specified.</p> <p>POST /api/v1/workflows/{namespace}</p> Parameter In Type Required Description namespace path string true Namespace of the workflows. body body object true Workflow object to submit."},{"location":"design/rest_api_design/#delete-workflow","title":"Delete Workflow","text":"<p>The following operation deletes a workflow in the namespace specified.</p> <p>DELETE /api/v1/workflows/{namespace}/{name}</p> Parameter In Type Required Description namespace path string true Namespace of the workflow. name path string true Name of the workflow."},{"location":"design/rest_api_design/#retrieve-workflow-status","title":"Retrieve Workflow Status","text":"<p>The following operation retrieves the status of the desired workflow running in the namespace specified.</p> <p>GET /api/v1/workflows/{namespace}/{name}/status</p> Parameter In Type Required Description namespace path string true Namespace of the workflow. name path string true Name of the workflow."},{"location":"design/rest_api_design/#retrieve-workflow-logs","title":"Retrieve Workflow Logs","text":"<p>The following operation retrieves the logs of the desired workflow running in the namespace specified. To retrieve logs from the desired step, specify the 'logOptions.container' query parameter with the container name corresponding to the desired step.</p> <p>GET /api/v1/workflows/{namespace}/{name}/logs</p> Parameter In Type Required Description namespace path string true Namespace of the workflow. name path string true Name of the workflow."},{"location":"design/rest_api_design/#data-model","title":"Data Model","text":"<p>The full reference of the data model is provided on https://argo-workflows.readthedocs.io/en/latest/fields/</p>"},{"location":"design/rest_api_spec/","title":"REST API Specification","text":""},{"location":"design/rest_api_spec/#argo-workflows-api-version","title":"Argo Workflows API VERSION","text":"<p>Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. For more information, please see https://argo-workflows.readthedocs.io/en/latest/</p>"},{"location":"design/rest_api_spec/#archivedworkflowservice","title":"ArchivedWorkflowService","text":""},{"location":"design/rest_api_spec/#get-apiv1archived-workflows","title":"GET /api/v1/archived-workflows","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namePrefix</code> query None No <code>namespace</code> query None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1archived-workflows-label-keys","title":"GET /api/v1/archived-workflows-label-keys","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>namespace</code> query None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1archived-workflows-label-values","title":"GET /api/v1/archived-workflows-label-values","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> query None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1archived-workflowsuid","title":"GET /api/v1/archived-workflows/{uid}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>name</code> query None No <code>namespace</code> query None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1archived-workflowsuid","title":"DELETE /api/v1/archived-workflows/{uid}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>namespace</code> query None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1archived-workflowsuidresubmit","title":"PUT /api/v1/archived-workflows/{uid}/resubmit","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1archived-workflowsuidretry","title":"PUT /api/v1/archived-workflows/{uid}/retry","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#clusterworkflowtemplateservice","title":"ClusterWorkflowTemplateService","text":""},{"location":"design/rest_api_spec/#get-apiv1cluster-workflow-templates","title":"GET /api/v1/cluster-workflow-templates","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1cluster-workflow-templates","title":"POST /api/v1/cluster-workflow-templates","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1cluster-workflow-templateslint","title":"POST /api/v1/cluster-workflow-templates/lint","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1cluster-workflow-templatesname","title":"GET /api/v1/cluster-workflow-templates/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>getOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>name</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1cluster-workflow-templatesname","title":"PUT /api/v1/cluster-workflow-templates/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No DEPRECATED: This field is ignored. <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1cluster-workflow-templatesname","title":"DELETE /api/v1/cluster-workflow-templates/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>name</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#cronworkflowservice","title":"CronWorkflowService","text":""},{"location":"design/rest_api_spec/#get-apiv1cron-workflowsnamespace","title":"GET /api/v1/cron-workflows/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1cron-workflowsnamespace","title":"POST /api/v1/cron-workflows/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1cron-workflowsnamespacelint","title":"POST /api/v1/cron-workflows/{namespace}/lint","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1cron-workflowsnamespacename","title":"GET /api/v1/cron-workflows/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>getOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1cron-workflowsnamespacename","title":"PUT /api/v1/cron-workflows/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No DEPRECATED: This field is ignored. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1cron-workflowsnamespacename","title":"DELETE /api/v1/cron-workflows/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1cron-workflowsnamespacenameresume","title":"PUT /api/v1/cron-workflows/{namespace}/{name}/resume","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1cron-workflowsnamespacenamesuspend","title":"PUT /api/v1/cron-workflows/{namespace}/{name}/suspend","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#eventsourceservice","title":"EventSourceService","text":""},{"location":"design/rest_api_spec/#get-apiv1event-sourcesnamespace","title":"GET /api/v1/event-sources/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1event-sourcesnamespace","title":"POST /api/v1/event-sources/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1event-sourcesnamespacename","title":"GET /api/v1/event-sources/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1event-sourcesnamespacename","title":"PUT /api/v1/event-sources/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1event-sourcesnamespacename","title":"DELETE /api/v1/event-sources/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1streamevent-sourcesnamespace","title":"GET /api/v1/stream/event-sources/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1streamevent-sourcesnamespacelogs","title":"GET /api/v1/stream/event-sources/{namespace}/logs","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>eventName</code> query None No optional - only return entries for this event name (e.g. `example`). <code>eventSourceType</code> query None No optional - only return entries for this event source type (e.g. `webhook`). <code>grep</code> query None No optional - only return entries where `msg` matches this regular expression. <code>name</code> query None No optional - only return entries for this event source. <code>namespace</code> path None No <code>podLogOptions.container</code> query None No The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. <code>podLogOptions.follow</code> query None No Follow the log stream of the pod. Defaults to false. +optional. <code>podLogOptions.insecureSkipTLSVerifyBackend</code> query None No insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. <code>podLogOptions.limitBytes</code> query None No If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. <code>podLogOptions.previous</code> query None No Return previous terminated container logs. Defaults to false. +optional. <code>podLogOptions.sinceSeconds</code> query None No A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. <code>podLogOptions.sinceTime.nanos</code> query None No Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. <code>podLogOptions.sinceTime.seconds</code> query None No Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. <code>podLogOptions.tailLines</code> query None No If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime +optional. <code>podLogOptions.timestamps</code> query None No If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#eventservice","title":"EventService","text":""},{"location":"design/rest_api_spec/#post-apiv1eventsnamespacediscriminator","title":"POST /api/v1/events/{namespace}/{discriminator}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No The event itself can be any data. <code>discriminator</code> path None No Optional discriminator for the io.argoproj.workflow.v1alpha1. This should almost always be empty. Used for edge-cases where the event payload alone is not provide enough information to discriminate the event. This MUST NOT be used as security mechanism, e.g. to allow two clients to use the same access token, or to support webhooks on unsecured server. Instead, use access tokens. This is made available as `discriminator` in the event binding selector (`/spec/event/selector)` <code>namespace</code> path None No The namespace for the io.argoproj.workflow.v1alpha1. This can be empty if the client has cluster scoped permissions. If empty, then the event is \"broadcast\" to workflow event binding in all namespaces. <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflow-event-bindingsnamespace","title":"GET /api/v1/workflow-event-bindings/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#infoservice","title":"InfoService","text":""},{"location":"design/rest_api_spec/#get-apiv1info","title":"GET /api/v1/info","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1trackingevent","title":"POST /api/v1/tracking/event","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1userinfo","title":"GET /api/v1/userinfo","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1version","title":"GET /api/v1/version","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#sensorservice","title":"SensorService","text":""},{"location":"design/rest_api_spec/#get-apiv1sensorsnamespace","title":"GET /api/v1/sensors/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1sensorsnamespace","title":"POST /api/v1/sensors/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1sensorsnamespacename","title":"GET /api/v1/sensors/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>getOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1sensorsnamespacename","title":"PUT /api/v1/sensors/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1sensorsnamespacename","title":"DELETE /api/v1/sensors/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1streamsensorsnamespace","title":"GET /api/v1/stream/sensors/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1streamsensorsnamespacelogs","title":"GET /api/v1/stream/sensors/{namespace}/logs","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>grep</code> query None No option - only return entries where `msg` contains this regular expressions. <code>name</code> query None No optional - only return entries for this sensor name. <code>namespace</code> path None No <code>podLogOptions.container</code> query None No The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. <code>podLogOptions.follow</code> query None No Follow the log stream of the pod. Defaults to false. +optional. <code>podLogOptions.insecureSkipTLSVerifyBackend</code> query None No insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. <code>podLogOptions.limitBytes</code> query None No If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. <code>podLogOptions.previous</code> query None No Return previous terminated container logs. Defaults to false. +optional. <code>podLogOptions.sinceSeconds</code> query None No A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. <code>podLogOptions.sinceTime.nanos</code> query None No Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. <code>podLogOptions.sinceTime.seconds</code> query None No Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. <code>podLogOptions.tailLines</code> query None No If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime +optional. <code>podLogOptions.timestamps</code> query None No If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. <code>triggerName</code> query None No optional - only return entries for this trigger. <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#workflowservice","title":"WorkflowService","text":""},{"location":"design/rest_api_spec/#get-apiv1streameventsnamespace","title":"GET /api/v1/stream/events/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflow-eventsnamespace","title":"GET /api/v1/workflow-events/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>fields</code> query None No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflowsnamespace","title":"GET /api/v1/workflows/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>fields</code> query None No Fields to be included or excluded in the response. e.g. \"items.spec,items.status.phase\", \"-items.status.nodes\". <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1workflowsnamespace","title":"POST /api/v1/workflows/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1workflowsnamespacelint","title":"POST /api/v1/workflows/{namespace}/lint","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1workflowsnamespacesubmit","title":"POST /api/v1/workflows/{namespace}/submit","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflowsnamespacename","title":"GET /api/v1/workflows/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>fields</code> query None No Fields to be included or excluded in the response. e.g. \"spec,status.phase\", \"-status.nodes\". <code>getOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1workflowsnamespacename","title":"DELETE /api/v1/workflows/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>force</code> query None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflowsnamespacenamelog","title":"GET /api/v1/workflows/{namespace}/{name}/log","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>grep</code> query None No <code>logOptions.container</code> query None No The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. <code>logOptions.follow</code> query None No Follow the log stream of the pod. Defaults to false. +optional. <code>logOptions.insecureSkipTLSVerifyBackend</code> query None No insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. <code>logOptions.limitBytes</code> query None No If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. <code>logOptions.previous</code> query None No Return previous terminated container logs. Defaults to false. +optional. <code>logOptions.sinceSeconds</code> query None No A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. <code>logOptions.sinceTime.nanos</code> query None No Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. <code>logOptions.sinceTime.seconds</code> query None No Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. <code>logOptions.tailLines</code> query None No If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime +optional. <code>logOptions.timestamps</code> query None No If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. <code>name</code> path None No <code>namespace</code> path None No <code>podName</code> query None No <code>selector</code> query None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenameresubmit","title":"PUT /api/v1/workflows/{namespace}/{name}/resubmit","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenameresume","title":"PUT /api/v1/workflows/{namespace}/{name}/resume","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenameretry","title":"PUT /api/v1/workflows/{namespace}/{name}/retry","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenameset","title":"PUT /api/v1/workflows/{namespace}/{name}/set","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenamestop","title":"PUT /api/v1/workflows/{namespace}/{name}/stop","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenamesuspend","title":"PUT /api/v1/workflows/{namespace}/{name}/suspend","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenameterminate","title":"PUT /api/v1/workflows/{namespace}/{name}/terminate","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflowsnamespacenamepodnamelog","title":"GET /api/v1/workflows/{namespace}/{name}/{podName}/log","text":"<p>DEPRECATED: Cannot work via HTTP if podName is an empty string. Use WorkflowLogs.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>grep</code> query None No <code>logOptions.container</code> query None No The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. <code>logOptions.follow</code> query None No Follow the log stream of the pod. Defaults to false. +optional. <code>logOptions.insecureSkipTLSVerifyBackend</code> query None No insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. <code>logOptions.limitBytes</code> query None No If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. <code>logOptions.previous</code> query None No Return previous terminated container logs. Defaults to false. +optional. <code>logOptions.sinceSeconds</code> query None No A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. <code>logOptions.sinceTime.nanos</code> query None No Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. <code>logOptions.sinceTime.seconds</code> query None No Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. <code>logOptions.tailLines</code> query None No If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime +optional. <code>logOptions.timestamps</code> query None No If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. <code>name</code> path None No <code>namespace</code> path None No <code>podName</code> path None No <code>selector</code> query None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#workflowtemplateservice","title":"WorkflowTemplateService","text":""},{"location":"design/rest_api_spec/#get-apiv1workflow-templatesnamespace","title":"GET /api/v1/workflow-templates/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namePattern</code> query None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1workflow-templatesnamespace","title":"POST /api/v1/workflow-templates/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1workflow-templatesnamespacelint","title":"POST /api/v1/workflow-templates/{namespace}/lint","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflow-templatesnamespacename","title":"GET /api/v1/workflow-templates/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>getOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflow-templatesnamespacename","title":"PUT /api/v1/workflow-templates/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No DEPRECATED: This field is ignored. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1workflow-templatesnamespacename","title":"DELETE /api/v1/workflow-templates/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#artifactservice","title":"ArtifactService","text":""},{"location":"design/rest_api_spec/#get-artifact-filesnamespaceiddiscriminatoridnodeidartifactdiscriminatorartifactname","title":"GET /artifact-files/{namespace}/{idDiscriminator}/{id}/{nodeId}/{artifactDiscriminator}/{artifactName}","text":"<p>Get an artifact.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>artifactDiscriminator</code> path None No <code>artifactName</code> path None No <code>id</code> path None No <code>idDiscriminator</code> path None No <code>namespace</code> path None No <code>nodeId</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-artifacts-by-uiduidnodeidartifactname","title":"GET /artifacts-by-uid/{uid}/{nodeId}/{artifactName}","text":"<p>Get an output artifact by UID.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>artifactName</code> path None No <code>nodeId</code> path None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-artifactsnamespacenamenodeidartifactname","title":"GET /artifacts/{namespace}/{name}/{nodeId}/{artifactName}","text":"<p>Get an output artifact.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>artifactName</code> path None No <code>name</code> path None No <code>namespace</code> path None No <code>nodeId</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-input-artifacts-by-uiduidnodeidartifactname","title":"GET /input-artifacts-by-uid/{uid}/{nodeId}/{artifactName}","text":"<p>Get an input artifact by UID.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>artifactName</code> path None No <code>nodeId</code> path None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-input-artifactsnamespacenamenodeidartifactname","title":"GET /input-artifacts/{namespace}/{name}/{nodeId}/{artifactName}","text":"<p>Get an input artifact.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>artifactName</code> path None No <code>name</code> path None No <code>namespace</code> path None No <code>nodeId</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/sw_design/","title":"Software Design Document for Argo-Workflow","text":""},{"location":"design/sw_design/#general","title":"General","text":"<p>This section provides a detailed look at the design aspects of each component used in the software. </p>"},{"location":"design/sw_design/#component-identifier","title":"Component Identifier","text":"<ul> <li>Component Identifier: Argo-Workflows</li> <li>System Logical Component Workflow Engine: </li> </ul> <p>Argo Workflows enables the chaining of container-based modules and workflows.</p>"},{"location":"design/sw_design/#type","title":"Type","text":""},{"location":"design/sw_design/#logical-characteristics","title":"Logical Characteristics","text":"<p>Argo Workflows is the main functional component implementing the Data Processing Environment sub-system of the OHDSA platform.</p>"},{"location":"design/sw_design/#physical-characteristics","title":"Physical Characteristics","text":"<p>Argo Workflows is a Commercial-Of-The-Shelf (COTS) Kubernetes-native workflow engine that orchestrates parallel jobs on a Kubernetes cluster. It uses Kubernetes resources like custom resources, pods, jobs, ConfigMaps, and Secrets to implement and manage workflows. Each workflow is defined as a Workflow object, which dictates the sequence of tasks to be executed. These tasks are encapsulated as Kubernetes Jobs or Pods, making each step of the workflow a discrete, schedulable unit under Kubernetes management.</p>"},{"location":"design/sw_design/#purpose","title":"Purpose","text":"<p>Argo Workflows component is designed to fulfill a pivotal role in managing and executing complex workflows that integrate various modules within Docker containers. The purpose of Argo Workflows is to enable users to submit both simple and elaborate workflows with a versatile Workflow Definition Language that accommodates Directed Acyclic Graph models, dynamic resource allocation, retry strategies, the utilisation of specialized hardware, and completion notifications. </p>"},{"location":"design/sw_design/#function","title":"Function","text":"<p>Argo Workflows is a container-native workflow engine for Kubernetes, designed to orchestrate parallel jobs in a cloud environment.</p> <p>The following subsections focus on the implementation of the OHDSA capabilities supported by the workflow engine.</p>"},{"location":"design/sw_design/#api","title":"API","text":"<p>The Argo Workflows component provides a REST API that facilitates the creation, management, and monitoring of workflows.</p> <p>Key API Operations:</p> <ol> <li>Create Workflow Template: Allows the user to define and register a reusable workflow template.</li> <li>List Available Workflow Templates: Retrieves a list of all registered workflow templates.</li> <li>Describe Workflow Template: Provides detailed information about a specific workflow template.</li> <li>List Workflows: Returns a list of all active and completed workflows.</li> <li>Submit Workflow: Initiates a new workflow based on a specified workflow template.</li> <li>Delete Workflow: Removes a specified workflow from the system.</li> <li>Retrieve Workflow Status: Fetches the current status of a specific workflow.</li> <li>Retrieve Workflow Logs: Provides access to the logs generated by a specific workflow.</li> </ol> <p>For detailed documentation and examples of these API operations, refer to the API Design Documentation.</p>"},{"location":"design/sw_design/#reusable-workflow-templates","title":"Reusable Workflow Templates","text":"<p>Argo Workflows supports the creation and utilisation of reusable workflow templates to standardise and streamline workflow definitions.</p> <p>Reusable workflow templates in Argo Workflows allow users to define a set of tasks and their configurations once and reuse them across multiple workflows. </p> <p>The templates encapsulate the following elements:</p> <ul> <li>Entrypoint: Defines the main entry point for the workflow, specifying the starting task.</li> <li>Templates: Lists the tasks included in the workflow template, each with its own configuration such as container image, commands, and arguments.</li> </ul> <p>The following example represents a basic workflow template definition: <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: hello-world-wft\nspec:\n  entrypoint: whalesay\n  templates:\n    - name: whalesay\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: docker/whalesay\n        command: [ cowsay ]\n        args: [\"{{inputs.parameters.message}}\"]\n</code></pre></p> <p>The usage of workflow template works as follows:</p> <ol> <li> <p>Definition: Users define a workflow template, specifying the tasks and their configurations. This is done using YAML, where each task within the template is defined with its specific parameters and resources.</p> </li> <li> <p>Registration: The defined workflow template is registered with the Argo system using the appropriate API endpoint. This makes the template available for use in workflows.</p> </li> <li> <p>Reuse: Once registered, the workflow template can be referenced in multiple workflows. This is achieved by specifying the template reference in the workflow definition, allowing the workflow to utilise the predefined tasks and configurations.</p> </li> <li> <p>Execution: When a workflow that references a reusable template is executed, Argo orchestrates the tasks as defined in the template, ensuring that each task is executed according to the specified configuration.</p> </li> </ol> <p>Argo provides a REST API to manage workflow templates, including operations to create, list, describe, and delete templates. This API facilitates the integration of workflow template management into automated processes and external systems.</p>"},{"location":"design/sw_design/#dag-workflow-creation","title":"DAG Workflow Creation","text":"<p>Argo Workflows supports the creation of Directed Acyclic Graph (DAG) workflows, which allow for the definition of complex workflows with dependencies between tasks. This capability ensures optimal execution order and efficient resource utilisation.</p> <p>DAG workflows in Argo enable users to define workflows where tasks are executed based on their dependencies. This means that a task will only start once all its dependent tasks have been completed successfully.</p> <ul> <li>Task Dependencies: Tasks in a DAG can specify dependencies, indicating which tasks need to be completed before they can be executed.</li> <li>Parallel Execution: Independent tasks can run in parallel, maximising resource usage and reducing overall execution time.</li> <li>Template Reuse: DAG workflows can reference reusable workflow templates, allowing for modular and maintainable workflow definitions.</li> <li>Dynamic Execution: The DAG model supports dynamic execution paths, where the workflow can adapt based on the results of preceding tasks.</li> </ul> <p>The process of creating and executing a DAG workflow is as follows:</p> <ol> <li>Definition: A DAG workflow is defined in a YAML file, where the <code>templates</code> section includes a <code>dag</code> template that specifies the tasks and their dependencies.</li> <li>Task Specification: Each task within the DAG is defined with a <code>name</code>, <code>templateRef</code> to reference reusable templates, and <code>dependencies</code> to specify other tasks that must complete before this task starts.</li> <li>Execution: When the DAG workflow is submitted, Argo orchestrates the execution based on the defined dependencies. Tasks are scheduled to run in the optimal order, ensuring that each task only starts once all its dependencies are resolved.</li> </ol> <p>Below is a minimal example of a DAG workflow definition:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: dag-example-\nspec:\n  entrypoint: dag-example\n  templates:\n    - name: dag-example\n      dag:\n        tasks:\n          - name: task-1\n            templateRef:\n              name: some-template\n              template: some-task\n          - name: task-2\n            dependencies: [task-1]\n            templateRef:\n              name: some-template\n              template: another-task\n</code></pre> <p>In this example: * apiVersion: Specifies the API version. * kind: Indicates the resource type, which is Workflow. * metadata: Contains metadata, including the workflow name. * spec: Defines the specifications of the workflow. * entrypoint: Specifies the main entry point for the DAG workflow. * templates: Lists the tasks included in the DAG workflow. * dag: Defines the DAG structure with tasks and their dependencies.</p>"},{"location":"design/sw_design/#data-artefacts","title":"Data Artefacts","text":"<p>Argo Workflows supports the management of data artefacts, enabling the staging-in, staging-out, and passing of data between workflow steps. This capability is essential for handling input and output data efficiently within workflows.</p> <p>The features of Argo Workflows for managing data artefacts include:</p> <ul> <li>Artifact Repositories: Argo supports the use of artifact repositories for storing and retrieving data. These repositories can be any S3-compatible storage, such as Minio, or other interfaces like GIT.</li> <li>Input Artefacts: Workflows can specify input artefacts that are provided as arguments when the workflow is executed.</li> <li>Intermediate Artefacts: Intermediate data generated during workflow execution can be stored in artifact repositories to be used by subsequent steps.</li> <li>Output Artefacts: Workflows can create and store output artefacts in designated repositories, which can then be accessed by other workflows or external systems.</li> <li>Credential Agnosticism: Workflow templates do not manage credentials directly. Instead, artefact management is handled by the workflow runtime, ensuring security and separation of concerns.</li> </ul> <p>The process for managing data artefacts in Argo Workflows is as follows:</p> <ol> <li>Configuration: Define artifact repositories within Kubernetes ConfigMaps. These repositories are referenced in workflow templates to standardise artefact handling across workflows.</li> <li>Data Input: Specify input artefacts in the workflow template. These artefacts are passed as arguments when the workflow is invoked.</li> <li>Intermediate Storage: Store intermediate artefacts in the default or specified artifact repository, ensuring they are accessible for subsequent workflow steps.</li> <li>Data Output: Create and store output artefacts in artifact repositories, enabling reuse and integration with other workflows or systems.</li> </ol> <p>Below is a minimal example of an artifact repository configuration in a Kubernetes ConfigMap:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: artifact-repositories\ndata:\n  default-artifact-repository: |\n    s3:\n      bucket: argo-bucket\n      endpoint: minio:9000\n      insecure: true\n      accessKeySecret:\n        name: my-minio-cred\n        key: accesskey\n      secretKeySecret:\n        name: my-minio-cred\n        key: secretkey\n</code></pre> <p>In this example:</p> <ul> <li>apiVersion: Specifies the API version.</li> <li>kind: Indicates the resource type, which is ConfigMap.</li> <li>metadata: Contains metadata, including the ConfigMap name.</li> <li>data: Defines the artifact repository configuration, including S3 endpoint details and credentials.</li> </ul>"},{"location":"design/sw_design/#retry-strategy","title":"Retry Strategy","text":"<p>Argo Workflows provides robust retry strategies to handle task failures effectively, ensuring that workflows can recover from transient errors and improve reliability.</p> <p>The retry strategy in Argo Workflows allows for the configuration of retries for failed tasks, providing flexibility in handling different types of errors. Key features include:</p> <ul> <li>Limit Parameter: Specifies the maximum number of retry attempts for a task.</li> <li>Retry Policies: Defines conditions under which a task should be retried. Available policies include:</li> <li>Always: Retries all failed tasks regardless of the failure type.</li> <li>OnFailure: Retries tasks that are marked as failed by Kubernetes.</li> <li>OnError: Retries tasks that encounter errors related to the Argo controller or when init or wait containers fail.</li> <li>OnTransientError: Retries tasks that encounter transient errors or errors matching a specific pattern.</li> <li>Expressions: Utilises expressions to control retry behaviour, accessing variables such as <code>lastRetry.exitCode</code>, <code>lastRetry.status</code>, <code>lastRetry.duration</code>, and <code>lastRetry.message</code>.</li> <li>Backoff: Implements a delay between retry attempts to prevent immediate consecutive retries, which can help mitigate issues from transient failures.</li> </ul> <p>The process for implementing retry strategies in Argo Workflows is as follows:</p> <ol> <li>Define Retry Parameters: Specify the limit for retries and the retry policy within the workflow or template definition.</li> <li>Set Retry Conditions: Use expressions to determine when a retry should be attempted based on the status and exit codes of previous attempts.</li> <li>Configure Backoff: Define the backoff strategy to introduce delays between retry attempts, helping to manage the load and reduce the likelihood of repeated failures.</li> </ol> <p>Below is a minimal example of a retry strategy configuration in a workflow:</p> <p><pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: retry-strategy-\nspec:\n  entrypoint: retry-example\n  templates:\n    - name: retry-example\n      retryStrategy:\n        limit: 3\n        retryPolicy: \"OnFailure\"\n        backoff:\n          duration: \"5s\"\n          factor: 2\n      container:\n        image: busybox\n        command: [\"sh\", \"-c\"]\n        args: [\"exit 1\"]\n</code></pre> In this example:</p> <ul> <li>retryStrategy.limit: Specifies the maximum number of retry attempts.</li> <li>retryStrategy.retryPolicy: Sets the retry policy to retry on failure.</li> <li>retryStrategy.backoff: Configures a backoff strategy with an initial delay of 5 seconds and an exponential backoff factor of 2.</li> </ul>"},{"location":"design/sw_design/#notification","title":"Notification","text":"<p>Argo Workflows provides mechanisms to send notifications based on workflow events, ensuring users are informed about the status and results of their workflows.</p> <p>The notification features in Argo Workflows enable users to configure alerts and messages for various workflow events, such as completions, failures, and other significant statuses. Key features include:</p> <ul> <li>Exit Handlers: Define actions to be taken upon workflow completion, allowing notifications to be sent based on the workflow's outcome.</li> <li>Notification Types: Support for various notification types, such as emails, messages via protocols like AMQP, or custom scripts.</li> <li>Custom Containers: Use custom containers to execute commands that send notifications, offering flexibility in how notifications are handled and delivered.</li> </ul> <p>The notification system in Argo Workflows operates as follows:</p> <ol> <li>Define Exit Handlers: Configure exit handlers in the workflow definition. These handlers specify the actions to be executed when the workflow completes, regardless of success or failure.</li> <li>Custom Containers for Notifications: Implement custom containers within exit handlers to perform notification actions. These containers can run scripts or commands to send emails, messages, or other notifications.</li> </ol> <p>Below is a minimal example of a workflow configuration with an exit handler that sends a message to a RabbitMQ queue upon workflow completion:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: notify-rabbitmq-example-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      container:\n        image: busybox\n        command: [\"sh\", \"-c\"]\n        args: [\"exit 0\"]\n  onExit: notify\n  templates:\n    - name: notify\n      container:\n        image: rabbitmq:3-management\n        command: [\"sh\", \"-c\"]\n        args:\n          - |\n            apt-get update &amp;&amp; apt-get install -y curl\n            curl -i -u guest:guest -H \"content-type:application/json\" \\\n            -X POST -d '{\"properties\":{},\"routing_key\":\"\",\"payload\":\"Workflow completed successfully\",\"payload_encoding\":\"string\"}' \\\n            http://rabbitmq:15672/api/exchanges/%2F/amq.default/publish\n</code></pre>"},{"location":"design/sw_design/#hardware-constraints","title":"Hardware Constraints","text":"<p>Argo Workflows allows for the specification of hardware constraints to ensure that workflows run on nodes with the appropriate resources and capabilities. This feature is essential for optimising resource usage and ensuring that workflows can utilise specialised hardware as needed.</p>"},{"location":"design/sw_design/#capabilities-and-usage","title":"Capabilities and Usage","text":"<p>The features for managing hardware constraints in Argo Workflows include:</p> <ul> <li>Resource Requests and Limits: Define the amount of CPU and memory resources required for each task.</li> <li>Node Affinity: Specify node affinity rules to ensure that tasks are scheduled on nodes with specific labels, indicating specialised hardware or other node-specific attributes.</li> <li>Tolerations: Define tolerations to allow tasks to be scheduled on tainted nodes, which can be used to reserve nodes for specific types of workloads.</li> </ul>"},{"location":"design/sw_design/#how-hardware-constraints-work","title":"How Hardware Constraints Work","text":"<p>The process for implementing hardware constraints in Argo Workflows is as follows:</p> <ol> <li>Define Resource Requirements: In the workflow template, specify the CPU and memory resources required for each task using <code>resources.requests</code> and <code>resources.limits</code>.</li> <li>Set Node Affinity: Use <code>nodeSelector</code> or <code>affinity</code> fields in the workflow template to ensure tasks are scheduled on nodes with the desired hardware characteristics.</li> <li>Configure Tolerations: Add tolerations to the workflow template to allow tasks to run on nodes with specific taints, enabling the reservation of nodes for specialised tasks.</li> </ol>"},{"location":"design/sw_design/#benefits","title":"Benefits","text":"<p>The advantages of using hardware constraints in Argo Workflows include:</p> <ul> <li>Optimised Resource Usage: Ensures that tasks are scheduled on nodes with sufficient and appropriate resources, preventing resource contention and optimising performance.</li> <li>Specialised Hardware Utilisation: Allows workflows to take advantage of nodes with specialised hardware, such as GPUs or high-memory nodes, by specifying appropriate constraints.</li> <li>Improved Workflow Efficiency: Enhances the efficiency of workflows by ensuring that tasks are executed on nodes that meet their resource and hardware requirements.</li> </ul>"},{"location":"design/sw_design/#example-configuration","title":"Example Configuration","text":"<p>Below is a minimal example of a workflow configuration with hardware constraints:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hardware-constraints-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      container:\n        image: busybox\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n      nodeSelector:\n        disktype: ssd\n      tolerations:\n        - key: \"dedicated\"\n          operator: \"Equal\"\n          value: \"gpu\"\n          effect: \"NoSchedule\"\n</code></pre> <p>In this example:</p> <ul> <li>resources.requests: Specifies the minimum required CPU and memory for the task.</li> <li>resources.limits: Specifies the maximum allowed CPU and memory for the task.</li> <li>nodeSelector: Ensures the task is scheduled on a node with the label disktype: ssd.</li> <li>tolerations: Allows the task to be scheduled on nodes with the taint dedicated=gpu:NoSchedule.</li> </ul>"},{"location":"design/sw_design/#artefact-persistence-and-retention","title":"Artefact Persistence and Retention","text":"<p>TBD section</p> <p>Argo Workflows provides mechanisms to manage the persistence of artefacts, ensuring that important data generated during workflow execution is retained and accessible for future use.</p> <p>The features for managing artefact persistence in Argo Workflows include:</p> <ul> <li>Persistent Artefacts: Mark specific artefacts for persistence, ensuring they are not discarded after workflow completion.</li> <li>Retention Policies: Define policies to manage the lifecycle of persistent artefacts, including archiving and expiration.</li> </ul> <p>The workflow engine operator must configure retention policies to remove expired workflows automatically (see Archiving workflows. </p> <p>The workflow archive stores information about the workflows such as the status, pods executed, results and more. This information is stored in a database such as PostgreSQL.</p> <p>Archive TTL: Specifies the time period to keep archived workflows before they will be deleted by the archived workflow garbage collection function. The default is forever.</p> <p>Example: <pre><code>persistence:\n  archiveTTL: 10d\n</code></pre></p>"},{"location":"design/sw_design/#logging","title":"Logging","text":"<p>Argo Workflows provides robust logging capabilities to help users monitor and debug their workflows. Logs are crucial for understanding the behaviour of workflows and diagnosing issues when they arise.</p> <p>The logging features in Argo Workflows include:</p> <ul> <li>Log Retrieval: Access logs for individual tasks directly through the Argo Workflows UI, CLI, or API.</li> <li>Log Storage: Configure log storage to persist logs for later analysis.</li> <li>Integration with Log Management Systems: Forward logs to external log management systems like Elasticsearch, Fluentd, or Kibana for advanced searching, indexing, and analysis.</li> </ul> <p>Argo Workflows captures logs from the containers running each workflow step. These logs can be accessed and managed as follows:</p> <ol> <li> <p>Log Retrieval via UI: Users can view the logs of each workflow step directly in the Argo Workflows web UI. This interface provides a convenient way to inspect logs without needing to access the underlying infrastructure.</p> </li> <li> <p>Log Retrieval via CLI: The <code>argo logs</code> command allows users to fetch and display logs for a specific workflow or task from the command line. This is useful for scripting and automated log retrieval.</p> </li> <li> <p>Log Retrieval via API: Logs can also be accessed programmatically through the Argo Workflows API, allowing integration with other systems and custom dashboards.</p> </li> <li> <p>Log Storage Configuration: Configure the storage of logs by specifying log archival options. This can be done using Kubernetes ConfigMaps or similar mechanisms to define where and how logs should be stored.</p> </li> <li> <p>Integration with Log Management Systems: For advanced log management, integrate Argo Workflows with external systems like Elasticsearch and Fluentd. These systems provide capabilities for indexing, searching, and visualising logs, enhancing the ability to monitor and troubleshoot workflows.</p> </li> </ol>"},{"location":"design/sw_design/#metrics","title":"Metrics","text":"<ul> <li>Monitoring: Argo produces metrics that provide information on the controller's status. As those metrics follow the same format as required by prometheus, those can be integrated with Prometheus.   Two types of metrics are emitted by Argo:</li> <li>Controller metrics: concerns the state of the controller;</li> <li>Custom metrics: regards the state of a Workflow, or a series of Workflows. Those metrics can be defined on the Workflow/Step/Task emitting the metric. using the same name and help string, is a required by prometheus to track the metrics over time.</li> </ul> <p>For more information, see: See: https://argo-workflows.readthedocs.io/en/stable/metrics/</p>"},{"location":"design/sw_design/#subordinates","title":"Subordinates","text":"<p>Argo Workflows encompasses the various components and resources it manages and interacts with to orchestrate and execute workflows.</p> <ul> <li>Workflow Templates : These are predefined configurations that describe specific workflows. They serve as blueprints from which instances of workflows are generated and executed. Workflow templates define the sequence of tasks, their dependencies, and the resources needed for each task.</li> <li>Tasks/Pods - In the context of Argo Workflow, each step in a workflow is executed as a separate pod within the Kubernetes environment. These pods can be considered subcomponents of the workflow they belong to, executing specific actions defined by the workflow and then reporting back their status upon completion or failure.</li> </ul> <p>Note: identify the DB, message broker, prometheus, etc.</p>"},{"location":"design/sw_design/#dependencies","title":"Dependencies","text":"<ul> <li>Description: Detail any operations or conditions that must be met before this component can function properly, including exclusions during its operation.</li> </ul> <p>Note: if relevant, express dependency with other components (e.g. workflow needs module registry) Reflect the config of charts </p>"},{"location":"design/sw_design/#interfaces","title":"Interfaces","text":"<ul> <li>Control Flow: Describe how the component starts and terminates, including any interactions during execution (such as interrupts).</li> <li>Data Flow: Explain the input and output data flows, ensuring data structures are linked with control flows and interface components through common data areas or files.</li> </ul> <p>Documentation detailing the API specifications and usage is provided in teh API Design</p> <p>NOte: sequence diagram and link to the api-design document.</p>"},{"location":"design/sw_design/#resources","title":"Resources","text":"<ul> <li>Requirements: Itemize what the component needs from its environment to perform its function, excluding items that are part of the component interface.</li> </ul> <p>NOTE: include docker dependencies, might include maven dependencies (libs)</p>"},{"location":"design/sw_design/#data","title":"Data","text":"<p>Argo Workflows utilises complex internal data structures to orchestrate and manage workflows within Kubernetes. The main structures include:</p> <ul> <li>Workflow Definitions: Defined as custom resources in YAML, these include:</li> <li>Element Descriptions: Identifiers (names), types (e.g., script, container), and dimensions (step hierarchies and dependencies).</li> <li>Relationships: Defines parent-child relationships among steps using dependencies that structure the workflow as a Directed Acyclic Graph (DAG).</li> <li>Value Range: Includes everything from static strings to dynamic outputs from previous steps.</li> <li> <p>Initial Values: Elements such as parameters may have default values specified in the workflow template.</p> </li> <li> <p>Workflow Status: This structure tracks the real-time state of each step within the workflow, indicating whether a step is pending, running, succeeded, or failed.</p> </li> <li> <p>Artifacts: Used to manage data inputs and outputs for each step. Artifacts can be files stored in various storage backends (e.g., S3, Artifactory).</p> </li> </ul> <p>For more detailed information about the internal data structures used by Argo Workflows, including their specifications and how they are utilised within workflows, refer to the official Argo Workflows Field Reference. This documentation provides comprehensive guides on defining and managing workflows, with specific sections on workflow specifications, status monitoring, and artifact handling.</p>"}]}