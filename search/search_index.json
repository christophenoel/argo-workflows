{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Argo Workflow","text":""},{"location":"#overview","title":"Overview","text":"<p>Argo Workflows enables the chaining of container-based modules and workflows. </p> <p>Argo Workflows is a Commercial-Of-The-Shelf (COTS) Kubernetes-native workflow engine that orchestrates parallel jobs on a Kubernetes cluster. </p> <p>The component description is provided in the context of the Operational Hyper-spectral Data Store and Access (OHDSA) platform (Dim-Sum).</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Chaining of Container-Based Modules: Create workflows connecting multiple containerized processes.</li> <li>Seamless Container Interfacing: Facilitate smooth interaction between containers.</li> <li>Workflow Completion Notifications: Provide notifications upon workflow completion (planned in a future version of the DimSum platform)</li> <li>Workflow Monitoring and Management: Monitor and manage workflows.</li> <li>Versatile Workflow Definition Language: Support Directed Acyclic Graph (DAG) models.</li> <li>Dynamic Resource Allocation: Allocate resources dynamically.</li> <li>Retry Strategies: Implement retry strategies for task failures.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To begin using this project, please refer to the Installation Manual.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Detailed documentation is available to help you understand and use the project effectively:</p> <ul> <li>User Manual: A comprehensive guide for end-users on how to use the application.</li> <li>Software Design: Documentation detailing the API specifications and usage.</li> <li>API Design: Documentation detailing the API specifications and usage.</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>For a detailed description of the API, see the API Specifications.</p>"},{"location":"#configuration-and-deployment","title":"Configuration and Deployment","text":"<p>Configuration instructions and deployment guides are provided for different environments:</p> <ul> <li>Helm Charts: Guidelines on using and configuring Helm charts for Kubernetes deployment.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Explore practical examples to better understand how to integrate and use the project's features:</p> <ul> <li>Example Usage</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<p>The component relies on the open source software Argo Workflows (Copyright 2017-2018 The Argo Authors) licensed under the Apache License, Version 2.0 (the \"License\").</p> <p>You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>All work developed within the context of the OHDSA project, including modifications and extensions to the original components, is licensed under the EUROPEAN SPACE AGENCY COMMUNITY LICENSE \u2013 V2.4 PERMISSIVE (TYPE 3).</p>"},{"location":"docker/","title":"Docker Image Deployment for [Project Name]","text":""},{"location":"docker/#overview","title":"Overview","text":"<p>This document outlines the Docker image configuration and deployment instructions for [Component Name]. It includes details on building the Docker image, running containers, and handling common configurations.</p>"},{"location":"docker/#building-the-docker-image","title":"Building the Docker Image","text":"<p>To build the Docker image, navigate to the directory containing the <code>Dockerfile</code> and run the following command:</p> <pre><code>docker build -t [your-image-name]:[tag] .\n</code></pre> <p>Replace <code>[your-image-name]</code> with the appropriate name for your Docker image and <code>[tag]</code> with the desired version or tag.</p>"},{"location":"docker/#running-the-docker-container","title":"Running the Docker Container","text":"<p>Once the image is built, you can run a container based on that image with the following command:</p> <pre><code>docker run -d --name [your-container-name] -p [port]:[port] [your-image-name]:[tag]\n</code></pre> <p>Replace <code>[your-container-name]</code> with a name for your container, <code>[port]</code> with the appropriate port numbers, and <code>[your-image-name]:[tag]</code> with the image name and tag used in the build step.</p>"},{"location":"docker/#additional-resources","title":"Additional Resources","text":"<ul> <li>Docker Documentation</li> </ul>"},{"location":"examples/","title":"Examples for [Project Name]","text":""},{"location":"examples/#overview","title":"Overview","text":"<p>This directory contains various examples to help you understand and utilize the functionalities of [Project Name]. Each example is self-contained and illustrates specific use cases or features of the software.</p>"},{"location":"examples/#list-of-examples","title":"List of Examples","text":"<p>Below are brief descriptions of the examples included in this directory:</p>"},{"location":"examples/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<ul> <li>Description: Demonstrates the basic functions of the application.</li> <li>Path: <code>basic_usage/</code></li> </ul>"},{"location":"examples/#example-2-advanced-features","title":"Example 2: Advanced Features","text":"<ul> <li>Description: Showcases the advanced features and how to configure them for custom use.</li> <li>Path: <code>advanced_features/</code></li> </ul>"},{"location":"examples/#example-3-integration","title":"Example 3: Integration","text":"<ul> <li>Description: Illustrates how to integrate [Project Name] with other software or services.</li> <li>Path: <code>integration/</code></li> </ul>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<p>To run an example, navigate to the example's directory and follow the instructions specified in the <code>README.md</code> file of that directory. Generally, you can run an example using the following commands:</p> <pre><code>cd [example-directory]\n./run_example.sh\n</code></pre>"},{"location":"helm_charts/","title":"Helm Charts for [Project Name]","text":""},{"location":"helm_charts/#overview","title":"Overview","text":"<p>The official chart for Argo Workflows is used to deploy Argo Workflows. It is described and detailed in Argo Workflows Helm Documentation with a list of all properties.</p> <p>The present repository provides the specific parameters used for the DimSum platform. This document only details these parameters and those that need to be provided during installation (depending on the environment).</p> <p>The exact configuration and prerequisites required for the DimSum platform are detailed in the installation manual.</p>"},{"location":"helm_charts/#prerequisites","title":"Prerequisites","text":"<p>Before you can use this Helm chart, ensure you meet the following prerequisites: * S3 Bucket must be deployed and a <code>secret</code> providing the credentials. * Target cluster with Kubernetes 1.23+ (with PV support) * Helm 3.8.0+ installed locally</p>"},{"location":"helm_charts/#installing-the-helm-chart","title":"Installing the Helm Chart","text":"<ol> <li> <p>Clone locally the present GitHub repository, which contains the DimSum <code>charts/values.yaml</code> with some overridden values.</p> <pre><code>git clone https://github.com/argoproj/argo-helm.git /charts\n</code></pre> </li> <li> <p>Add the Helm chart repository:</p> <pre><code>helm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update\n</code></pre> </li> <li> <p>Edit the necessary parameters in charts/values.yml (see further section)</p> </li> <li> <p>Install the official Argo Workflows Helm chart with:</p> <p><pre><code>helm install [release-name] argo/argo-workflows --namespace [namespace] -f /charts/values.yaml \n</code></pre>    Replace <code>[release-name]</code> with a name for your Helm release, and <code>[namespace]</code> with the Kubernetes namespace where you want to deploy.</p> </li> </ol>"},{"location":"helm_charts/#uninstalling-the-chart","title":"Uninstalling the Chart","text":"<p>To remove the deployed Helm chart: <pre><code>helm delete [release-name] --namespace [namespace]\n</code></pre></p>"},{"location":"helm_charts/#configuration-and-installation-details","title":"Configuration and installation details","text":"<p>This section details the parameters that need to be adapted for the environment on which Argo Workflows is deployed. </p>"},{"location":"helm_charts/#artifact-repository-configuration","title":"Artifact Repository Configuration","text":"<p>To configure the artifact repository, you need to set up the <code>artifactRepositoryRef</code> in the <code>values.yaml</code> file. This configuration defines where Argo Workflows will store its artifacts. </p> <p>By default, the workflow uses the artifact repositories defined by the config map <code>artifact-repositories</code> (specifying one or more repositories). Within this ConfigMap, the  default repository must be designated using the annotation workflows.argoproj.io/default-artifact-repository. </p> <p>The following configuration defines the details of the default repository.</p> <p>You need to specify the bucket name, endpoint, and the secrets for the access key and secret key. Here's how it works:</p> <ul> <li>Bucket: The name of the bucket where artifacts will be stored.</li> <li>Endpoint: The endpoint URL for the S3-compatible storage service.</li> <li>Access Key and Secret Key: These are used to authenticate with the storage service. You need to provide the secrets for these keys. The <code>accessKeySecret</code> and <code>secretKeySecret</code> fields specify the names and keys of the Kubernetes secrets containing the access key and secret key, respectively.</li> </ul> <pre><code>artifactRepositoryRef:\n  artifact-repositories:\n    annotations:\n      workflows.argoproj.io/default-artifact-repository: default-artifact-repository\n      default-artifact-repository:\n        s3:\n            bucket: test\n            insecure: true\n            endpoint: l-k8s01-master.spb.spacebel.be:30901 # TBD create an additional property\n            accessKeySecret:\n              name: minio-credentials\n              key: accessKey\n            secretKeySecret:\n              name: minio-credentials\n              key: secretKey\n</code></pre>"},{"location":"helm_charts/#service-account-parameters","title":"Service Account Parameters","text":"<p>IMPORTANT: These parameters should only be modified by experienced users for advanced configurations. </p> <p>Each workflow is associated with a service account that dictates the permissions and actions the workflow can perform within the cluster. </p> <p>It is recommended to create a default service account <code>argo-workflow</code>, assign all required roles for workflows, and bind the role to that default service account.</p> <pre><code>workflow:\n  serviceAccount:\n    create: true\n    name: \"argo-workflow\"\n  rbac:\n    create: true\nserver:\n  rbac:\n    create: true\ncontroller:\n  workflowDefaults:\n    spec:\n      serviceAccountName: argo-workflow\n</code></pre>"},{"location":"helm_charts/#authentication-parameters","title":"Authentication Parameters","text":"<p>Authentication parameters define the authentication modes and settings for Argo Workflows. These parameters are specified in the values.yaml file.</p>"},{"location":"helm_charts/#ingress-parameters","title":"Ingress Parameters","text":"<pre><code>server:\n  ingress:\n    enabled: true\n    annotations:\n      kubernetes.io/ingress.class: nginx\n      # Add other necessary annotations here\n    hosts:\n      - argo.your-domain.com\n    paths:\n      - /\n    tls: \n      - secretName: argo-tls\n        hosts:\n          - argo.your-domain.com\n</code></pre>"},{"location":"helm_charts/#parameters","title":"Parameters","text":"<p>The present section provides the definition of the properties overridden in the yaml file ../charts/values.yaml.</p>"},{"location":"helm_charts/#artefacts-repository-parameters","title":"Artefacts Repository Parameters","text":"Key Type Default Description artifactRepositoryRef object See example above Store artifact in a S3-compliant object store"},{"location":"helm_charts/#service-account-parameters_1","title":"Service Account Parameters","text":"Key Type Value Description workflow.serviceAccount.create bool <code>true</code> Specifies whether a service account should be created workflow.serviceAccount.name string <code>\"argo-workflow\"</code> Service account which is used to run workflows workflow.serviceAccount.pullSecrets list <code>[]</code>  (TBD) Secrets with credentials to pull images from a private registry. Same format as <code>.Values.images.pullSecrets</code> controller.workflowDefaults object <code>{    spec.serviceAccountName: argo-workflow}</code> Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level. Only valid for 2.7+"},{"location":"helm_charts/#authentication-parameters_1","title":"Authentication Parameters","text":"Key Type Default Description server.authModes list <code>[]</code> A list of supported authentication modes. Available values are <code>server</code>, <code>client</code>, or <code>sso</code>. If you provide sso, please configure <code>.Values.server.sso</code> as well."},{"location":"helm_charts/#ingress-configuration","title":"Ingress Configuration","text":"Key Type Default Description server.serviceType string <code>\"ClusterIP\"</code> Service type for server pods server.serviceNodePort string <code>nil</code> Service node port"},{"location":"helm_charts/#additional-resources","title":"Additional Resources","text":"<ul> <li>Helm Documentation</li> <li>Kubernetes Documentation</li> </ul>"},{"location":"installation_manual/","title":"Installation Manual","text":""},{"location":"installation_manual/#overview","title":"Overview","text":"<p>This manual provides comprehensive guidance on how to install and set up Argo Workflow. Ensure all prerequisites are met before proceeding with the installation.</p>"},{"location":"installation_manual/#prerequisites","title":"Prerequisites","text":"<p>Before you begin the installation, ensure that the following software is installed and configured on your system:</p> <ul> <li>Docker</li> <li>Kubernetes</li> <li>Helm</li> <li>Argo CLI</li> </ul> <p>NB: Argo CLI can be downloaded from here: https://github.com/argoproj/argo-workflows/releases/</p>"},{"location":"installation_manual/#build-procedure","title":"Build Procedure","text":"<p>NO Build process required.</p>"},{"location":"installation_manual/#deployment","title":"Deployment","text":"<p>This project uses Helm for deployment on Kubernetes. Follow the steps below to deploy the application using Helm charts.</p> <p>Make sure Helm is installed and set up correctly. For installation instructions, see the official Helm documentation.</p> <p><pre><code>### Verifying the Installation\n\nEnsure the installation was successful, by submitting a workflow to Argo:\n</code></pre> argo -n argo-helm submit --serviceaccount executor https://raw.githubusercontent.com/argoproj/argo-workflows/master/examples/hello-world.yaml --watch <pre><code>### Addition configuration\n\n#### Label Kubernetes nodes\n\nLabels associated with nodes are used by Argo Workflow and Kubernetes to choose on which node a pods must be scheduled.\nHere is an example of how to associate a label with a node:\n</code></pre> kubectl label nodes  accelerator= <pre><code>To list labels associated with nodes:\n</code></pre> kubectl get nodes --show-labels=true <pre><code>## Troubleshooting\n\nOffer common issues and solutions encountered during the installation process.\n\n### Custom resource Definition (CRD) already defined\nCustom resource Definition (CRD) are defined cluster wide and not bound to a namespace. This can cause a conflict when installing multiple instances of Argo Workflows within the same Kubernetes cluster.\nIf CRD are already installed on the Kubernetes cluster, and you don\u2019t want/need to change those, you can install Argo and skip the CRD installation like this:\n</code></pre> helm install --namespace argo-helm argo-workflows argo/argo-workflows --version 0.41.4 --set crds.install=false <pre><code>## Uninstall\n</code></pre> helm uninstall --namespace argo-helm argo-workflows kubectl delete namespace argo-helm ```"},{"location":"kubernetes/","title":"Kubernetes Configuration for [Project Name]","text":""},{"location":"kubernetes/#overview","title":"Overview","text":"<p>This document provides an overview of the Kubernetes configurations used for deploying and managing [Component Name]. It includes details on deployment manifests, service definitions, and other Kubernetes resources critical to the operation of the component.</p>"},{"location":"kubernetes/#configuration-files","title":"Configuration Files","text":"<p>Below is a list of key Kubernetes configuration files included in this directory:</p> <ul> <li><code>deployment.yaml</code>: Defines the Deployment configuration for [Component Name].</li> <li><code>service.yaml</code>: Specifies the Service configuration that provides network access to the component.</li> <li><code>ingress.yaml</code>: (Optional) Manages external access to the services, typically via HTTP.</li> <li><code>configmap.yaml</code>: Contains non-confidential data in key-value pairs that can be consumed by pods.</li> <li><code>secret.yaml</code>: Manages sensitive information, such as passwords and API keys, used by the component.</li> </ul>"},{"location":"kubernetes/#deployment","title":"Deployment","text":"<p>To deploy the component to a Kubernetes cluster, use the following command:</p> <pre><code>kubectl apply -f deployment.yaml\n</code></pre>"},{"location":"kubernetes/#services","title":"Services","text":"<p>To establish the network access to the component, apply the service configuration:</p> <pre><code>kubectl apply -f service.yaml\n</code></pre>"},{"location":"kubernetes/#scaling","title":"Scaling","text":"<p>To scale the component, adjust the <code>replicas</code> field in the <code>deployment.yaml</code> file or use the following command:</p> <pre><code>kubectl scale deployment [deployment-name] --replicas=[number]\n</code></pre>"},{"location":"kubernetes/#updating","title":"Updating","text":"<p>To update the component, modify the relevant Docker image or configuration and apply the changes:</p> <pre><code>kubectl apply -f deployment.yaml\n</code></pre>"},{"location":"kubernetes/#cleanup","title":"Cleanup","text":"<p>To remove the deployed resources from your cluster, use:</p> <pre><code>kubectl delete -f .\n</code></pre>"},{"location":"kubernetes/#additional-resources","title":"Additional Resources","text":"<ul> <li>Kubernetes Documentation</li> <li>Helm Charts for managing complex deployments with Helm.</li> </ul>"},{"location":"tutorial_cli/","title":"CLI Tutorial","text":""},{"location":"tutorial_cli/#argo-cli","title":"Argo CLI","text":"<p>This tutorial demonstrates how to use the Argo CLI to register a workflow template, submit a workflow, monitor its status, and retrieve logs and results.</p>"},{"location":"tutorial_cli/#installing-argo-cli","title":"Installing Argo CLI","text":"<p>The Argo CLI is a command-line interface tool that allows users to interact with Argo Workflows. Follow the steps below to install the Argo CLI:</p> <ol> <li>Download the latest Argo CLI release:</li> </ol> <p>Visit the Argo Workflows GitHub releases page and download the appropriate binary for your operating system.</p> <p><code>Example: curl -sLO https://github.com/argoproj/argo-workflows/releases/latest/download/argo-linux-amd64.gz</code></p> <ol> <li>Unzip the downloaded binary:</li> </ol> <p>Unzip the downloaded binary.</p> <p><code>Example: gunzip argo-linux-amd64.gz</code></p> <ol> <li>Make the binary executable:</li> </ol> <p>Make the unzipped binary executable.</p> <p><code>Example: chmod +x argo-linux-amd64</code></p> <ol> <li>Move the binary to your PATH:</li> </ol> <p>Move the executable binary to a directory included in your system's <code>PATH</code>.</p> <p><code>Example: mv argo-linux-amd64 /usr/local/bin/argo</code></p> <ol> <li>Verify the installation:</li> </ol> <p>Confirm that the Argo CLI has been installed correctly by running the version command.</p> <p><code>Example: argo version</code></p>"},{"location":"tutorial_cli/#workflow-template-operations","title":"Workflow Template operations","text":"<p>The main operations required to manage Workflow Templates are the following:</p> <ul> <li>List Workflow templates: <code>argo template list</code></li> <li>Create Workflow template: <code>argo -n &lt;k8s-namespace&gt; template create &lt;workflow-template-definition.yaml&gt;</code></li> <li>Delete workflow template: <code>argo -n &lt;k8s-namespace&gt; template delete &lt;workflow-template-name&gt;</code></li> </ul>"},{"location":"tutorial_cli/#workflow-operations","title":"Workflow operations","text":"<p>The main operations required to manage Workflows are the following:</p> <ul> <li>List Workflows: <code>argo -n &lt;k8s-namespace&gt; list</code></li> <li>Submit a workflow: <code>argo -n &lt;k8s-namespace&gt; submit &lt;workflow-definition.yaml&gt;</code></li> <li>Get status of a workflow: <code>argo -n &lt;k8s-namespace&gt; get &lt;workflow-name&gt;</code></li> <li>Retrieve logs of a workflow: <code>argo -n &lt;k8s-namespace&gt; logs &lt;workflow-name&gt;</code></li> <li>Delete a workflow: <code>argo -n &lt;k8s-namespace&gt; delete &lt;workflow-name&gt;</code></li> </ul>"},{"location":"tutorial_cli/#register-a-workflow-template","title":"Register a Workflow template","text":"<p>To register a workflow template, we first need to define one.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\nname: hello-world-wft\nspec:\nentrypoint: whalesay\ntemplates:\n- name: whalesay\ninputs:\nparameters:\n- name: message\ncontainer:\nimage: docker/whalesay\ncommand: [ cowsay ]\nargs: [\"{{inputs.parameters.message}}\"]\n</code></pre> <p>See: hello-world-wf-template.yml</p> <p>After defining a workflow template in a YAML file, we can register it using the Argo CLI. To proceed, open a new terminal window at the root of this project and execute the following command:</p> <pre><code>argo -n &lt;k8s-namespace&gt; template create examples/hello-world-template/hello-world-wf-template.yml\n</code></pre> <p>After registering the workflow template, verify its registration with the following command: <pre><code>argo -n &lt;k8s-namespace&gt; template list\n</code></pre></p>"},{"location":"tutorial_cli/#submit-a-workflow","title":"Submit a workflow","text":"<p>Define the workflow in a YAML file as shown below.</p> <p><pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-workflow-\nspec:\n  arguments:\n    parameters:\n      - name: message\n        value: This is a whale message # Message to pass to the template\n  # Reference the WorkflowTemplate\n  workflowTemplateRef:\n    name: hello-world-wft\n</code></pre> See: hello-world-wf.yml</p> <p>Once we have defined a workflow in a YAML file, we can submit it by calling ARGO CLI. To proceed, open a new terminal windows at the root of this project. Then execute the following command:</p> <pre><code>argo -n &lt;k8s-namespace&gt; submit examples/hello-world-template/hello-world-wf.yml\n</code></pre>"},{"location":"tutorial_cli/#monitor-a-workflow","title":"Monitor a workflow","text":"<p>In order to monitor a submitted workflow, we can retrieve its status with the following command: <pre><code>argo -n &lt;k8s-namespace&gt; get &lt;workflow-name&gt;\n</code></pre> Note that <code>&lt;workflow-name&gt;</code> correspond to the name of the workflow (available in the response to the submit workflow operation). In this example, it corresponds to: hello-world-workflow-qltqd</p> <p>In the response, we can observe that <code>Status</code> is <code>Succeeded</code>. meaning that the execution of this workflow terminated successfully.</p> <p>To retrieve the logs associated with this workflow, one can use the following command: <pre><code>argo -n &lt;k8s-namespace&gt; get &lt;workflow-name&gt;\n</code></pre></p>"},{"location":"tutorial_gui/","title":"GUI Tutorial","text":""},{"location":"tutorial_gui/#web-gui-tutorial","title":"WEB GUI Tutorial","text":"<p>In this tutorial we will see how we can leverage the graphical user interface to manage workflow templates, submit and monitor workflows.</p> <p>The first step is to open a web browser and open the url on which your argo workflow web server is exposed. From the Home page you can use the navigation menu displayed on the left to choose the desired section.</p> <p></p> <p>In our case we will select the 'Workflow Templates tab'. From this page we can see the different workflow templates available, create new ones, delete old ones,....</p> <p></p> <p>After deploying the desired workflow template, you can choose the 'Workflow' page (still from the navigation menu on the left).</p> <p></p> <p>From this page, we can see the workflows already submitted, submit a new one, monitor a workflow, delete one,...</p> <p>Now, if we click on a workflow we can see its status, this section can also help us to monitor a workflow during its execution.</p> <p></p> <p>By clicking on the output artifact 'results.txt' we can see its content and download it.</p> <p></p> <p>This concludes this tutorial. You can find more information online on: Argo Workflow - User Guide</p>"},{"location":"tutorial_rest/","title":"REST Tutorial","text":""},{"location":"tutorial_rest/#rest-tutorial","title":"REST Tutorial","text":"<p>In this section, we will see how to use the REST API for managing workflows. For more information on those operations,see API design documentation: API</p> <p>Note: Authentication aspects are not covered in this tutorial. See https://argo-workflows.readthedocs.io/en/latest/argo-server-auth-mode/</p> <p>Postman collection of requests used in this tutorial: Postman DEMO Collection</p> <p>Open the Postman collection HELLOWOLRD-TEMPLATE-EXAMPLE</p>"},{"location":"tutorial_rest/#register-a-workflow-template","title":"Register a Workflow template","text":"<p>In this tutorial, we will use the template located at hello-world-wf-template.yml</p> <p>Once we have defined a workflow template in a YAML file, we can register this template by calling the REST API.</p> <p>Open the Postman collection and select the request located in Template/ Create Workflow Template:</p> <p></p>"},{"location":"tutorial_rest/#submit-a-workflow","title":"Submit a Workflow","text":"<p>Open the Postman request located in Workflow / Submit Workflow:</p> <p></p>"},{"location":"tutorial_rest/#monitor-a-workflow","title":"Monitor a workflow","text":"<p>The REST API provide an operation to retrieve the status of a Workflow including the status of the pods deployed on the Kubernetes cluster.</p> <p>Open the Postman collection and select the request located in Workflow/ Get Status:</p> <p></p> <p>To retrieve the logs associated with a workflow, a REST Operation also exists.</p> <p>Open the Postman collection and select the request located in Workflow/ Get LOGS:</p> <p></p>"},{"location":"user_manual/","title":"User Manual for Argo Workflow","text":""},{"location":"user_manual/#purpose-of-the-software","title":"Purpose of the Software","text":"<p>Argo Workflow is an open-source engine tailored for orchestrating workflows on Kubernetes, enabling users to define, manage, and execute tasks in containers.</p>"},{"location":"user_manual/#operations-environment","title":"Operations Environment","text":"<p>Argo Workflow is executed on a Kubernetes cluster, on which it orchestrates the containers required fo executing the desired workflows.</p>"},{"location":"user_manual/#hardware-configuration","title":"Hardware Configuration","text":"<p>Argo Workflow requires a Kubernetes cluster to operate effectively. </p>"},{"location":"user_manual/#operations-manual","title":"Operations Manual","text":""},{"location":"user_manual/#set-up-and-initialization","title":"Set-up and Initialization","text":"<p>The set-up procedures are described in the Installation Manual.</p>"},{"location":"user_manual/#normal-operations","title":"Normal Operations","text":"<p>The normal operations of the Argo Workflow engine are thoroughly documented on the official Argo Workflows documentation site, with detailed references to various operational fields. </p> <p>The resources below aim to complement the official documentation with guidance for performing activities expected within the DimSum platform:</p> <ul> <li>Workflow Design Manual: This manual explains concepts and provides guidance on designing workflows that include features relevant to the platform, such as workflow templates, artefacts, and retry strategies. Workflow Design Manual</li> <li>API Design: This section describes the operations from the REST API specification expected to be used by the Workflow Management System (WMTS) for submitting workflows. API Design</li> </ul> <p>The subsection below provides a summary of the operations and sequences expected to be used by the WMTS. Additionally, a few tutorials are provided for testing purposes to facilitate the learning of Argo Workflows.</p>"},{"location":"user_manual/#argo-workflow-key-concepts","title":"Argo Workflow Key Concepts","text":"<p>In the context of Argo Workflows, three key concepts are essential: Workflow, template, and Workflow Template.</p> <ul> <li>Workflow: A structured sequence of tasks that defines the steps to be executed. It includes a specification (<code>spec</code>) of the tasks, a current status (<code>state</code>), and may reference a <code>WorkflowTemplate</code>.</li> <li>template: A task within a workflow that acts as a function or method.</li> <li>Workflow Template: A predefined definition of a workflow that can be persisted, submitted, or referenced within other workflows.</li> </ul> <pre>63e31daa-f447-461b-96ee-dfadac30e5e6</pre>"},{"location":"user_manual/#dimsum-platform-main-operations","title":"DimSum Platform Main Operations","text":"<p>This section outlines the operations expected to be executed by the Workflow Management System (WMTS) in the DimSum platform. The process is divided into three parts: registration, execution, and status polling.</p>"},{"location":"user_manual/#registration-of-the-workflow-template","title":"Registration of the Workflow Template","text":"<ol> <li>The WMTS submits the Argo Workflow Template to the Module Registry microservice.</li> <li>The Module Registry registers the workflow template in the DimSum namespace using the Argo Workflow API.</li> </ol> <pre>d47de026-ea3c-4667-b804-efd9bffe05a5</pre>"},{"location":"user_manual/#execution-of-the-workflow","title":"Execution of the Workflow","text":"<ol> <li>The WMTS submits the Workflow using the REST API.</li> <li>The WMTS monitors the Workflow until it is complete.</li> <li>Upon completion, the WMTS retrieves the results.</li> <li>If the execution fails, the WMTS retrieves logs from an operator.</li> </ol> <pre>9c0b790f-cba8-430c-86d1-aa64647650f9</pre>"},{"location":"user_manual/#workflow-notifications","title":"Workflow Notifications","text":"<p>Argo Workflows does not natively support notification mechanisms for status updates, except through implementing a custom container in an exit handler to handle such notifications. However, this functionality can be considered in phase 2 of the project.</p> <pre>28913fc1-d4fb-4ae7-8343-025d399a714e</pre> <p>See Workflow Notifications Documentation for additional details.</p>"},{"location":"user_manual/#normal-termination","title":"Normal Termination","text":"<p>To cease or interrupt the use of Argo Workflow, users can terminate their workflows through the Argo CLI or Argo UI. Using the Argo CLI, the <code>argo stop</code> or <code>argo terminate</code> commands can be employed to stop or terminate a workflow, respectively. Additionally, the Argo UI provides an intuitive interface to manage and halt workflows.</p> <p>To verify if the termination has been normal, users should check the status of the workflow. A normally terminated workflow will have a status of <code>Succeeded</code> or <code>Failed</code>, indicating that the workflow has completed its execution or encountered an error during the process. Users can inspect the workflow status via the Argo CLI with the <code>argo get</code> command or through the Argo UI.</p>"},{"location":"user_manual/#error-conditions","title":"Error Conditions","text":"<p>Common error conditions in Argo Workflow include:</p> <ol> <li>Workflow Fails to Start: This may be due to incorrect workflow definitions or missing dependencies. Detection methods include checking the Argo CLI or UI for error messages indicating validation issues.</li> </ol> <p>Troubleshooting Steps:   - Verify the workflow YAML file for syntax errors.   - Ensure all required parameters and artifacts are correctly defined.   - Use the <code>argo lint</code> command to validate the workflow definition.</p> <ol> <li>Pod Failures: Pods may fail to start or run due to resource constraints, image pull errors, or misconfigurations. Detection involves monitoring pod status in the Kubernetes dashboard or using the Argo CLI.</li> </ol> <p>Troubleshooting Steps:   - Check pod logs using <code>kubectl logs &lt;pod-name&gt;</code> for detailed error messages.   - Ensure the container images are accessible and correctly specified.   - Verify resource requests and limits to ensure the cluster can accommodate the pod.</p> <ol> <li>Timeouts: Workflows or specific steps may time out if they exceed the allotted execution time. Detection methods include checking the workflow status for timeout errors.</li> </ol> <p>Troubleshooting Steps:   - Review and adjust the timeout settings in the workflow definition.   - Optimize the workflow steps to reduce execution time.   - Ensure external services or dependencies are responsive and not causing delays.</p> <ol> <li>Permission Denied Errors: These errors occur when the workflow does not have the necessary permissions to perform certain actions. Detection involves examining the error messages related to access issues.</li> </ol> <p>Troubleshooting Steps:   - Verify the service account permissions and roles assigned to the workflow.   - Update the role-based access control (RBAC) settings to grant the required permissions.   - Check and correct any file or directory permissions within the workflow.</p> <ol> <li>Resource Quota Exceeded: Workflows may fail if they exceed the assigned resource quotas in the cluster. Detection involves checking for resource-related error messages in the workflow status.</li> </ol> <p>Troubleshooting Steps:   - Review the resource quotas assigned to the namespace and adjust if necessary.   - Optimize workflow resource usage to stay within the quota limits.   - Consult with the cluster administrator to request increased resource allocations if needed.</p>"},{"location":"user_manual/#recover-runs","title":"Recover Runs","text":"<p>In the event of workflow failures or interruptions, Argo Workflows provides mechanisms to restart or recover runs, ensuring continuity and minimizing downtime.</p> <ol> <li>Retry Failed Steps: Argo allows users to retry failed steps within a workflow. This can be configured in the workflow YAML by specifying the <code>retryStrategy</code>. Users can define the number of retries and the backoff strategy.</li> </ol> <p>Procedure:   - Edit the workflow definition to include a <code>retryStrategy</code> for the relevant steps.   - Resubmit the workflow using the <code>argo submit</code> command with the updated YAML file.</p> <ol> <li>Resume Suspended Workflows: If a workflow is manually or automatically suspended, it can be resumed using the Argo CLI.</li> </ol> <p>Procedure:   - Use the <code>argo resume &lt;workflow-name&gt;</code> command to resume a suspended workflow.   - Verify the workflow status to ensure it continues from the point of suspension.</p> <ol> <li>Resubmit Failed Workflows: Entire workflows that have failed can be resubmitted. This can be useful if the failure was due to transient issues or if changes have been made to the environment.</li> </ol> <p>Procedure:   - Identify the failed workflow using the <code>argo list</code> command.   - Resubmit the workflow using the <code>argo resubmit &lt;workflow-name&gt;</code> command.   - Monitor the workflow status to ensure it runs to completion.</p> <ol> <li>Workflow Archiving and Retrieval: Argo Workflows can be configured to archive completed workflows. Archived workflows can be retrieved and resubmitted if needed.</li> </ol> <p>Procedure:   - Ensure workflow archiving is enabled in the Argo configuration.   - Retrieve archived workflows using the <code>argo archive get &lt;workflow-name&gt;</code> command.   - Resubmit the archived workflow using the <code>argo submit</code> command.</p> <ol> <li>Handling Emergency Situations: In emergencies, such as cluster failures or critical resource shortages, maintaining workflow continuity is crucial.</li> </ol> <p>Procedure:   - Implement backup and restore strategies for the Argo Workflow controller and associated databases.   - Utilize high-availability (HA) configurations for critical components to minimize downtime.   - Regularly test disaster recovery plans to ensure workflows can be recovered quickly.</p>"},{"location":"user_manual/#tutorials","title":"Tutorials","text":"<p>The following tutorials are provided for testing purposes to facilitate the learning of Argo Workflows:</p> <ul> <li>CLI Tutorial: Offers a brief overview of the main operations of the Argo CLI, a command-line interface that allows users to manage and interact with their workflows efficiently.</li> <li>REST API Tutorial: Demonstrates how to interact with the Argo Workflows through REST API calls, providing examples of common operations such as submitting, monitoring, and managing workflows.</li> <li>GUI Tutorial: Guides users through the graphical user interface of Argo Workflows, showcasing how to visually design, manage, and monitor workflows using the web-based interface.</li> </ul>"},{"location":"workflow_design_manual/","title":"Workflow Design Manual","text":""},{"location":"workflow_design_manual/#workflow-structure","title":"Workflow Structure","text":"<p>A Workflow is composed of several key components:</p> <ul> <li>kind: Defines the type or category of the workflow.</li> <li>metadata: Contains metadata information about the workflow, such as name, namespace, labels, and annotations.</li> <li>spec: The specification of the workflow, which includes detailed definitions of the workflow's behavior and structure.<ul> <li>entrypoint: The starting point or main task of the workflow.</li> <li>templates: A collection of tasks or functions to be executed within the workflow.</li> <li>arguments: Parameters or inputs required for the workflow to execute.</li> </ul> </li> </ul> <pre>cde323d0-834a-4606-a0c7-38c6057535fe</pre>"},{"location":"workflow_design_manual/#templates-structure","title":"Templates Structure","text":"<p>In the context of a Workflow, the <code>templates</code> section defines the tasks to be executed. Each template can have the following components:</p> <ul> <li>name: The name of the template. This is a unique identifier for the template within the workflow.</li> <li>inputs: (Optional) Specifies the inputs required by the template. This can include parameters, artifacts, and other input resources.</li> <li>outputs: (Optional) Specifies the outputs produced by the template. This can include parameters, artifacts, and other output resources.</li> <li>type: (Optional) Indicates the type of template. Common types include:<ul> <li><code>container</code>: Specifies a container to run.</li> <li><code>steps</code>: Defines a set of sequential or parallel steps to be executed.</li> <li><code>dag</code>: Defines a Directed Acyclic Graph of tasks, allowing more complex workflows.</li> <li><code>resource</code>: Manages Kubernetes resources as part of the workflow.</li> <li><code>script</code>: Executes a script.</li> </ul> </li> </ul> <p>The following example shows the details of the container template with inputs and outputs.</p> <pre><code>templates:\n  - name: whalesay\n    container:\n      image: docker/whalesay\n      command: [ cowsay ]\n      args: [ \"hello world\" ]\n    inputs:\n      parameters:\n        - name: message\n          value: \"hello world\"\n    outputs:\n      artifacts:\n        - name: output\n          path: /path/to/output\n</code></pre> <p>In this example, the whalesay template is of type container. It specifies the container image, command, and arguments to run. The template also defines an input parameter (message) and an output artifact (output).</p>"},{"location":"workflow_design_manual/#workflow-parameter-and-argument","title":"Workflow Parameter and Argument","text":"<p>It is important to understand the distinction between parameters and arguments:</p> <ul> <li>Input Parameter: This is a definition of the inputs required by a task or function. For example, in a function definition <code>addFour(int a) {}</code>, <code>a</code> is an input parameter.</li> <li>Output Parameter: This is a definition of the output produced by a task or function. For example, in a function definition <code>int addFour(\u2026) {}</code>, the function returns an integer, which is an output parameter.</li> <li>Argument: This is the actual value provided to a parameter when a task or function is executed. For instance, if you call the function <code>addFour(4)</code>, the value <code>4</code> is the argument.</li> </ul> <p>The following example demonstrates how input parameters and arguments are used within a workflow:</p> <pre><code>spec:\n  entrypoint: whalesay\n  arguments:\n    parameters:\n      - name: message\n        value: hello world\n  templates:\n    - name: whalesay\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: docker/whalesay\n        command: [ cowsay ]\n        args: [ \"{{inputs.parameters.message}}\" ]\n</code></pre> <p>In this example:</p> <ul> <li>The <code>arguments</code> section at the <code>spec</code> level provides live parameter values. Here, the parameter named <code>message</code> is given the value <code>\"hello world\"</code>.</li> <li>The <code>templates</code> section defines a template named <code>whalesay</code>. This template has an input parameter named <code>message</code>.</li> <li>Within the container specification, the command to be run (<code>cowsay</code>) uses the argument provided to the <code>message</code> parameter (<code>\"hello world\"</code>), which is referenced as <code>{{inputs.parameters.message}}</code>.</li> </ul>"},{"location":"workflow_design_manual/#workflowtemplate","title":"WorkflowTemplate","text":"<p>A WorkflowTemplate is a reusable definition of a workflow that can be registered, persisted, and referenced by other workflows. This template defines a workflow's structure and behavior, making it possible to standardize and reuse workflow definitions across multiple workflows.</p> <p>Key Components:</p> <ul> <li>entrypoint: The main task or starting point of the workflow. This defines the initial action to be executed when the workflow starts.</li> <li>inputs: Parameters required by the entrypoint. These parameters define what inputs are necessary for the workflow to run.</li> <li>outputs: Parameters produced by the entrypoint. These parameters define the results or outputs generated by the workflow.</li> </ul>"},{"location":"workflow_design_manual/#external-interface","title":"External Interface","text":"<p>The entrypoint, along with its input and output parameters, defines the external interface of the registerable <code>WorkflowTemplate</code>. This interface specifies what inputs are needed and what outputs will be produced, making it possible for other workflows to interact with this template.</p> <p>When a workflow references a <code>WorkflowTemplate</code>, it must provide the required parameter values as arguments. This ensures that all necessary inputs are available for the workflow to execute correctly.</p> <p></p>"},{"location":"workflow_design_manual/#managing-artefacts","title":"Managing Artefacts.","text":"<p>Requirements: - This tutorial requires that your argo workflow installation uses a default artifact repository (see the installation manual for more information on this) - It also requires that a bucket named 'test' is presetn at the root of your artifact repository (S3 storage).</p> <p>This tutorial covers how we can consume and produce artifacts in a workflow. For this purpose, we will use the artifact-consumer example located in examples/artifact/consumer</p> <p>The workflow Template is as follows: <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: artifact-consumer-wft\nspec:\n  entrypoint: process-artifact\n  templates:\n    - name: process-artifact\n      inputs:\n        parameters:\n          - name: source-path\n          - name: target-path\n        artifacts:\n          - name: input-file\n            path: /tmp/input-file\n            s3:\n              bucket: test\n              key: \"{{inputs.parameters.source-path}}\"\n      outputs:\n        artifacts:\n          - name: output-file\n            path: /tmp/output-file.txt\n            archive:\n              none: { }\n            s3:\n              key: \"{{inputs.parameters.target-path}}\"\n      container:\n        image: docker/whalesay\n        command: [ sh, -c ]\n        args: [\"ls -lh /tmp/input-file &gt;&gt; /tmp/output-file.txt\"]\n</code></pre> See: examples/artifact/consumer/artifact-consumer-wf-template.yml</p> <p>This workflow template defines an input artifact, which references a bucket 'test' and the key is provided as a parameter. <pre><code>artifacts:\n          - name: input-file\n            path: /tmp/input-file\n            s3:\n              bucket: test\n              key: \"{{inputs.parameters.source-path}}\"\n</code></pre></p> <p>It also defines an output artifact, which key is also provided as a parameter: <pre><code>artifacts:\n          - name: output-file\n            path: /tmp/output-file.txt\n            archive:\n              none: { }\n            s3:\n              key: \"{{inputs.parameters.target-path}}\"\n</code></pre></p> <p>The first step, is to register this workflow template: <pre><code>argo -n &lt;k8s-namespace&gt; template create examples/artifact-consumer/artifact-consumer-wf-template.yml\n</code></pre></p> <p>After registering the workflow template, we can submit a workflow. This workflow will have to define the key used for the input (source-path) and output (target-path) artifacts. <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: artifact-consumer-wft-\nspec:\n  arguments:\n    parameters:\n      - name: source-path\n        value: lorem.txt # The specific key for the S3 input object\n      - name: target-path\n        value: result.txt # The specific key where the S3 output object will be written\n\n  # Reference the WorkflowTemplate\n  workflowTemplateRef:\n    name: artifact-consumer-wft\n</code></pre> See: examples/artifact/consumer/artifact-consumer-wf.yml</p> <p>Submit the workflow for execution with the following command: <pre><code>argo -n &lt;k8s-namespace&gt; submit examples/artifact-consumer/artifact-consumer-wf.yml\n</code></pre></p> <p>To monitor the workflow execution progress, use the same commands as in the basic tutorial.</p>"},{"location":"workflow_design_manual/#creating-dag","title":"Creating DAG","text":"<p>The workflow Template is as follows: <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: dag-template\nspec:\n  entrypoint: gdal-manipulation\n  nodeSelector:\n    accelerator: \"example-gpu\"\n  templates:\n    - name: gdal-manipulation\n      podSpecPatch: '{\"containers\":[{\"name\":\"main\", \"resources\":{\"requests\":{\"memory\": \"100M\" }}}]}'\n      retryStrategy:\n        limit: \"10\"\n      inputs:\n        parameters:\n          - name: image-url\n          - name: export-key\n      outputs:\n        artifacts:\n          - name: result\n            globalName: result\n            from: \"{{tasks.stage-out.outputs.artifacts.result}}\"\n      dag:\n        tasks:\n          - name: gdal-translate\n            templateRef:\n              name: gdal-translate-template\n              template: gdal-translate\n            arguments:\n              artifacts:\n                - name: input-dataset\n                  http:\n                    url: \"{{inputs.parameters.image-url}}\"\n          - name: gdal-info\n            dependencies: [gdal-translate]\n            templateRef:\n              name: gdal-info-template\n              template: gdal-info\n            arguments:\n              artifacts:\n                - name: input-dataset\n                  from: \"{{tasks.gdal-translate.outputs.artifacts.processed-dataset}}\"\n          - name: stage-out\n            dependencies: [gdal-info]\n            templateRef:\n              name: stage-out-template\n              template: stage-out\n            arguments:\n              parameters:\n                - name: export-key\n                  value: \"{{inputs.parameters.export-key}}\"\n              artifacts:\n                - name: artifact-to-export\n                  from: \"{{tasks.gdal-translate.outputs.artifacts.processed-dataset}}\"\n</code></pre></p> <p>See: Example</p> <p>This template defines a Direct Acyclic Graph (DAG), each task re-using a workflow template: 1. GDAL Translate: Used to append metadata to a file Template 2. GDAl Info: used to generate a report concernin the file modified in task 1. Template 3. Stage OUT: use to stage out the file modified in task 1.Template</p> <p>The first tak argument is an artifact of type HTTP artifact. The url is provided in the Workflow definition: <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: dag-workflow-\nspec:\n  arguments:\n    parameters:\n      - name: image-url\n        value: https://dagshub.com/DagsHub-Datasets/sentinel-2-l2a-cogs-dataset/raw/e9420f518fa204e0b3665bf66aba30ba38449c2b/s3:/sentinel-cogs/sentinel-s2-l2a-cogs/1/C/CV/2024/1/S2B_1CCV_20240106_0_L2A/B01.tif\n      - name: export-key\n        value: result.tif\n  workflowTemplateRef:\n    name: dag-template\n</code></pre> See: Workflow Example</p>"},{"location":"design/rest_api_design/","title":"Argo Workflows REST API","text":""},{"location":"design/rest_api_design/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Interface Overview</li> <li>Interface Type</li> <li>Producers and Consumers</li> <li>Operations</li> <li>Data Model</li> </ul>"},{"location":"design/rest_api_design/#interface-overview","title":"Interface Overview","text":"<p>The Argo Workflows API provides a robust platform for defining, managing, and executing complex workflows on Kubernetes. It is designed to enable users to automate the orchestration of containerized tasks, integrating seamlessly with other systems and services within a Kubernetes environment. </p> <p>The API allows for the creation, submission, monitoring, and management of workflows, facilitating tasks such as data processing, CI/CD pipelines, and machine learning workflows. Argo Workflows is designed to be highly scalable and flexible, supporting a wide range of use cases and providing extensive customization options through its API.</p>"},{"location":"design/rest_api_design/#interface-type","title":"Interface Type","text":"<ul> <li>Description: REST API</li> <li>Communication Protocols: HTTP/HTTPS</li> </ul>"},{"location":"design/rest_api_design/#producers-and-consumers","title":"Producers and Consumers","text":"<p>Producers: - CI/CD systems that need to automate deployment pipelines. - Data processing systems that require complex task orchestration. - Machine learning platforms that automate model training and deployment workflows. - Users and developers who define and manage workflows through the API.</p> <p>Consumers: - Monitoring systems that track the status and progress of workflows. Logging systems that aggregate and analyze workflow logs. Notification services that alert users about workflow events and statuses. Reporting tools that generate insights based on workflow executions. Users and administrators who retrieve workflow status, logs, and results through the API.</p>"},{"location":"design/rest_api_design/#operations","title":"Operations","text":"<p>Detail each operation provided by the API, including method names, request and response formats, and a brief description of their functionality.</p>"},{"location":"design/rest_api_design/#example-operation-getitem","title":"Example Operation: GetItem","text":"<ul> <li>HTTP Method: <code>GET</code></li> <li>Endpoint: <code>/items/{id}</code></li> <li>Description: Retrieves an item by its unique ID.</li> <li>Request Parameters:<ul> <li><code>id</code> (path): The unique identifier for the item.</li> </ul> </li> <li>Response:<ul> <li>Success (200 OK):   <pre><code>{\n  \"id\": \"item1\",\n  \"name\": \"Example Item\",\n  \"description\": \"A sample item in the catalog.\"\n}\n</code></pre></li> <li>Error (404 Not Found):   <pre><code>{\n  \"error\": \"Item not found.\"\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"design/rest_api_design/#workflow-template-creation","title":"Workflow Template creation","text":"<p>The following operation is used to register a reusable workflow template by providing the target namespace as a path parameter and the workflow template in the request's body.</p> <p>POST /api/v1/workflow-templates/{namespace} </p> Parameter In Type Required Description namespace path string true Namespace of the workflow template. body body object true Workflow template object to create."},{"location":"design/rest_api_design/#list-available-workflow-templates","title":"List available Workflow Templates","text":"<p>The following operation list the reusable workflow templates already available. This request requires to provide the namespace as a path parameter, other parameters must be provided as query parameters and are optionals parameters used to filter the list returned by this operation.</p> <p>GET /api/v1/workflow-templates/{namespace}</p> Parameter In Type Required Description namespace path string true Namespace of the workflow template."},{"location":"design/rest_api_design/#describe-workflow-template","title":"Describe Workflow Template","text":"<p>The following operation provide information about a specific workflow template.</p> <p>GET /api/v1/workflow-templates/{namespace}/{name}</p> Parameter In Type Required Description namespace path string true Namespace of the workflow template. name path string true Name of the workflow template."},{"location":"design/rest_api_design/#list-workflows","title":"List Workflows","text":"<p>The following operation list all workflows submitted in a specific namespace.</p> <p>GET /api/v1/workflows/{namespace}</p> Parameter In Type Required Description namespace path string true Namespace of the workflows."},{"location":"design/rest_api_design/#submit-workflow","title":"Submit Workflow","text":"<p>The following operation submit a workflow for execution in the namespace specified.</p> <p>POST /api/v1/workflows/{namespace}</p> Parameter In Type Required Description namespace path string true Namespace of the workflows. body body object true Workflow object to submit."},{"location":"design/rest_api_design/#delete-workflow","title":"Delete Workflow","text":"<p>The following operation deletes a workflow in the namespace specified.</p> <p>DELETE /api/v1/workflows/{namespace}/{name}</p> Parameter In Type Required Description namespace path string true Namespace of the workflow. name path string true Name of the workflow."},{"location":"design/rest_api_design/#retrieve-workflow-status","title":"Retrieve Workflow Status","text":"<p>The following operation retrieves the status of the desired workflow running in the namespace specified.</p> <p>GET /api/v1/workflows/{namespace}/{name}/status</p> Parameter In Type Required Description namespace path string true Namespace of the workflow. name path string true Name of the workflow."},{"location":"design/rest_api_design/#retrieve-workflow-logs","title":"Retrieve Workflow Logs","text":"<p>The following operation retrieves the logs of the desired workflow running in the namespace specified. To retrieve logs from the desired step, specify the 'logOptions.container' query parameter with the container name corresponding to the desired step.</p> <p>GET /api/v1/workflows/{namespace}/{name}/logs</p> Parameter In Type Required Description namespace path string true Namespace of the workflow. name path string true Name of the workflow."},{"location":"design/rest_api_design/#data-model","title":"Data Model","text":"<p>The full reference of the data model is provided on https://argo-workflows.readthedocs.io/en/latest/fields/</p>"},{"location":"design/rest_api_spec/","title":"REST API Specification","text":""},{"location":"design/rest_api_spec/#argo-workflows-api-version","title":"Argo Workflows API VERSION","text":"<p>Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. For more information, please see https://argo-workflows.readthedocs.io/en/latest/</p>"},{"location":"design/rest_api_spec/#archivedworkflowservice","title":"ArchivedWorkflowService","text":""},{"location":"design/rest_api_spec/#get-apiv1archived-workflows","title":"GET /api/v1/archived-workflows","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namePrefix</code> query None No <code>namespace</code> query None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1archived-workflows-label-keys","title":"GET /api/v1/archived-workflows-label-keys","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>namespace</code> query None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1archived-workflows-label-values","title":"GET /api/v1/archived-workflows-label-values","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> query None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1archived-workflowsuid","title":"GET /api/v1/archived-workflows/{uid}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>name</code> query None No <code>namespace</code> query None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1archived-workflowsuid","title":"DELETE /api/v1/archived-workflows/{uid}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>namespace</code> query None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1archived-workflowsuidresubmit","title":"PUT /api/v1/archived-workflows/{uid}/resubmit","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1archived-workflowsuidretry","title":"PUT /api/v1/archived-workflows/{uid}/retry","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#clusterworkflowtemplateservice","title":"ClusterWorkflowTemplateService","text":""},{"location":"design/rest_api_spec/#get-apiv1cluster-workflow-templates","title":"GET /api/v1/cluster-workflow-templates","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1cluster-workflow-templates","title":"POST /api/v1/cluster-workflow-templates","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1cluster-workflow-templateslint","title":"POST /api/v1/cluster-workflow-templates/lint","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1cluster-workflow-templatesname","title":"GET /api/v1/cluster-workflow-templates/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>getOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>name</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1cluster-workflow-templatesname","title":"PUT /api/v1/cluster-workflow-templates/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No DEPRECATED: This field is ignored. <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1cluster-workflow-templatesname","title":"DELETE /api/v1/cluster-workflow-templates/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>name</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#cronworkflowservice","title":"CronWorkflowService","text":""},{"location":"design/rest_api_spec/#get-apiv1cron-workflowsnamespace","title":"GET /api/v1/cron-workflows/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1cron-workflowsnamespace","title":"POST /api/v1/cron-workflows/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1cron-workflowsnamespacelint","title":"POST /api/v1/cron-workflows/{namespace}/lint","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1cron-workflowsnamespacename","title":"GET /api/v1/cron-workflows/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>getOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1cron-workflowsnamespacename","title":"PUT /api/v1/cron-workflows/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No DEPRECATED: This field is ignored. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1cron-workflowsnamespacename","title":"DELETE /api/v1/cron-workflows/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1cron-workflowsnamespacenameresume","title":"PUT /api/v1/cron-workflows/{namespace}/{name}/resume","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1cron-workflowsnamespacenamesuspend","title":"PUT /api/v1/cron-workflows/{namespace}/{name}/suspend","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#eventsourceservice","title":"EventSourceService","text":""},{"location":"design/rest_api_spec/#get-apiv1event-sourcesnamespace","title":"GET /api/v1/event-sources/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1event-sourcesnamespace","title":"POST /api/v1/event-sources/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1event-sourcesnamespacename","title":"GET /api/v1/event-sources/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1event-sourcesnamespacename","title":"PUT /api/v1/event-sources/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1event-sourcesnamespacename","title":"DELETE /api/v1/event-sources/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1streamevent-sourcesnamespace","title":"GET /api/v1/stream/event-sources/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1streamevent-sourcesnamespacelogs","title":"GET /api/v1/stream/event-sources/{namespace}/logs","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>eventName</code> query None No optional - only return entries for this event name (e.g. `example`). <code>eventSourceType</code> query None No optional - only return entries for this event source type (e.g. `webhook`). <code>grep</code> query None No optional - only return entries where `msg` matches this regular expression. <code>name</code> query None No optional - only return entries for this event source. <code>namespace</code> path None No <code>podLogOptions.container</code> query None No The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. <code>podLogOptions.follow</code> query None No Follow the log stream of the pod. Defaults to false. +optional. <code>podLogOptions.insecureSkipTLSVerifyBackend</code> query None No insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. <code>podLogOptions.limitBytes</code> query None No If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. <code>podLogOptions.previous</code> query None No Return previous terminated container logs. Defaults to false. +optional. <code>podLogOptions.sinceSeconds</code> query None No A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. <code>podLogOptions.sinceTime.nanos</code> query None No Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. <code>podLogOptions.sinceTime.seconds</code> query None No Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. <code>podLogOptions.tailLines</code> query None No If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime +optional. <code>podLogOptions.timestamps</code> query None No If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#eventservice","title":"EventService","text":""},{"location":"design/rest_api_spec/#post-apiv1eventsnamespacediscriminator","title":"POST /api/v1/events/{namespace}/{discriminator}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No The event itself can be any data. <code>discriminator</code> path None No Optional discriminator for the io.argoproj.workflow.v1alpha1. This should almost always be empty. Used for edge-cases where the event payload alone is not provide enough information to discriminate the event. This MUST NOT be used as security mechanism, e.g. to allow two clients to use the same access token, or to support webhooks on unsecured server. Instead, use access tokens. This is made available as `discriminator` in the event binding selector (`/spec/event/selector)` <code>namespace</code> path None No The namespace for the io.argoproj.workflow.v1alpha1. This can be empty if the client has cluster scoped permissions. If empty, then the event is \"broadcast\" to workflow event binding in all namespaces. <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflow-event-bindingsnamespace","title":"GET /api/v1/workflow-event-bindings/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#infoservice","title":"InfoService","text":""},{"location":"design/rest_api_spec/#get-apiv1info","title":"GET /api/v1/info","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1trackingevent","title":"POST /api/v1/tracking/event","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1userinfo","title":"GET /api/v1/userinfo","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1version","title":"GET /api/v1/version","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#sensorservice","title":"SensorService","text":""},{"location":"design/rest_api_spec/#get-apiv1sensorsnamespace","title":"GET /api/v1/sensors/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1sensorsnamespace","title":"POST /api/v1/sensors/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1sensorsnamespacename","title":"GET /api/v1/sensors/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>getOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1sensorsnamespacename","title":"PUT /api/v1/sensors/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1sensorsnamespacename","title":"DELETE /api/v1/sensors/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1streamsensorsnamespace","title":"GET /api/v1/stream/sensors/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1streamsensorsnamespacelogs","title":"GET /api/v1/stream/sensors/{namespace}/logs","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>grep</code> query None No option - only return entries where `msg` contains this regular expressions. <code>name</code> query None No optional - only return entries for this sensor name. <code>namespace</code> path None No <code>podLogOptions.container</code> query None No The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. <code>podLogOptions.follow</code> query None No Follow the log stream of the pod. Defaults to false. +optional. <code>podLogOptions.insecureSkipTLSVerifyBackend</code> query None No insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. <code>podLogOptions.limitBytes</code> query None No If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. <code>podLogOptions.previous</code> query None No Return previous terminated container logs. Defaults to false. +optional. <code>podLogOptions.sinceSeconds</code> query None No A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. <code>podLogOptions.sinceTime.nanos</code> query None No Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. <code>podLogOptions.sinceTime.seconds</code> query None No Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. <code>podLogOptions.tailLines</code> query None No If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime +optional. <code>podLogOptions.timestamps</code> query None No If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. <code>triggerName</code> query None No optional - only return entries for this trigger. <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#workflowservice","title":"WorkflowService","text":""},{"location":"design/rest_api_spec/#get-apiv1streameventsnamespace","title":"GET /api/v1/stream/events/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflow-eventsnamespace","title":"GET /api/v1/workflow-events/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>fields</code> query None No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflowsnamespace","title":"GET /api/v1/workflows/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>fields</code> query None No Fields to be included or excluded in the response. e.g. \"items.spec,items.status.phase\", \"-items.status.nodes\". <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1workflowsnamespace","title":"POST /api/v1/workflows/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1workflowsnamespacelint","title":"POST /api/v1/workflows/{namespace}/lint","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1workflowsnamespacesubmit","title":"POST /api/v1/workflows/{namespace}/submit","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflowsnamespacename","title":"GET /api/v1/workflows/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>fields</code> query None No Fields to be included or excluded in the response. e.g. \"spec,status.phase\", \"-status.nodes\". <code>getOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1workflowsnamespacename","title":"DELETE /api/v1/workflows/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>force</code> query None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflowsnamespacenamelog","title":"GET /api/v1/workflows/{namespace}/{name}/log","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>grep</code> query None No <code>logOptions.container</code> query None No The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. <code>logOptions.follow</code> query None No Follow the log stream of the pod. Defaults to false. +optional. <code>logOptions.insecureSkipTLSVerifyBackend</code> query None No insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. <code>logOptions.limitBytes</code> query None No If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. <code>logOptions.previous</code> query None No Return previous terminated container logs. Defaults to false. +optional. <code>logOptions.sinceSeconds</code> query None No A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. <code>logOptions.sinceTime.nanos</code> query None No Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. <code>logOptions.sinceTime.seconds</code> query None No Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. <code>logOptions.tailLines</code> query None No If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime +optional. <code>logOptions.timestamps</code> query None No If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. <code>name</code> path None No <code>namespace</code> path None No <code>podName</code> query None No <code>selector</code> query None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenameresubmit","title":"PUT /api/v1/workflows/{namespace}/{name}/resubmit","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenameresume","title":"PUT /api/v1/workflows/{namespace}/{name}/resume","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenameretry","title":"PUT /api/v1/workflows/{namespace}/{name}/retry","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenameset","title":"PUT /api/v1/workflows/{namespace}/{name}/set","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenamestop","title":"PUT /api/v1/workflows/{namespace}/{name}/stop","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenamesuspend","title":"PUT /api/v1/workflows/{namespace}/{name}/suspend","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflowsnamespacenameterminate","title":"PUT /api/v1/workflows/{namespace}/{name}/terminate","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflowsnamespacenamepodnamelog","title":"GET /api/v1/workflows/{namespace}/{name}/{podName}/log","text":"<p>DEPRECATED: Cannot work via HTTP if podName is an empty string. Use WorkflowLogs.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>grep</code> query None No <code>logOptions.container</code> query None No The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. <code>logOptions.follow</code> query None No Follow the log stream of the pod. Defaults to false. +optional. <code>logOptions.insecureSkipTLSVerifyBackend</code> query None No insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to.  This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet.  If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. <code>logOptions.limitBytes</code> query None No If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. <code>logOptions.previous</code> query None No Return previous terminated container logs. Defaults to false. +optional. <code>logOptions.sinceSeconds</code> query None No A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. <code>logOptions.sinceTime.nanos</code> query None No Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. <code>logOptions.sinceTime.seconds</code> query None No Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. <code>logOptions.tailLines</code> query None No If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime +optional. <code>logOptions.timestamps</code> query None No If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. <code>name</code> path None No <code>namespace</code> path None No <code>podName</code> path None No <code>selector</code> query None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#workflowtemplateservice","title":"WorkflowTemplateService","text":""},{"location":"design/rest_api_spec/#get-apiv1workflow-templatesnamespace","title":"GET /api/v1/workflow-templates/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>listOptions.allowWatchBookmarks</code> query None No allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. +optional. <code>listOptions.continue</code> query None No The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. <code>listOptions.fieldSelector</code> query None No A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. <code>listOptions.labelSelector</code> query None No A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. <code>listOptions.limit</code> query None No limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. <code>listOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.resourceVersionMatch</code> query None No resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>listOptions.timeoutSeconds</code> query None No Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. <code>listOptions.watch</code> query None No Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. <code>namePattern</code> query None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1workflow-templatesnamespace","title":"POST /api/v1/workflow-templates/{namespace}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#post-apiv1workflow-templatesnamespacelint","title":"POST /api/v1/workflow-templates/{namespace}/lint","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-apiv1workflow-templatesnamespacename","title":"GET /api/v1/workflow-templates/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>getOptions.resourceVersion</code> query None No resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset +optional <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#put-apiv1workflow-templatesnamespacename","title":"PUT /api/v1/workflow-templates/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>body</code> body None No <code>name</code> path None No DEPRECATED: This field is ignored. <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#delete-apiv1workflow-templatesnamespacename","title":"DELETE /api/v1/workflow-templates/{namespace}/{name}","text":"<p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>deleteOptions.dryRun</code> query None No When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. <code>deleteOptions.gracePeriodSeconds</code> query None No The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. <code>deleteOptions.orphanDependents</code> query None No Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. <code>deleteOptions.preconditions.resourceVersion</code> query None No Specifies the target ResourceVersion +optional. <code>deleteOptions.preconditions.uid</code> query None No Specifies the target UID. +optional. <code>deleteOptions.propagationPolicy</code> query None No Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. <code>name</code> path None No <code>namespace</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#artifactservice","title":"ArtifactService","text":""},{"location":"design/rest_api_spec/#get-artifact-filesnamespaceiddiscriminatoridnodeidartifactdiscriminatorartifactname","title":"GET /artifact-files/{namespace}/{idDiscriminator}/{id}/{nodeId}/{artifactDiscriminator}/{artifactName}","text":"<p>Get an artifact.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>artifactDiscriminator</code> path None No <code>artifactName</code> path None No <code>id</code> path None No <code>idDiscriminator</code> path None No <code>namespace</code> path None No <code>nodeId</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-artifacts-by-uiduidnodeidartifactname","title":"GET /artifacts-by-uid/{uid}/{nodeId}/{artifactName}","text":"<p>Get an output artifact by UID.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>artifactName</code> path None No <code>nodeId</code> path None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-artifactsnamespacenamenodeidartifactname","title":"GET /artifacts/{namespace}/{name}/{nodeId}/{artifactName}","text":"<p>Get an output artifact.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>artifactName</code> path None No <code>name</code> path None No <code>namespace</code> path None No <code>nodeId</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-input-artifacts-by-uiduidnodeidartifactname","title":"GET /input-artifacts-by-uid/{uid}/{nodeId}/{artifactName}","text":"<p>Get an input artifact by UID.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>artifactName</code> path None No <code>nodeId</code> path None No <code>uid</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/rest_api_spec/#get-input-artifactsnamespacenamenodeidartifactname","title":"GET /input-artifacts/{namespace}/{name}/{nodeId}/{artifactName}","text":"<p>Get an input artifact.</p> <p>Input parameters</p> Parameter In Type Default Nullable Description <code>BearerToken</code> header string N/A No <code>artifactName</code> path None No <code>name</code> path None No <code>namespace</code> path None No <code>nodeId</code> path None No <p> Response 200 OK </p> <p> Other responses </p>"},{"location":"design/sw_design/","title":"Software Design Document for Argo-Workflow","text":""},{"location":"design/sw_design/#table-of-contents","title":"Table of Contents","text":"<ul> <li>General</li> <li>Component Identifier</li> <li>Type</li> <li>Purpose</li> <li>Function</li> <li>Subordinates</li> <li>Dependencies</li> <li>Interfaces</li> <li>Resources</li> <li>Data</li> </ul>"},{"location":"design/sw_design/#general","title":"General","text":"<p>This section provides a detailed look at the design aspects of each component used in the software. </p>"},{"location":"design/sw_design/#component-identifier","title":"Component Identifier","text":"<ul> <li>Description: Argo Workflows enables the chaining of container-based modules and workflows.</li> <li>Naming Convention: Argo-Workflows</li> </ul>"},{"location":"design/sw_design/#type","title":"Type","text":"<ul> <li>Logical Characteristics: Argo Workflows is the main functional component implementing the Data Processing Environment sub-system of the OHDSA platform.</li> <li>Physical Characteristics: Argo Workflows is a Commercial-Of-The-Shelf (COTS) Kubernetes-native workflow engine that orchestrates parallel jobs on a Kubernetes cluster. It uses Kubernetes resources like custom resources, pods, jobs, ConfigMaps, and Secrets to implement and manage workflows. Each workflow is defined as a Workflow object, which dictates the sequence of tasks to be executed. These tasks are encapsulated as Kubernetes Jobs or Pods, making each step of the workflow a discrete, schedulable unit under Kubernetes management.</li> </ul>"},{"location":"design/sw_design/#purpose","title":"Purpose","text":"<p>Argo Workflows component is designed to fulfill a pivotal role in managing and executing complex workflows that integrate various modules within Docker containers. The purpose of Argo Workflows is to enable users to submit both simple and elaborate workflows with a versatile Workflow Definition Language that accommodates Directed Acyclic Graph models, dynamic resource allocation, retry strategies, the utilisation of specialized hardware, and completion notifications. </p>"},{"location":"design/sw_design/#function","title":"Function","text":"<p>Argo Workflows is a container-native workflow engine for Kubernetes, designed to orchestrate parallel jobs in a cloud environment. Here are its major functions:</p> <ul> <li>Workflow Orchestration: Executes workflows where each step is a container, handling sequential, parallel, or conditionally executed steps.   Each Step and Directed Acyclic Graph (DAG) Task triggers the creation of a Pod.    Each Pod comprises three containers:</li> <li>Main Container: This container executes the user-specified image. The argoexec utility is volume-mounted within this container and functions as the primary command, invoking the configured command as a subprocess.</li> <li>Init Container: Known as an InitContainer, this container is responsible for fetching artifacts and parameters, making them accessible to the main container.</li> <li>Wait Container: This container is tasked with performing necessary cleanup operations, including the preservation of parameters and artifacts.</li> </ul> <p>TBD: add diagram to represent container usage</p> <p>For more information, see: https://argo-workflows.readthedocs.io/en/stable/architecture/</p> <ul> <li> <p>DAG Execution: Argo Workflows can manage task dependencies using Directed Acyclic Graphs to ensure optimal execution order.In a DAG template, tasks without any dependency will be run immediately.</p> </li> <li> <p>Resource Optimization: Dynamically allocates resources based on task demands, optimizing cluster resource use.   To achieve resource usage optimization, one can leverage Kubernetes resource mechanisms (limits and requests). </p> </li> <li> <p>Error Handling and Retry Logic: Argo Workflow provides built-in mechanisms for error recovery and task retries.</p> </li> <li> <p>Monitoring: Argo produces metrics that provide information on the controller's status. As those metrics follow the same format as required by prometheus, those can be integrated with Prometheus.   Two types of metrics are emitted by Argo: </p> </li> <li>Controller metrics: concerns the state of the controller;</li> <li>Custom metrics: regards the state of a Workflow, or a series of Workflows. Those metrics can be defined on the Workflow/Step/Task emitting the metric. using the same name and help string, is a required by prometheus to track the metrics over time. </li> </ul> <p>For more information, see: See: https://argo-workflows.readthedocs.io/en/stable/metrics/</p> <ul> <li>Logging:  It is officially not recommended to rely on Argo to archive logs as it is a naive solution, not designed for indexing, searching, and storing logs (see: https://argo-workflows.readthedocs.io/en/stable/configure-archive-logs/).  In a Kubernetes environment, logs can be forwarded by an agent running on the node (see: https://kubernetes.io/docs/concepts/cluster-administration/logging/#using-a-node-logging-agent).  This agent can forward logs to be saved and indexed for a future usage. Such solution can be provided by Fluentd (acting as the agent forwarding logs). Such logs can be forwarded to ElasticSearch (ELK) which supports storing, indexing and searching capabilities.</li> </ul> <p>TBD: add diagram to represent logging workflow</p> <ul> <li> <p>Role-Based Access Control (RBAC): Utilizes Kubernetes RBAC to control access to workflow execution and management.   All users of the Argo Server must use a service account in order to interact with the Argo Controller. A single service account can be shared by multiple users, as it is used to list  possible actions a user can do.   Rules  defined in Argo can associate a user (using their OIDC group) to a service account in the same namespace as Argo server by annotating the desired service account. By using such rules, users from the OIDC provider are associated to the appropriate service account, with which they can interact with Argo Workflow server to manage workflows.   For more information, see: https://argo-cd.readthedocs.io/en/stable/operator-manual/user-management/keycloak/</p> </li> <li> <p>Artifacts Argo supports staging-in, staging-out and passing artifacts between steps thanks to artifact repositories. An artifact repository can be used with any S3 compatible API (like Minio). Artifact repositories are defined in Kubernetes configmaps and referenced in workflow templates in order to reduce information duplication and simplify artifact repository usage. It also supports other interfaces such as GIT. For more information,see: https://argo-workflows.readthedocs.io/en/stable/configure-artifact-repository/</p> </li> <li> <p>Notification A default Exit handler can be configured in order to send notifications (such as emails or anything) to notify the completion of a workflow execution (failure/success). The exit handler is defined as a container executing a command, which permits to do anything with the end of the workflow, making it possible to communicate over any protocol.   https://argo-workflows.readthedocs.io/en/stable/workflow-notifications/</p> </li> </ul> <p>For more detailed information, visit the official Argo Workflows documentation.</p> <p>The following subsections focus on the implementation of the OHDSA specific capabilities supported by the workflow engine.</p>"},{"location":"design/sw_design/#api","title":"API","text":"<p>Argo Workflow provide a REST API endpoint with plenty operations, in which we can find the following ones: - Workflow Template creation - List available Workflow Templates  - Describe Workflow Template - List Workflows - Submit Workflow - Delete Workflow - Retrieve Workflow Status - Retrieve Workflow Logs</p> <p>For more information on those operations,see API design documentation: API</p>"},{"location":"design/sw_design/#reusable-workflow-templates","title":"Reusable Workflow Templates","text":"<p>TBD: Workflow designer prepares reusable workflow templates for standardizing and streamlining reusable steps</p> <p>Argo Workflow supports defining workflow templates reused in workflow definitions. The REST API provide an operation to register such workflow template. For more information on this, see section 'Workflow Template creation' in the API Design document: API The following example represents a workflow template definition: <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: hello-world-wft\nspec:\n  entrypoint: whalesay\n  templates:\n    - name: whalesay\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: docker/whalesay\n        command: [ cowsay ]\n        args: [\"{{inputs.parameters.message}}\"]\n</code></pre> See: Example</p>"},{"location":"design/sw_design/#dag-workflow-creation","title":"DAG Workflow Creation","text":"<p>Workflow designer creates workflows using a Directed Acyclic Graph (DAG) model. Workflow templates can be referenced in DAG workflows in order to define workflow's tasks.</p> <p>The following example represents a DAG workflow definition: <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: dag-test-workflow-\nspec:\n  entrypoint: whalesay\n  templates:\n    - name: whalesay\n      dag:\n        tasks:\n          - name: call-wf-template-1\n            templateRef:\n              name: wf-template-1\n              template: whalesay-workflow-template\n            arguments:\n              parameters:\n                - name: message\n                  value: \"hello world\"\n</code></pre></p>"},{"location":"design/sw_design/#data-artefacts","title":"Data Artefacts","text":"<p>For managing input and output data, the following principles are applied: 1. Data Inputs as Artefacts: In a WorkflowTemplate, input data should be provided as an artifact, which is submitted as an argument by the workflow calling this template. See: Example 2. Intermediate Artefacts: In a workflow template with multiple steps, intermediate artifacts are stored in the default repository (fast storage). See: Example 3. Creating Artefacts: Workflows (which can call the templates) can create artifacts from: A key in the default S3 repository. See: Example A key in a specified internal repository. SeeSee: Example Remote sources such as S3 or HTTP, offering flexibility. See: Example 4. Credential Agnosticism: WorkflowTemplates do not handle credentials. External artifacts (HTTP, S3) are managed by a workflow generated by the Processing Gateway.</p> <p>TBD interfacing with fast / slow storage areas on S3 and management of artefacts Show how to define 2 multiple artifact repository (fast &amp; slow) +configure one as the default one.</p> <p>Artifact repositories are configured within a Kuebrnetes configmap within the same namespace as the workflow or within the management namespace. <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: artifact-repositories\n  annotations:\n    workflows.argoproj.io/default-artifact-repository: slow-s3-artifact-repository\ndata:\n  slow-s3-artifact-repository: |\n    s3:\n      bucket: argo-bucket\n      endpoint: slow-minio:9000\n      insecure: true\n      accessKeySecret:\n        name: my-minio-cred\n        key: accesskey\n      secretKeySecret:\n        name: my-minio-cred\n        key: secretkey\n  fast-s3-artifact-repository: |\n    s3:\n      bucket: argo-bucket\n      endpoint: fast-minio:9000\n      insecure: true\n      accessKeySecret:\n        name: my-minio-cred\n        key: accesskey\n      secretKeySecret:\n        name: my-minio-cred\n        key: secretkey\n</code></pre></p> <ul> <li>TBD:Steps I/O Mappings: Workflow designer maps intermediary steps data outputs to inputs steps of the workflow.</li> <li>TBD:Data Transfer Steps: Workflow designer integrates data stage-in and stage-out steps managing interactions between persistent storage system (for external interfaces), intermediate data storage system (for intermediate steps).</li> </ul>"},{"location":"design/sw_design/#retry-strategy","title":"Retry Strategy","text":"<p>TBD Workflow designer implements retry strategies for failed or errored workflow steps List the different strategies +show examples (retry number, backoff,...)</p> <p>The retry strategy is used to decide in which case a step is retried.</p> <p>Limit parameter: A limit parameter san be specified in order to limit the number of reties.</p> <p>Retry Policy: In addition to this limit parameter, one can define a retry policy. Retry policies in Argo Workflows are configured using the retryPolicy parameter defined in Workflow specification. It determines the conditions under which failed steps should be retried. The available retry policies include: - Always: Retries all failed steps, regardless of the failure type. - OnFailure: Retries steps where the main container is marked as failed by Kubernetes. - OnError: Retries steps that encounter errors related to the Argo controller, or steps where the init or wait containers fail. - OnTransientError: Retries steps that encounter transient errors or errors that match the pattern specified in the TRANSIENT_ERROR_PATTERN environment variable.</p> <p>Expressions:In addition to retry policies, retries in Argo Workflows can be controlled using expressions. These expressions have access to the following variables: - lastRetry.exitCode: The exit code of the last retry, or \"-1\" if not available.  - lastRetry.status: The phase of the last retry, which can be \"Error\" or \"Failed\". - lastRetry.duration: The duration of the last retry, measured in seconds.  - lastRetry.message: The message output from the last retry.</p> <p>If the expression evaluates to false, the step will not be retried. The result of the expression is logically combined (using a logical AND) with the retryPolicy. Both the expression and the retry policy must evaluate to true for a retry to occur.</p> <p>Backoff: The backoff parameter can be used to avoid too frequent retries too soon by introducing a delay between retries, thereby preventing immediate subsequent retry attempts.</p> <p>TBD: add example of retry strategy</p> <p>For more information, see: https://argo-workflows.readthedocs.io/en/stable/retries/</p>"},{"location":"design/sw_design/#notification","title":"Notification","text":"<p>TBD: describe how exit handler can be used to send notification of any type (using a container to send a AMQP message)</p>"},{"location":"design/sw_design/#hardware-constraints","title":"Hardware Constraints","text":"<p>TBD: Resource requirements/limits and affinities Describe how to use affinities to deploy pods on nodes with specific harware Describe how to define resource requests/limits to deploy on node with sufficient resource (optimize resource usage).</p> <p>When defining a workflow template, resource requirements can be specified within the 'container' section. The 'podSpecPatch' field in either the workflow or workflow template allows for overriding the resource limits and requests for a container. See: Example</p> <p>TBD: add request/limits example</p> <p>To ensure a container runs on a Kubernetes node with specific hardware configurations, a nodeSelector can be defined in the workflow specification or workflow template. This nodeSelector should reference labels that match those defined on the desired node.  It is essential that Kubernetes nodes with specific hardware configurations use labels matching those used in the workflow/template nodeSelector parameter.</p> <p>TBD: add nodeSelector example</p>"},{"location":"design/sw_design/#artefact-persistence","title":"Artefact Persistence","text":"<p>TBD:  Workflow designer mark specific artefacts for persistence, overriding their default temporary status</p>"},{"location":"design/sw_design/#retention-policies","title":"Retention Policies","text":"<p>TBD:  Workflow engine operator configure retention policies to remove expired workflows automatically Archiving workflows: https://argo-workflows.readthedocs.io/en/stable/workflow-archive/</p> <p>The workflow archive stores information about the workflows such as the status, pods executed, results and more. This information is stored in a database such as PostgreSQL.</p> <p>Archive TTL: Specifies the time period to keep archived workflows before they will be deleted by the archived workflow garbage collection function. The default is forever.</p> <p>Example: <pre><code>persistence:\n  archiveTTL: 10d\n</code></pre></p>"},{"location":"design/sw_design/#subordinates","title":"Subordinates","text":"<p>Argo Workflows encompasses the various components and resources it manages and interacts with to orchestrate and execute workflows.</p> <ul> <li>Workflow Templates : These are predefined configurations that describe specific workflows. They serve as blueprints from which instances of workflows are generated and executed. Workflow templates define the sequence of tasks, their dependencies, and the resources needed for each task.</li> <li>Tasks/Pods - In the context of Argo Workflow, each step in a workflow is executed as a separate pod within the Kubernetes environment. These pods can be considered subcomponents of the workflow they belong to, executing specific actions defined by the workflow and then reporting back their status upon completion or failure.</li> </ul> <p>Note: identify the DB, message broker, prometheus, etc.</p>"},{"location":"design/sw_design/#dependencies","title":"Dependencies","text":"<ul> <li>Description: Detail any operations or conditions that must be met before this component can function properly, including exclusions during its operation.</li> </ul> <p>Note: if relevant, express dependency with other components (e.g. workflow needs module registry) Reflect the config of charts </p>"},{"location":"design/sw_design/#interfaces","title":"Interfaces","text":"<ul> <li>Control Flow: Describe how the component starts and terminates, including any interactions during execution (such as interrupts).</li> <li>Data Flow: Explain the input and output data flows, ensuring data structures are linked with control flows and interface components through common data areas or files.</li> </ul> <p>Documentation detailing the API specifications and usage is provided in teh API Design</p> <p>NOte: sequence diagram and link to the api-design document.</p>"},{"location":"design/sw_design/#resources","title":"Resources","text":"<ul> <li>Requirements: Itemize what the component needs from its environment to perform its function, excluding items that are part of the component interface.</li> </ul> <p>NOTE: include docker dependencies, might include maven dependencies (libs)</p>"},{"location":"design/sw_design/#data","title":"Data","text":"<p>Argo Workflows utilises complex internal data structures to orchestrate and manage workflows within Kubernetes. Here's a summary of these structures:</p> <ul> <li>Workflow Definitions: Defined as custom resources in YAML, these include:</li> <li>Element Descriptions: Identifiers (names), types (e.g., script, container), and dimensions (step hierarchies and dependencies).</li> <li>Relationships: Defines parent-child relationships among steps using dependencies that structure the workflow as a Directed Acyclic Graph (DAG).</li> <li>Value Range: Includes everything from static strings to dynamic outputs from previous steps.</li> <li> <p>Initial Values: Elements such as parameters may have default values specified in the workflow template.</p> </li> <li> <p>Workflow Status: This structure tracks the real-time state of each step within the workflow, indicating whether a step is pending, running, succeeded, or failed.</p> </li> <li> <p>Artifacts: Used to manage data inputs and outputs for each step. Artifacts can be files stored in various storage backends (e.g., S3, Artifactory).</p> </li> </ul> <p>For more detailed information about the internal data structures used by Argo Workflows, including their specifications and how they are utilised within workflows, refer to the official Argo Workflows Field Reference. This documentation provides comprehensive guides on defining and managing workflows, with specific sections on workflow specifications, status monitoring, and artifact handling.</p>"}]}